
This section outlines practical considerations for computating the local
sensitivity in \thmref{etat_deriv}. The mixed, second-order partial term
$\crosshessian$ is inexpensive to compute using automatic differentiation.
Typically, it is the computation and inversion of the Hessian matrix that is the
most expensive step in computing local sensitivity, especially when the
dimension of $\eta$ is large. To invert the Hessian, we will take advantage of
the fact that all the models we consider in \secref{results} can be decomposed
into a relatively small dimensional set of ``global'' variables (e.g., $\beta$
and $\nu$), and a relatively lare set of local variables ($\z$). This
factorization will be useful for optimizing the $\mathrm{KL}$ objective as well.

\subsection{The cross-hessian}

We are interested in sensitivity to the stick-breaking density, so in our
experiments (\secref{results}), only the prior on stick-breaking proportions
$\nu = (\nu_1, ..., \nu_{\kmax - 1})$ will depend on $\t$. Because the elements
of $\nu$ fully factorize under both the prior and the variational density,
$\crosshessian$ decomposes as
%
\begin{align}
  \crosshessian
  &= \sum_{\k=1}^{\kmax - 1}
         \evalat{\frac{\partial}{\partial \etanuk} \expect{\q(\nuk \vert \eta)}
                {
                \fracat{\partial \log \ptil(\nuk \vert \t)}{\partial \t}{\t = 0}
                }}{\etaopt}.
\eqlabel{sens_mixed_partial}
\end{align}

We approximate the expectation using GH quadrature (\appref{gh_quadrature}). In
all the functional forms for $\t \mapsto \ptil(\nuk \vert \t)$ considered below,
the derivative can be provided either analytically or computed with automatic
differentiation.  The resulting GH approximation is a deterministic function of
$\eta$, and thus the derivative in \eqref{sens_mixed_partial} can be computed
with another application of automatic differentiation. Note that $\crosshessian$
is sparse in \eqref{sens_mixed_partial}: it is zero for all entries of $\eta$
other than those that parameterize the stick distributions.

\subsection{A global and local factorization}

To compute the local sensitivity in practice, we make use of a factorization
common to all the models we will consider: the latent variables can be
partitioned into a set of \textit{global} variables and a set of \textit{local}
latent variables. The global variables are common to all data points, while the
local variables are unique to each data point. In the GMM example
(\exref{iris_bnp_process}), the global variables are $(\beta, \nu)$, while $\z$
are local variables.

Let $\gamma$ denote the collection of global latent variables and let $\etaglob$
be their variational parameters. Similarly, let $\ell$ denote the local latent
variables and let $\etalocal$ be the local variational parameters. Notice that
because the local latent variables are unique to each data point, the dimension
of $\etalocal$ scales with $\N$. The global parameter $\etaglob$ does not scale
directly with $\N$---in the GMM example, $\etaglob$ scales only with $\kmax$.

In all models we will consider, the conditional posterior $\p(\z \vert
\gamma,\x)$ has a tractable closed form.  Since we choose a conjugate mean field
approximating family for $\q(\z \vert \eta)$, this implies that the optimal
local variational parameters $\etaoptlocal$ can be written as a closed-form
function of the global variational parameters $\etaglob$.  Let
$\etaoptlocal(\eta_\gamma; \t)$ denote this mapping, so that
%
\begin{align}\eqlabel{local_eta_optim}
\etaoptlocal(\etaglob; \t) :=
    \argmin_{\etalocal} \KL{(\eta_\gamma, \etalocal), \t}.
\end{align}
%
When $\t=0$, we will simply write $\etaoptlocal(\etaglob; \t=0) =
\etaoptlocal(\etaglob)$. The next example details this mapping for the Gaussian
mixture model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}[Optimalility of $\etalocal$ in a GMM]\exlabel{qz_optimality}
Recall that under our truncated variational approximation,
the cluster assignment $\z_\n$ is a discrete random variable
over $\kmax$ categories.

Let $\eta_{\z_\n}$ be the categorical parameters in its exponential family
natural parameterization. That is, we let $\eta_{\z_\n} = (\rho_{\n1},
\rho_{\n2}, ..., \rho_{\n(\kmax-1)})$ be an unconstrained vector in
$\mathbb{R}^{\kmax-1}$; in this parameterization, the assignment probabilities
are
%
\begin{align*}
  p_{\n\k} := \expect{\q(\z_\n \vert \etaz)}{\z_{\n\k}} =
  \frac{\exp(\rho_{\n\k})}{1 + \sum_{\k'=1}^{\kmax-1}\exp(\rho_{\n\k})}
\end{align*}
%
We use the exponential family parameterization because we require the optimal
variational parameters $\etaopt$ to be interior to $\etadom$ in
\assuitemref{kl_opt_ok}{kl_opt_interior}.

Fixing $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$,
the optimal $\etaopt_{\z_\n}$ must satisfy
%
\begin{align*}
& \q(\z_\n | \etaopt_{\z_\n}) \propto \exp\left(\tilde \rho_{\n\k}\right)\\
& \mathwhere \tilde \rho_{\n\k} := \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
See \citet{bishop:2006:PRML} and \citet{blei:2017:vi_review} for details.
To satisfy this optimality condition,
we set the optimal $\etaopt_{\z_\n}$ to be
%
\begin{align*}
%
\etaopt_{\z_\n} = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}},
\log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots,
\log\frac{\tilde\rho_{\n(\kmax-1)}}{\tilde\rho_{\n\kmax}}\right).
%
\end{align*}
%
Thus, as long as the expectations $\tilde\rho_{\n\k}$ can be provided
as a closed-form function of
$(\etabeta, \etanu)$, the optimal $\etaopt_{\z_\n}$ can be also be set in closed-form as
a function of $(\etabeta, \etanu)$.
%
\end{ex}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The availability of the minimizer \eqref{local_eta_optim} in closed-form will be
useful for both optimizing the variational parameters and for computing local
sensitivity.  Using \eqref{local_eta_optim}, we can rewrite our objective as a
function of the global parameters, which will aid in both optimization and
derivative computations.  Define
%
\begin{align*}
\KLglobal(\etaglob, \t) :=
    \mathrm{KL}\Big((\etaglob, \etaoptlocal(\etaglob; \t)), \t\Big).
\end{align*}
%
The $\etaoptglob(\t)$ which minimizes $\KLglobal(\etaglob, \t)$ is the same as
the corresponding sub-vector of the $\etaopt(\t)$ that minimizes $\KL{\eta,
\t}$.  Therefore we can use the objective function $\KLglobal(\etaglob, \t)$ as
a numerical surrogate for $\KL{\eta, \t}$ when optimizing or computing
derivatives.

For example rather than optimizing the $\KL{\eta}$ over all variational
parameters, we numerically optimize $\KLglobal$, which is a function only of the
relatively low-dimensional global parameters.  To minimize $\KLglobal(\etaglob)$
in practice, we run the BFGS algorithm with a loose convergence tolerance
followed by trust-region Newton conjugate gradient
\citep[Chapter~7]{nocedal:2006:numerical} to find a high-quality optimum. After
the optimization terminates at an optimal $\etaoptglob$, the optimal local
parameters $\etaoptlocal$ can be set in closed form to produce the entire vector
of optimal variational parameters $\etaopt = (\etaoptglob, \etaoptlocal)$.
%
The construction of $\KLglobal$ will also play an important role in inverting
the Hessian matrix for local sensitivity, which we now detail.


\subsection{The Hessian inversion}

Typically, it is the computation and inversion of the $\etadim \times
\etadim$-dimensional Hessian matrix $\hessopt$ that is the most computationally
intensive part of \eqref{vb_eta_sens}, especially when the dimension of $\eta$
is large.  Recalling that the dimension of $\eta$ scales with $N$, it is easy to
imagine how it may even be impossible to instantiate a dense representation of
$\hessopt$ in memory.

To efficiently invert $\hessopt$, we again take advantage the factorization into
global and local variables. For generic variables $a$ and $b$, let
$\hess{ab}$ denote the sub-matrix $\evalat{\partial^2 \KL{\eta} / \partial
\eta_a \eta_b^T}{\etaopt}$, the Hessian with respect to the variational
parameters governing $a$ and $b$. We decompose the Hessian matrix $\hessopt$
into four blocks according to the global / local decomposition:
%
\begin{align*}
%
\hessopt =
\fracat{\partial^2 \KL{\eta}}
       {\partial \eta \partial \eta^T}
       {\etaopt} ={}&
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right).
%
\end{align*}
%
Simiarly, let $\crosshessian_\gamma$ be the components of $\crosshessian$
corresponding to the variational parameters $\etaglob$.  For example,
$\crosshessian_\gamma$ is given by replacing the operator $\partial / \partial
\eta$ with $\partial / \partial \etaglob$ in \eqref{sens_mixed_partial}. The
analogous quantity for the local parameters, $\crosshessian_\ell$, is zero,
since no local variables enter the expectation in \eqref{sens_mixed_partial}
when we are perturbing the stick-breaking distribution.
%
We can thus write
\begin{align*}
  \crosshessian = \left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right).
  %
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right),
%
\end{align*}
%
and an application of the Schur complement gives
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\ell\ell}^{-1} \hess{\ell\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right)^{-1} \crosshessian_\gamma,
%
\end{align*}
%
where $I_{\gamma\gamma}$ is the identity matrix with
the same dimension as $\eta_\gamma$.
%
Specifically, observe that the sensitivity of the global parameters
is given by
%
\begin{align}\eqlabel{global_sens}
  \fracat{d \etaopt_\gamma(\t)}{d \t}{t = 0} &=
  - \hessopt_\gamma^{-1}\crosshessian_\gamma
  \mathwhere
  \hessopt_\gamma := \left(\hess{\gamma\gamma} -
        \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right),
\end{align}
%
In our model, $\hess{\ell\ell}$ is sparse, and the size of $\hess{\gamma\gamma}$
does not grow with $\N$. Thus, each term of $\hessopt_\gamma$ can be tractably
computed, stored in  memory, and inverted, even on very large datasets.

One can derive the exact same identity using the optimality of
$\etaoptlocal(\eta_\gamma)$.  By applying the chain rule, one can
verify that
%
\begin{align}\eqlabel{global_kl}
\hessopt_{\gamma} &=
    \frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
    \KLglobal(\etaopt_\gamma, 0).
\end{align}
%
In practice, we evaluate $\hessopt_\gamma$ using automatic differentiation and
\eqref{global_kl} rather than the Schur complement.

Using \eqref{global_sens}, we can compute a Taylor series for the
global parameters only:
%
\begin{align}\eqlabel{global_lin_approx}
  \etalin_\gamma(\t) := \etaopt_\gamma +
  \fracat{d \etaopt_\gamma(\t)}{d \t}{\t=0} \t .
\end{align}
%
When our function of interest $\g_{loc}$ depends on the full vector $\eta$,
including the local parameters, we can again take advantage of the closed form
of $\etaoptlocal(\etaglob)$, by redefining our quantity of interest as
%
\begin{align}\eqlabel{g_as_global}
\g(\etaglob(\t)) :=
    \g_{loc}\left(
        \left(\etaglob(\t), \etaoptlocal(\etaglob; \t) \right) \right)
\end{align}
%
\Exref{vb_insample_nclusters_simple} above employs this technique. By using
\eqref{g_as_global} for our approximation, we both render the derivative
computation easier, and retain nonlinearities in the map $\etaglob \mapsto
\etaoptlocal(\etaglob; \t)$.  As a shorthand, we will use
$\g(\etalin_\gamma(t))$ to denote the posterior computed under our linearized
global parameters at prior parameter $\t$ using the mapping \eqref{g_as_global}.

Finally, we will briefly discuss how to numerically solve the linear system
$\hessopt_\gamma^{-1}\crosshessian_\gamma$ of \eqref{global_sens}.  We used the
conjugate gradient (CG) algorithm, which requires only Hessian-vector products
which are readily available through automatic differentiation. Iterative methods
such as CG avoid instatiating the Hessian matrix in memory. We could have also
computed the Hessian and either factorized it (e.g. with a Cholesky
decomposition). Then, the Hessian inverse (or its Cholesky decomposition) can be
re-used for different perturbations. In our approach, we need to re-solve the
linear system \eqref{global_sens} with CG for each choice of prior perturbation
(since different choices of prior perturbation require solving different
$\crosshessian$). On our applications in \secref{results}, the conjugate
gradient algorithm was at least an order of magnitude faster than refitting, and
did not pose a meaningful bottleneck to exploring different perturbations.
