
This section outlines practical considerations for computating
the local sensitivity in \thmref{etat_deriv}.
The cross-hessian term $\crosshessian$
\todo{is this actually called a cross hessian? need to give a name}
is inexpensive to compute using automatic differentiation.
Typically, it is the computation and inversion of the Hessian matrix that is
the most expensive step in computing local sensitivity,
especially when the dimension of $\eta$, is large.
To invert the Hessian, we will take advantage of the fact that
all the models we will consider factorize into a set of \textit{global} latent
variables and a set of \textit{local} latent variables.
This factorization will be useful for optimizing the $\mathrm{KL}$ objective
as well.

\subsection{The cross-hessian}

\todo{would this subsection make more sense immediately after \thmref{etat_deriv}?}

We are interested in sensitivity to the stick-breaking distribution,
so only the prior terms on stick-breaking proportions
$\nu = (\nu_1, ..., \nu_{\kmax - 1})$ will depend on $t$.
Because the elements of $\nu$ fully factorize
under both the prior and the variational distributions,
$\crosshessian$ decomposes as
\begin{align}
  \crosshessian &=
  \sum_{\k=1}^{\kmax - 1}
          \expect{\q(\nuk \vert \eta)}
                 {
                 \lqgrad{\nuk \vert \etaopt}
                 \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                 } \notag\\
  &= \sum_{\k=1}^{\kmax - 1}
         \evalat{\nabla_\eta \expect{\q(\nuk \vert \eta)}
                {
                \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                }}{\eta = \etaopt(0)},
\eqlabel{sens_mixed_partial}
\end{align}
where we assumed that $\q(\theta \vert \eta)$ is normalized, so
$\lqgradbar{\theta \vert \etaopt} = \lqgrad{\theta \vert \etaopt}$,
and that the assumptions of \thmref{etat_deriv} hold, so we
can freely exchange derivatives with expectations.

We approximate the expectation using GH quadrature (\appref{gh_quadrature}),
with the integrand $f$ in \eqref{gh_integral} being
$f(\nu_k) = \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}$.
In all the functional forms for
$\t \mapsto \pstick(\nuk \vert \t)$ considered below,
$f(\nu_k)$ can be provided in either closed-form or computed with automatic differentiation.
The resulting GH approximation is a deterministic function of $\eta$,
and thus the gradient in \eqref{sens_mixed_partial} can be computed
with another application of automatic differentiation.
Note that $\crosshessian$ is sparse in \eqref{sens_mixed_partial}:
it is zero for all entries of
$\eta$ other than those that parameterize the sticks.

\subsection{A global and local factorization}

To compute the local sensitivity in practice, we
make use of a factorization common to all the models we will consider:
the latent variables can be partitioned into a set of \textit{global}
variables and a set of \textit{local} latent variables.
The global variables are common to all data points, while the local variables
are unique to each data point.
In the GMM example, the global variables are $(\beta, \nu)$,
while $\z_1, ..., \z_\N$ are local variables.

Let $\gamma = (\beta,\nu)$ be the global latent variables
and let $\etaglob = (\etabeta, \etanu)$ be their variational parameters.
Similarly, let $\ell$ be the local latent variables and
let $\etalocal$ be the local variational parameters.
In the GMM example, $\ell = \z$ and $\etalocal = \etaz$,
but in later models, the local variables may include more than just $\z$.
Notice that the dimension of $\eta_\gamma$ scales
with only $\kmax$, while $\eta_\ell$ scales with $\N\times\kmax$.

In all models we will consider,
the optimal local variational parameters $\etaoptlocal$ can be written
as a closed-form function of the global variational parameters $\etaglob$.
Let $\etaoptlocal(\eta_\gamma; \t)$ denote this mapping; that is,
\begin{align*}
  \etaoptlocal(\etaglob; \t) := \argmin_{\etalocal} \KL{(\eta_\gamma, \etalocal), \t}.
\end{align*}
With this minimizer available in closed-form, we can define
\begin{align}\eqlabel{kl_global}
\KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptlocal(\eta_\gamma; \t)), \t\Big),
\end{align}
which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum.

\begin{ex}[Optimalility of $\etalocal$ in a GMM]\exlabel{qz_optimality}
Recall that under our truncated variational approximation,
the cluster assignment $\z_\n$ is a discrete random variable
over $\kmax$ categories.

Let $\eta_{\z_\n}$ be the categorical parameters in its
exponential family natural parameterization.
That is, we let $\eta_{\z_\n} = (\rho_{\n1}, \rho_{\n2}, ..., \rho_{\n(\kmax-1)})$
be an unconstrained vector in $\mathbb{R}^{\kmax-1}$;
in this parameterization, the assignment probabilities are
%
\begin{align*}
  p_{\n\k} := \expect{\q(\z_\n \vert \etaz)}{\z_{\n\k}} =
  \frac{\exp(\rho_{\n\k})}{1 + \sum_{\k'=1}^{\kmax-1}\exp(\rho_{\n\k})}
\end{align*}
%
We use the exponential family parameterization because
we require the optimal variational parameters $\etaopt$
to be interior to $\etadom$ in \thmref{etat_deriv}.
In the mean parameterization,
$\sum_{\k=1}^\kmax p_{\n\k} = 1$, so the
optimal mean parameters $\hat p_{\n}$ cannot be
interior to $\Delta^{\kmax - 1}$.
On the other hand, $\eta_{\z_\n}$ as defined
is unconstrained in $\mathbb{R}^{\kmax - 1}$.

Fixing $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$,
the optimal $\etaopt_{\z_\n}$ must satisfy
%
\begin{align*}
& \q(\z_\n | \etaopt_{\z_\n}) \propto \exp\left(\tilde \rho_{\n\k}\right)\\
& \mathwhere \tilde \rho_{\n\k} := \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
See \citet{bishop:2006:PRML} and \citet{blei:2017:vi_review} for details.
To satisfy this optimality condition,
we set the optimal $\etaopt_{\z_\n}$ to be
%
\begin{align*}
%
\etaopt_{\z_\n} = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}},
\log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots,
\log\frac{\tilde\rho_{\n(\kmax-1)}}{\tilde\rho_{\n\kmax}}\right).
%
\end{align*}
%
Thus, as long as the expectation $\tilde\rho_{\n\k}$ has a closed-form as a function of
$(\etabeta, \etanu)$, the optimal $\etaopt_{\z_\n}$ can be also be set in closed-form as
a function of $(\etabeta, \etanu)$.
%
\end{ex}



Rather than optimizing the $\mathrm{KL}$ over all variational parameters, both global and local,
we optimize $\KLglobal$ (\eqref{kl_global}), which is a function of global parameters only.
Minimizing $\KLglobal(\etaglob)$ keeps the
dimension of the optimization parameter independent of the number of data points.
After the optimization terminates at an optimal $\etaoptglob$,
the optimal local parameters $\etaoptlocal$ can be set in closed form
to produce the entire vector of optimal variational parameters $\etaopt = (\etaoptglob, \etaoptlocal)$.
The construction of $\KLglobal$ will also play an
important role in computing the local sensitivity in practice,
which we now detail.


% Using the optimality of $\etaoptz(\eta_\gamma; \t)$ and applying the chain rule, one can check that
% \begin{align}\eqlabel{global_kl}
% \hessopt_{\gamma} &=
% \frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
% \KLglobal(\etaopt_\gamma, 0) \\
% & \mathwhere
% % \mathrm{KL}_{glob}(\eta_\gamma) := \evalat{\KL{\eta, \t}}{\eta = (\eta_\gamma, \etaoptz(\eta_\gamma; t))}.
% \KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptz(\eta_\gamma; \t)), \t\Big).
% \notag
% \end{align}

% the global latent variables $\gamma := (\nu, \beta)$.
% We also defined local variational parameters $\eta_\ell$ which parameterize
% the local latent variables $\z$ (or $(\z, \b)$ in the regression model).
% Instead of optimizing


\subsection{The Hessian inversion}

Typically, it is the computation and inversion of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
the dimension of $\eta$, is large.
For especially high-dimensional $\eta$,
instantiating the full Hessian in memory
may be impossible.

To invert the Hessian, we take advantage of the fact that the latent variables
factorize into a set of global and local latent variables.
For generic latent variables $a$ and $b$,
let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
\eta_b^T}{\etaopt(0), t=0}$, the Hessian with respect to the variational
parameters governing $a$ and $b$.
We decompose the Hessian matrix $\hessopt$ into four blocks:
%
\begin{align*}
%
\hessopt =
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \eta^T}
       {\etaopt(0), t= 0} ={}&
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right).
%
\end{align*}

Next, let $\crosshessian_\gamma$ be the components of
$\crosshessian$ corresponding to the variational parameters
$\etaglob$---that is, $\crosshessian_\gamma$ is given by replacing
the operator $\nabla_\eta$ with $\nabla_{\eta_\gamma}$ in \eqref{sens_mixed_partial}.
The analogous quantity for the local parameters, $\crosshessian_\ell$,
is zero, since no local variables enter the expectation in \eqref{sens_mixed_partial}.
We can thus write
\begin{align*}
  \crosshessian = \left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right).
  %
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right),
%
\end{align*}
%
and an application of the Schur complement gives
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\ell\ell}^{-1} \hess{\ell\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right)^{-1} \crosshessian_\gamma,
%
\end{align*}
where $I_{\gamma\gamma}$ is the identity matrix with
the same dimension as $\eta_\gamma$.

Specifically, observe that the sensitivity of the global parameters
is given by
\begin{align}\eqlabel{global_sens}
  \fracat{d \etaopt_\gamma(\t)}{d \t}{t = 0} &=
  - \hessopt_\gamma^{-1}\crosshessian_\gamma
  \mathwhere
  \hessopt_\gamma := \left(\hess{\gamma\gamma} -
        \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right).
\end{align}
Each term of $\hessopt_\gamma$ can be easily computed using automatic differentiation.

Alternatively, using the optimality of $\etaoptlocal(\eta_\gamma; \t)$ and
applying the chain rule, one can check that
\begin{align}\eqlabel{global_kl}
\hessopt_{\gamma} &=
\frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
\KLglobal(\etaopt_\gamma, 0)
\end{align}

\eqref{global_kl} is how we calculate $\hessopt_\gamma$ in practice:
we implement $\mathrm{KL}_{glob}$, which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum;
we then use automatic differentiation to
compute the Hessian of $\mathrm{KL}_{glob}$.

Crucially, the size of $\hessopt_\gamma$ scales with $(\kmax)^2$,
while the full Hessian scales with both $(\kmax\times\N)^2$
Hence, $\hessopt_\gamma$ is more memory efficient to store
and faster to invert
than the full Hessian of all the variational parameters.


Now that we can compute the sensitivity of the global parameters in
\eqref{global_sens}, we produce its linear approximation:
\begin{align}\eqlabel{global_lin_approx}
  \etalin_\gamma(\t) := \etaopt_\gamma +
  \fracat{d \etaopt_\gamma(\t)}{d \t}{\t=0} \t .
\end{align}

Finally, given a posterior quantity $\g$,
we again take advantage of the fact that the optimal
local parameters can be found in closed form given global parameters.
In the same way that $\KLglobal$ implicitly sets the local parameters at their optimum
and is a function of only global parameters and the prior parameter $\t$,
we can construct an analogous mapping for $\g$,
\begin{align}\eqlabel{g_as_global}
(\t, \etaglob) \mapsto g\Big(\big(\etaglob, \etaoptz(\etaglob, \t))\Big).
\end{align}

This mapping can be used for any posterior quantity.
Therefore, linearizing the global parameters using \eqref{global_sens, global_lin_approx} is sufficient;
we do not need to invert the full Hessian
and linearize the entire set of variational parameters, global and local.
As a shorthand, we will use $\g(\etalin_\gamma(t))$ to
denote the posterior computed under our linearized global parameters
at prior parameter $\t$ using the mapping \eqref{g_as_global}.


% We have the option of again taking advantage of the fact that the optimal
% local parameters can be found in closed form given global parameters
% to compute any posterior statistic $\g$.
% In the same way that
%
%
%
% We assume that $\etaoptz(\eta_\gamma; \t)$ is inexpensive to compute.
% Then, instead of linearizing both local and variational parameters,
% we linearize only the global parameters and approximate
% the full set of variational parameters at $\t \not= 0$ using
% %
% \begin{align}\eqlabel{global_lin_approx_all}
% \etalinglobal(\t) = \left(\begin{array}{c} \etalin_\gamma(\t) \\
% \etaoptz(\etalin_\gamma(\t); \t)
% \end{array}\right).
% \end{align}
%
% The advantage to $\etalinglobal$ is that the Hessian
% required for forming the approximation does not scale with $\N$.
% Note also that the $\mathrm{KL}$ evaluated at $\etalinglobal$
% will always be smaller than the $\mathrm{KL}$ evaluated at $\etalin$ obtained by
% linearizing all variational parameters,
% and thus $\etalinglobal$ may be a better approximator
% to the actual refitted variational parameters.
%
%
%


% \subsection*{old text}
%
%
%
%
%
% % Moreover, the $\hess{\z\z}$ term is block-diagonal with $N$ blocks of size $\kmax\times\kmax$;
% % the $n$-th block corresponds to the variational parameters governing $z_n$.
% % Each block, and resulting matrix $\hess{\z\z}$, has a closed form inverse.
%
%
% % However, this Hessian matrix does not depend on $\t$, and
% % so can be computed once and re-used for many different values of $\t$ or many
% % different classes of prior perturbations. In our case, w
%
%   Fortunately, the Hessian of the KL divergence
% is sparse and highly structured in $\etaz$.  In particular, it is
% block-diagonal, with
% %
% \begin{align*}
% %
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta_{\z_{n}} \partial \eta_{\z_{m}}^T}
%        {\etaopt(t = 0), \t_0} ={}& 0 \mathtxt{for}n\ne m.
% %
% \end{align*}
% %
% Recall from \exref{qz_unconstrained} that $\eta_{\z_\n}$ is of dimension $\kmax -
% 1$, and that the Hessian of $\expect{\q(\zeta \vert \eta)}{\logp(\x \vert
% \zeta)}$ with respect to $\etaz$ may be nonzero despite $\logp(\x \vert \zeta)$
% being linear in $\z$ due to the unconstrained parameterization of $\etaz$.
%
% We propose the following techniques for computing the inverse Hessian when the
% fully matrix is prohibitively large.  To describe the techniques it will be
% useful to use the following compact notation.  Let $\gamma = (\beta, \nu)$
% denote all the parameters besides $\z$, and for generic parameters $a$ and $b$
% let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
% \eta_b^T}{\etaopt(\t_0), \t_0}$, the Hessian with respect to the variational
% parameters governing $a$ and $b$.  Specifically:
% %
% \begin{align*}
% %
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta \partial \eta^T}
%        {\etaopt(\t_0), \t_0} ={}&
% \hess{\zeta\zeta} =
% \left(
% \begin{array}{cc}
%    \hess{\gamma\gamma} & \hess{\gamma\z} \\
%    \hess{\z\gamma}     & \hess{\z\z} \\
% \end{array}
% \right).
% %
% \end{align*}
% %
% The terms $\hess{\gamma\gamma}$ and $\hess{\z\gamma} = \hess{\gamma\z}^T$ are
% typically dense and easily computed using atuomatic differentiation, and the
% term $\hess{\z\z}$ is block diagonal with a closed-form inverse.
% %
% Analogously, let
% %
% \begin{align*}
% %
% \hess{\zeta\t} :=
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta \partial \t}
%        {\etaopt(\t_0), \t_0},
% %
% \end{align*}
% %
% and observe that
% %
% \begin{align*}
% %
% \hess{\zeta\t} ={}& \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right).
% %
% \end{align*}
%
% In this notation,
% %
% \begin{align*}
% %
% \fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
% -\left(
% \begin{array}{cc}
%    \hess{\gamma\gamma} & \hess{\gamma\z} \\
%    \hess{\z\gamma}     & \hess{\z\z} \\
% \end{array}
% \right)^{-1}
% \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right)
% %
% \end{align*}
% %
% We can then use the Schur complement to get the computationally tractable
% expression:
% %
% \begin{align*}
% %
% \fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
% -\left(\begin{array}{c}
% I_{\gamma\gamma} \\
% \hess{\z\z}^{-1} \hess{\z\gamma}
% \end{array}\right)
% \left(\hess{\gamma\gamma} -
%       \hess{\gamma\z} \hess{\z\z}^{-1} \hess{\z\gamma}\right)^{-1} \hess{\gamma\t}.
% %
% \end{align*}
%
% In fact, the former expression can be computed entirely using automatic
% differentiation using the fact that $\etaoptz$ has an explicit closed form given
% $\etaoptgamma$, as dicussed in \exref{qz_form}.  TODO: is this worth writing
% here?
%
% When even $\hess{\gamma\gamma}$ is too large to form in memory, one can
% use the conjugate gradient algorithm.  TODO: fill in details.
%
%
%
% %
% %  we can evaluate the mixed partial using the derivative of
% % \eqref{gh_integral}.  We will consider particular functional forms for
% % $\t \mapsto \pstick(\nuk \vert \t)$ in below.
% %
% %
% % In order to evaluate $d\etaopt(\t) / d\t$ as given by \eqref{vb_eta_sens}, we
% % need to solve a linear system involving the $\etadim \times \etadim$ Hessian of
% % the objective function $\KL{\eta, 0}$ and the $\etadim \times 1$ mixed
% % second-order partial derivative of $\KL{\eta, \t}$ with respect to $\eta$ and
% % $\t$.
% % \todo{Theorem 1 isn't in the form of a mixed partial; maybe this needs more explaining.
% % Perhaps we could put \eqref{sens_mixed_partial} and a discussion about this earlier?
% % e.g. after corollary 1?}
% % We will discuss these two tasks in turn.
% %
% % First, consider the mixed partial derivative $\partial^2 \KL{\eta, \t} /
% % \partial \eta \partial \t$.  In our case, only the priors $\log \pstick(\nuk
% % \vert \t)$ depend on $\t$, and so
% % %
% % \begin{align}\eqlabel{sens_mixed_partial}
% % %
% % \fracat{\partial^2 \KL{\eta, \t}}
% %        {\partial \eta \partial \t}
% %        {\etaopt(\t_0), \t_0} ={}&
% % \sum_{\k=1}^{\kmax}
% %     \evalat{
% %         \frac{\partial}{\partial\eta}
% %         \expect{\q(\nuk \vert \eta)}
% %                {\fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t_0}
% %                }
% %         }
% %         {\etaopt(\t_0)}.
% % %
% % \end{align}
% % %
% % Consequently, as long as we can explicitly evaluate $\fracat{\log \pstick(\nuk
% % \vert \t)}{\partial \t}{\t_0} $, either in closed form or with automatic
% % differentiation, we can evaluate the mixed partial using the derivative of
% % \eqref{gh_integral}.  We will consider particular functional forms for
% % $\t \mapsto \pstick(\nuk \vert \t)$ in below.
% %
% % Observe that \eqref{sens_mixed_partial} will be zero for all entries of
% % $\eta$ other than those that parameterize the sticks.
