This section outlines two important computational necessities for our
method and experiments below.
First, we discuss how to evaluate, or approximate, expectations with respect
to the variational distribution $\mathcal{Q}$.
Expectations appear in the $\mathrm{KL}$ objective,
the local sensitivity (e.g. the $\crosshessian$ term in \thmref{etat_deriv}),
and in posterior quantities (such as \exref{insample_nclusters}).
Secondly, all our models that we consider in \secref{results} will
factorize into \textit{global} and \textit{local} latent variables.
We take advantage of this factorization
for both the $\textrm{KL}$ optimization and
for the Hessian inversion required for local sensitivy.

\subsection{Evaluating expectations}

Some expectations with respect to $\mathcal{Q}$ are available
analytically, but taking advantage of conditional conjugacy, for example.
When analytic expectations are unavailable, we approximate expectations
either with numerical integration or Monte Carlo estimates.

\subsubsection{Conditional conjugacy}

For $\z$ and $\beta$ in all models we consider, we will
take advantage of conditional conjugacy to choose distributions
$\q(\z_\n\vert\eta)$ and $\q(\beta_k\vert\eta)$, unless otherwise stated.
This means that we will take $\q(\z_{\n}
\vert \eta)$ to be multinomial, matching $\p(\z_{\n}
\vert \x, \beta, \nu)$, and we will take $\q(\beta_\k \vert \eta)$
to match the distribution of $\p(\beta_{\k} \vert \x, \z, \nu)$.
With conditionally conjugate distributions,
all expectations with respect to $\z$ and $\beta$
in the $\mathrm{KL}$ objective will be available analytically.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\beta_\k$ in a GMM]\exlabel{iris_var_distr}
%
To evaluate the expectation in \eqref{vb_optimization}, we need to compute
the expected joint log-likelihood
\begin{align*}
  \expect{\q(\beta_\k \vert \eta)}{\log \p(\x_\n, \beta_\k)}. % , \quad
  % \expect{\q(\beta_\k \vert \eta)}{\log \q(\beta_\k \vert \eta)}.
\end{align*}

In \exref{iris_bnp_process}, $\beta_\k = (\mu_\k, \Lambda_\k)$,
and the likelihoods are Gaussian,
$\p(\x_\n \vert \beta_\k) = \normdist{\x_n \vert \mu_\k, \Lambda_\k^{-1}}$.
The prior $\pbetaprior(\beta_\k)$ a normal-Wishart.
Using the log densities displayed in \exref{iris_bnp_process}
and the mean-field assumption on $\q$,
observe that $\beta_\k$ enters the expected joint log-likelihood only through the
expected moments
%
\begin{align*}
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k},  \quad
\expect{\q(\beta_\k \vert \eta)}{\log|\Lambda_\k|},  \quad
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k\mu_\k}, \quad
\expect{\q(\beta_\k \vert \eta)}{\mu_\k\Lambda_\k\mu_\k}.
\end{align*}

The conditionally conjugate variational distribution on $\beta_\k$ is
normal-Wishart, which we denote as
$\q(\beta_\k \vert \eta) = \normalwishart{\beta_k \vert \eta}$.
With this choice of $\q(\beta_\k \vert \eta)$,
all the preceding expected moments
can be provided as closed-form functions of $\eta$.
%
\end{ex}

\begin{ex}[VB approximation for $\z_\n$ in a GMM]\exlabel{qz_form}
%
The conditionally conjugate variational distribution for $\z_\n$
is multinomial.
Our variational approximation is truncated at $\kmax$ so
$\z_{\n\k} = 0$ for all $\k > \kmax$;
the multinomial distribution under $\q$ has $\kmax$ discrete categories.

We parameterize the the multinomial distribution
using its natural parameterization
in exponential family form. That is,
we let $\eta_{\z_\n} = (\rho_{\n1}, \rho_{\n2}, ..., \rho_{\n(\kmax-1)})$
be an unconstrained vector in $\mathbb{R}^{\kmax-1}$;
in this parameterization, the multinomial expectations are
%
\begin{align*}
  p_{\n\k} := \expect{\q(\z_\n \vert \eta)}{\z_{\n\k}} =
  \frac{\exp(\rho_{\n\k})}{1 + \sum_{\k'=1}^{\kmax-1}\exp(\rho_{\n\k})}
\end{align*}
%
We use the exponential family parameterization because
we will require the optimal variational parameters $\etaopt$
to be interior to $\etadom$ in our sensitivity analysis
(\secref{local_sensitivity}).
In the mean parameterization,
$\sum_{\k=1}^\kmax p_{\n\k} = 1$, so the
optimal mean parameters $\hat p_{\n}$ cannot be
interior to $\Delta^{\kmax - 1}$.
On the other hand, $\eta_{\z_\n}$ as defined
is unconstrained in $\mathbb{R}^{\kmax - 1}$.

Notice that from the factorization in \eqref{bnp_model}
and the mean-field assumption on $\q$, the expected
joint log-likelihood depends on $\z_\n$
only through the assignment probabilities, $p_{\n\k}$,
which we showed has a closed for expression
with respect to the variational parameters $\eta_\z$.
%
\end{ex}

\subsubsection{Stick-breaking expectations}

To evaluate the $\mathrm{KL}$ objective, we also need
expectations over stick-breaking proportions,
\begin{align}\eqlabel{stick_expectations}
%
\expect{\q(\nuk \vert \eta)}{\log \nuk}
\textrm{,}\quad
\expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
\textrm{,}\quad\textrm{and}\quad
\expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
%
\end{align}
The first two expectations appear in the $\mathrm{KL}$
when decomposing the mixture weights
$\expect{}{\log \pi}$ into its component stick-breaking proportions (\eqref{stick_breaking}).

If the prior $\pstick(\nuk)$ were Beta-distributed like in the GEM construction,
then the conditionally conjugate distribution for $\q(\nuk \vert \eta)$ would also be Beta.
In this case, all the displayed expectations in \eqref{stick_expectations}
can be computed analytically as a function of
the Beta parameters in the variational approximation.

However, we will be considering stick-breaking distributions $\pstick(\nuk)$ that
are outside the family of Beta distributions.
Therefore, we chose the distribution on the unconstrained logit-sticks $\lnu_\k$ to be
normally distributed, which then induces a logit-normal distribution on
the $(0, 1)$-constrained sticks $\nu_\k$.
Let $\lnumean_\k$ and $\lnusd_\k$ be location and scale parameter,
respectively, of the normal distribution on $\lnu_\k$,
and recall that we let $\s$ be the sigmoid function
which maps logit-sticks to sticks.
To compute expectations of a smooth function
$f(\nuk)$ (such as $f(\nuk) = \pstick(\nuk)$),
the law of the unconscious statistician states that,
\begin{align*}
  \expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
  \expect{\q(\lnu_\k \vert \eta)}
         {f\circ \s\left(\lnu_\k\right)},
\end{align*}
By choosing $\q(\lnu_\k \vert \eta)$ to be Gaussian,
the right-hand side is a Gaussian integral,
which we approximate
using GH quadrature with $\ngh$ knots,
located at $\xi_g$, weighted by $\omega_g$:
%
\begin{align}\eqlabel{gh_integral}
%
\expect{\q(\lnu_\k \vert \eta)}
       {f\circ \s\left(\lnu_\k\right)}
\approx{}&
    \sum_{g=1}^{\ngh} \omega_g f\circ \s \left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
 \nonumber\\=:{}&
\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
%
\end{align}
%
Using GH quadrature to approximate the expectation
is similar to the \textit{reparameterization trick}
\citep{kingma2013autoencoding, rezende2014stochastic},
only using GH points rather than standard normal draws.

We also use GH quadrature to evaluate $\crosshessian$ in the local
sensitivity computation (\eqref{sens_mixed_partial}).
Here, $f(\nu_k) = \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}$.
In all the functional forms for
$\t \mapsto \pstick(\nuk \vert \t)$ we consider,
$f(\nu_k)$ can be provided in either closed-form or computed with automatic differentiation.
The resulting GH approximation is a deterministic function of $\eta$,
and thus the gradient in \eqref{sens_mixed_partial} can be computed
with another application of automatic differentiation.

\subsubsection{Posterior quantities}

All the posterior quantities we will consider in \secref{results}
can be expressed as an expectation over $\q$.
For some posterior quantities, such as the number of in-sample clusters $\gclusters$
with $\tau = 0$, the expectation is
available analytically (\exref{insample_nclusters}).
However, for other posterior quantities, the expectation over $\q$ will not be a simple
closed-form function of $\eta$.
For example, computing $\gclusters$ with a threshold $\tau > 0$
requires forming all ${\N\choose\tau}$ combination of $\tau$-length products
$\expect{}{\z_{\n_1\k}}\times \ldots \times \expect{}{\z_{\n_\tau\k}}$
for each $\k$.
In such cases, we to Monte Carlo approximations of the expectation.
Specifically, we used the reparameterization trick
to sample from the variational distribution.
In the case of $\gclusters$,
we construct an $\eta$-dependent transformation
$h(\cdot, \eta)$ that satisfies
\begin{align*}
  % u \iid\normdist{0, I} \implies
  u \iid\text{Uniform}(0, 1)^{\N\kmax} \implies
  h(u, \eta) \stackrel{d}{=} \z \sim \q(\cdot | \eta).
\end{align*}
To form a Monte Carlo estimate of $\gclusters$,
we sampled $u_1, \dots, u_m$,
and then averaged the expression inside the expectation evaluated at points
$h(u_1, \eta), \ldots, f(u_m, \eta)$.
The uniform draws $u_1, \dots, u_m$ can be fixed beforehand.
This is important for two reasons.
First, we will be evaluating the same $\g$ at different parameter vectors $\eta$;
conditional on the fixed $m$ uniform draws, $\g$ will be a deterministic function
of $\eta$, and we can compare how $\g$ changes without stochasticity.
Secondly, in the construction of the influence function
(\secref{functional_perturbations}), we require evaluating the
gradient of $\g$ with respect to $\eta$; conditional on the random draws,
$\g$ (or more precisely, our Monte Carlo approximation of $\g$), will be
differentiable with respect to $\eta$.

\subsection{A global and local factorization}

All the models we consider will factorize into a set \textit{global} latent
variables and a setof \textit{local} latent variables.
The global variables are common to all data points, while the local variables
are unique to each data point.
In the GMM example, the global variables are $(\beta, \nu)$,
while $\z_1, ..., \z_\N$ are local variables.

Let $\gamma = (\beta,\nu)$ be the global latent variables
and let $\etaglob = (\etabeta, \etanu)$ be their variational parameters.
Similarly, let $\ell$ be the local latent variables and
let $\etalocal$ be the local variational parameters.
In the GMM example, $\ell = \z$ and $\etalocal = \etaz$,
but in later models, the local variables will include more than just $\z$.
Thus, we keep the notation general.
Notice that the dimension of the global variational parameters scale
with only $\kmax$, while the local parameters scale with the number of
data points $\N$.

In all models we will consider,
the optimal local variational parameters $\etaoptlocal$ can be written
as a closed-form function of the global variational parameters $\etaglob$.
Let $\etaoptlocal(\eta_\gamma; \t)$ denote this mapping; that is,
\begin{align*}
  \etaoptlocal(\etaglob; \t) := \argmin_{\etalocal} \KL{(\eta_\gamma, \etalocal), \t}.
\end{align*}
With this minimizer available in closed-form, we can define
\begin{align}\eqlabel{kl_global}
\KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptlocal(\eta_\gamma; \t)), \t\Big),
\end{align}
which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum.

\begin{ex}[Optimalility of $\etalocal$ in a GMM]\exlabel{qz_optimality}
Fixing $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$,
the optimal $\etaopt_{\z_\n}$ must satisfy
%
\begin{align*}
& \q(\z_\n | \etaopt_{\z_\n}) \propto \exp\left(\tilde \rho_{\n\k}\right)\\
& \mathwhere \tilde \rho_{\n\k} := \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
See \citet{bishop:2006:PRML} and \citet{blei:2017:vi_review} for details.
Recall from \exref{qz_form}
that the multinomial distribution on $\z_\n$
is parameterized using the natural exponential family parameterization,
with parameters $\etaopt_{\z_\n}$.
To satisfy this optimality condition,
we set $\etaopt_{\z_\n}$ to be
%
\begin{align*}
%
\etaopt_{\z_\n} = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}},
\log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots,
\log\frac{\tilde\rho_{\n(\kmax-1)}}{\tilde\rho_{\n\kmax}}\right).
%
\end{align*}
%
Thus, as long as the expectation $\tilde\rho_{\n\k}$ has a closed-form as a function of
$(\etabeta, \etanu)$, the optimal $\etaopt_{\z_\n}$ can be also be set in closed-form as
a function of $(\etabeta, \etanu)$.
%
\end{ex}



Rather than optimizing the $\mathrm{KL}$ over all variational parameters, both global and local,
we optimize $\KLglobal$ (\eqref{kl_global}), which is a function of global parameters only.
Minimizing $\KLglobal(\etaglob)$ keeps the
dimension of the optimization parameter independent of the number of data points.
After the optimization terminates at an optimal $\etaoptglob$,
the optimal local parameters $\etaoptlocal$ can be set in closed form
to produce the entire vector of optimal variational parameters $\etaopt = (\etaoptglob, \etaoptlocal)$.
The construction of $\KLglobal$ will also play an
important role in computing the local sensitivity in practice,
which we now detail.


% Using the optimality of $\etaoptz(\eta_\gamma; \t)$ and applying the chain rule, one can check that
% \begin{align}\eqlabel{global_kl}
% \hessopt_{\gamma} &=
% \frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
% \KLglobal(\etaopt_\gamma, 0) \\
% & \mathwhere
% % \mathrm{KL}_{glob}(\eta_\gamma) := \evalat{\KL{\eta, \t}}{\eta = (\eta_\gamma, \etaoptz(\eta_\gamma; t))}.
% \KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptz(\eta_\gamma; \t)), \t\Big).
% \notag
% \end{align}

% the global latent variables $\gamma := (\nu, \beta)$.
% We also defined local variational parameters $\eta_\ell$ which parameterize
% the local latent variables $\z$ (or $(\z, \b)$ in the regression model).
% Instead of optimizing


\subsubsection{The Hessian inversion}

Typically, it is the computation and inversion of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
the dimension of $\eta$, is large.
The dimension of $\etaglob$ scales with $\kmax$,
while the dimension of $\etalocal$ scales with the number
of data points $\N$.
Depending on the size of $\eta$, instantiating the full Hessian in memory
may be impossible.

We again take advantage of the fact that the latent variables
factorize into a set of global and local latent variables.
For generic latent variables $a$ and $b$,
let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
\eta_b^T}{\etaopt(0), t=0}$, the Hessian with respect to the variational
parameters governing $a$ and $b$.
We decompose the Hessian matrix $\hessopt$ into four blocks:
%
\begin{align*}
%
\hessopt =
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \eta^T}
       {\etaopt(0), t= 0} ={}&
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right).
%
\end{align*}

Next, let $\crosshessian_\gamma$ be the components of
$\crosshessian$ corresponding to the variational parameters
$\etaglob$---that is, $\crosshessian_\gamma$ is given by replacing
the operator $\nabla_\eta$ with $\nabla_{\eta_\gamma}$ in \eqref{sens_mixed_partial}.
The analogous quantity for the local parameters, $\crosshessian_\ell$,
is zero, since no local variables enter the expectation in \eqref{sens_mixed_partial}.
We can thus write
\begin{align*}
  \crosshessian = \left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right).
  %
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right),
%
\end{align*}
%
and an application of the Schur complement gives
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\ell\ell}^{-1} \hess{\ell\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right)^{-1} \crosshessian_\gamma,
%
\end{align*}
where $I_{\gamma\gamma}$ is the identity matrix with
the same dimension as $\eta_\gamma$.

Specifically, observe that the sensitivity of the global parameters
is given by
\begin{align}\eqlabel{global_sens}
  \fracat{d \etaopt_\gamma(\t)}{d \t}{t = 0} &=
  - \hessopt_\gamma^{-1}\crosshessian_\gamma
  \mathwhere
  \hessopt_\gamma := \left(\hess{\gamma\gamma} -
        \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right).
\end{align}


Each term of $\hessopt_\gamma$ can be easily computed using automatic differentiation.
In fact, the $\hess{\ell\ell}$ is block-diagonal and has a closed form inverse.

Alternatively, using the optimality of $\etaoptz(\eta_\gamma; \t)$ and
applying the chain rule, one can check that
\begin{align}\eqlabel{global_kl}
\hessopt_{\gamma} &=
\frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
\KLglobal(\etaopt_\gamma, 0)
\end{align}

\eqref{global_kl} is how we calculate $\hessopt_\gamma$ in practice:
we implement $\mathrm{KL}_{glob}$, which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum;
we then use automatic differentiation to
compute the Hessian of $\mathrm{KL}_{glob}$
with respect to the global parameters $\eta_\gamma$.

Crucially, the size of $\hessopt_\gamma$ scales with $(\kmax)^2$,
while the full Hessian scales with both $(\kmax)^2$
and the squared number of data points $\N^2$
Hence, $\hessopt_\gamma$ is more memory efficient to store
and faster to invert
than the full Hessian of all the variational parameters.


Now that we can compute the sensitivity of the global parameters in
\eqref{global_sens}, we produce its linear approximation:
\begin{align}\eqlabel{global_lin_approx}
  \etalin_\gamma(\t) := \etaopt_\gamma +
  \fracat{d \etaopt_\gamma(\t)}{d \t}{\t=0} \t .
\end{align}

Finally, given a posterior quantity $\g$,
we again take advantage of the fact that the optimal
local parameters can be found in closed form given global parameters.
In the same way that $\KLglobal$ implicitly sets the local parameters at their optimum
and is a function of only global parameters and the prior parameter $\t$,
we can construct an analogous mapping for $\g$,
\begin{align}\eqlabel{g_as_global}
(\t, \etaglob) \mapsto g\Big(\big(\etaglob, \etaoptz(\etaglob, \t))\Big).
\end{align}

This mapping can be used for any posterior quantity.
Therefore, linearizing the global parameters using \eqref{global_sens, global_lin_approx} is sufficient;
we do not need to invert the full Hessian
and linearize the entire set of variational parameters, global and local.
As a shorthand, we will use $\g(\etalin_\gamma(t))$ to
denote the posterior computed under our linearized global parameters
at prior parameter $\t$ using the mapping \eqref{g_as_global}.


% We have the option of again taking advantage of the fact that the optimal
% local parameters can be found in closed form given global parameters
% to compute any posterior statistic $\g$.
% In the same way that
%
%
%
% We assume that $\etaoptz(\eta_\gamma; \t)$ is inexpensive to compute.
% Then, instead of linearizing both local and variational parameters,
% we linearize only the global parameters and approximate
% the full set of variational parameters at $\t \not= 0$ using
% %
% \begin{align}\eqlabel{global_lin_approx_all}
% \etalinglobal(\t) = \left(\begin{array}{c} \etalin_\gamma(\t) \\
% \etaoptz(\etalin_\gamma(\t); \t)
% \end{array}\right).
% \end{align}
%
% The advantage to $\etalinglobal$ is that the Hessian
% required for forming the approximation does not scale with $\N$.
% Note also that the $\mathrm{KL}$ evaluated at $\etalinglobal$
% will always be smaller than the $\mathrm{KL}$ evaluated at $\etalin$ obtained by
% linearizing all variational parameters,
% and thus $\etalinglobal$ may be a better approximator
% to the actual refitted variational parameters.
%
%
%


% \subsection*{old text}
%
%
%
%
%
% % Moreover, the $\hess{\z\z}$ term is block-diagonal with $N$ blocks of size $\kmax\times\kmax$;
% % the $n$-th block corresponds to the variational parameters governing $z_n$.
% % Each block, and resulting matrix $\hess{\z\z}$, has a closed form inverse.
%
%
% % However, this Hessian matrix does not depend on $\t$, and
% % so can be computed once and re-used for many different values of $\t$ or many
% % different classes of prior perturbations. In our case, w
%
%   Fortunately, the Hessian of the KL divergence
% is sparse and highly structured in $\etaz$.  In particular, it is
% block-diagonal, with
% %
% \begin{align*}
% %
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta_{\z_{n}} \partial \eta_{\z_{m}}^T}
%        {\etaopt(t = 0), \t_0} ={}& 0 \mathtxt{for}n\ne m.
% %
% \end{align*}
% %
% Recall from \exref{qz_unconstrained} that $\eta_{\z_\n}$ is of dimension $\kmax -
% 1$, and that the Hessian of $\expect{\q(\zeta \vert \eta)}{\logp(\x \vert
% \zeta)}$ with respect to $\etaz$ may be nonzero despite $\logp(\x \vert \zeta)$
% being linear in $\z$ due to the unconstrained parameterization of $\etaz$.
%
% We propose the following techniques for computing the inverse Hessian when the
% fully matrix is prohibitively large.  To describe the techniques it will be
% useful to use the following compact notation.  Let $\gamma = (\beta, \nu)$
% denote all the parameters besides $\z$, and for generic parameters $a$ and $b$
% let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
% \eta_b^T}{\etaopt(\t_0), \t_0}$, the Hessian with respect to the variational
% parameters governing $a$ and $b$.  Specifically:
% %
% \begin{align*}
% %
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta \partial \eta^T}
%        {\etaopt(\t_0), \t_0} ={}&
% \hess{\zeta\zeta} =
% \left(
% \begin{array}{cc}
%    \hess{\gamma\gamma} & \hess{\gamma\z} \\
%    \hess{\z\gamma}     & \hess{\z\z} \\
% \end{array}
% \right).
% %
% \end{align*}
% %
% The terms $\hess{\gamma\gamma}$ and $\hess{\z\gamma} = \hess{\gamma\z}^T$ are
% typically dense and easily computed using atuomatic differentiation, and the
% term $\hess{\z\z}$ is block diagonal with a closed-form inverse.
% %
% Analogously, let
% %
% \begin{align*}
% %
% \hess{\zeta\t} :=
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta \partial \t}
%        {\etaopt(\t_0), \t_0},
% %
% \end{align*}
% %
% and observe that
% %
% \begin{align*}
% %
% \hess{\zeta\t} ={}& \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right).
% %
% \end{align*}
%
% In this notation,
% %
% \begin{align*}
% %
% \fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
% -\left(
% \begin{array}{cc}
%    \hess{\gamma\gamma} & \hess{\gamma\z} \\
%    \hess{\z\gamma}     & \hess{\z\z} \\
% \end{array}
% \right)^{-1}
% \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right)
% %
% \end{align*}
% %
% We can then use the Schur complement to get the computationally tractable
% expression:
% %
% \begin{align*}
% %
% \fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
% -\left(\begin{array}{c}
% I_{\gamma\gamma} \\
% \hess{\z\z}^{-1} \hess{\z\gamma}
% \end{array}\right)
% \left(\hess{\gamma\gamma} -
%       \hess{\gamma\z} \hess{\z\z}^{-1} \hess{\z\gamma}\right)^{-1} \hess{\gamma\t}.
% %
% \end{align*}
%
% In fact, the former expression can be computed entirely using automatic
% differentiation using the fact that $\etaoptz$ has an explicit closed form given
% $\etaoptgamma$, as dicussed in \exref{qz_form}.  TODO: is this worth writing
% here?
%
% When even $\hess{\gamma\gamma}$ is too large to form in memory, one can
% use the conjugate gradient algorithm.  TODO: fill in details.
%
%
%
% %
% %  we can evaluate the mixed partial using the derivative of
% % \eqref{gh_integral}.  We will consider particular functional forms for
% % $\t \mapsto \pstick(\nuk \vert \t)$ in below.
% %
% %
% % In order to evaluate $d\etaopt(\t) / d\t$ as given by \eqref{vb_eta_sens}, we
% % need to solve a linear system involving the $\etadim \times \etadim$ Hessian of
% % the objective function $\KL{\eta, 0}$ and the $\etadim \times 1$ mixed
% % second-order partial derivative of $\KL{\eta, \t}$ with respect to $\eta$ and
% % $\t$.
% % \todo{Theorem 1 isn't in the form of a mixed partial; maybe this needs more explaining.
% % Perhaps we could put \eqref{sens_mixed_partial} and a discussion about this earlier?
% % e.g. after corollary 1?}
% % We will discuss these two tasks in turn.
% %
% % First, consider the mixed partial derivative $\partial^2 \KL{\eta, \t} /
% % \partial \eta \partial \t$.  In our case, only the priors $\log \pstick(\nuk
% % \vert \t)$ depend on $\t$, and so
% % %
% % \begin{align}\eqlabel{sens_mixed_partial}
% % %
% % \fracat{\partial^2 \KL{\eta, \t}}
% %        {\partial \eta \partial \t}
% %        {\etaopt(\t_0), \t_0} ={}&
% % \sum_{\k=1}^{\kmax}
% %     \evalat{
% %         \frac{\partial}{\partial\eta}
% %         \expect{\q(\nuk \vert \eta)}
% %                {\fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t_0}
% %                }
% %         }
% %         {\etaopt(\t_0)}.
% % %
% % \end{align}
% % %
% % Consequently, as long as we can explicitly evaluate $\fracat{\log \pstick(\nuk
% % \vert \t)}{\partial \t}{\t_0} $, either in closed form or with automatic
% % differentiation, we can evaluate the mixed partial using the derivative of
% % \eqref{gh_integral}.  We will consider particular functional forms for
% % $\t \mapsto \pstick(\nuk \vert \t)$ in below.
% %
% % Observe that \eqref{sens_mixed_partial} will be zero for all entries of
% % $\eta$ other than those that parameterize the sticks.
