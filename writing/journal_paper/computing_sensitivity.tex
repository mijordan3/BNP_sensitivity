
\subsection{Evaluating $d\etaopt(\t) / d\t$}\seclabel{evaluating_deriv}

In order to evaluate $d\etaopt(\t) / d\t$ as given by \eqref{vb_eta_sens}, we
need to solve a linear system involving the $\etadim \times \etadim$ Hessian of
the objective function $\KL{\eta, 0}$ and the $\etadim \times 1$ mixed
second-order partial derivative of $\KL{\eta, \t}$ with respect to $\eta$ and
$\t$.  We will discuss these two tasks in turn.

First, consider the mixed partial derivative $\partial^2 \KL{\eta, \t} /
\partial \eta \partial \t$.  In our case, only the priors $\log \pstick(\nuk
\vert \t)$ depend on $\t$, and so
%
\begin{align}\eqlabel{sens_mixed_partial}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \t}
       {\etaopt(\t_0), \t_0} ={}&
\sum_{\k=1}^{\kmax}
    \evalat{
        \frac{\partial}{\partial\eta}
        \expect{\q(\nuk \vert \eta)}
               {\fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t_0}
               }
        }
        {\etaopt(\t_0)}.
%
\end{align}
%
Consequently, as long as we can explicitly evaluate $\fracat{\log \pstick(\nuk
\vert \t)}{\partial \t}{\t_0} $, either in closed form or with automatic
differentiation, we can evaluate the mixed partial using the derivative of
\eqref{gh_integral}.  We will consider particular functional forms for
$\t \mapsto \pstick(\nuk \vert \t)$ in \secref{prior_perturbations}.

Observe that \eqref{sens_mixed_partial} will be zero for all entries of
$\eta$ other than those that parameterize the sticks.

Typically, it is the computation and factorization of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
$\etadim$ is large.  However, this Hessian matrix does not depend on $\t$, and
so can be computed once and re-used for many different values of $\t$, or many
different classes of prior perturbations.  In our case, we have written
$\KL{\eta, \t_0}$ as an explicit function of $\eta$.  Consequently, we can
compute either the Hessian itself or Hessian-vector products using automatic
differentiation CITE.

The Hessian matrix may be quite large, since it involves the VB parameters for
all the random effects.  In particular, though the dimensions of $\etanu$ and
$\etatheta$ are of order $\kmax$, the dimension of $\etaz$ is of order $\kmax
\times\N$.  For large $\N$, it may not be possible to instantiate the fully
dense Hessian matrix in memory.  Fortunately, the Hessian of the KL divergence
is sparse and highly structured in $\etaz$.  In particular, it is
block-diagonal, with
%
\begin{align*}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta_{\z_{n}} \partial \eta_{\z_{m}}^T}
       {\etaopt(\t_0), \t_0} ={}& 0 \mathtxt{for}n\ne m.
%
\end{align*}
%
Recall from \exref{qz_unconstrained} that $\eta_{\z_\n}$ is of dimension $\kmax -
1$, and that the Hessian of $\expect{\q(\zeta \vert \eta)}{\logp(\x \vert
\zeta)}$ with respect to $\etaz$ may be nonzero despite $\logp(\x \vert \zeta)$
being linear in $\z$ due to the unconstrained parameterization of $\etaz$.

We propose the following techniques for computing the inverse Hessian when the
fully matrix is prohibitively large.  To describe the techniques it will be
useful to use the following compact notation.  Let $\gamma = (\beta, \nu)$
denote all the parameters besides $\z$, and for generic parameters $a$ and $b$
let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
\eta_b^T}{\etaopt(\t_0), \t_0}$, the Hessian with respect to the variational
parameters governing $a$ and $b$.  Specifically:
%
\begin{align*}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \eta^T}
       {\etaopt(\t_0), \t_0} ={}&
\hess{\zeta\zeta} =
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\z} \\
   \hess{\z\gamma}     & \hess{\z\z} \\
\end{array}
\right).
%
\end{align*}
%
The terms $\hess{\gamma\gamma}$ and $\hess{\z\gamma} = \hess{\gamma\z}^T$ are
typically dense and easily computed using atuomatic differentiation, and the
term $\hess{\z\z}$ is block diagonal with a closed-form inverse.
%
Analogously, let
%
\begin{align*}
%
\hess{\zeta\t} :=
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \t}
       {\etaopt(\t_0), \t_0},
%
\end{align*}
%
and observe that
%
\begin{align*}
%
\hess{\zeta\t} ={}& \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right).
%
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\z} \\
   \hess{\z\gamma}     & \hess{\z\z} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right)
%
\end{align*}
%
We can then use the Schur complement to get the computationally tractable
expression:
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\z\z}^{-1} \hess{\z\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\z} \hess{\z\z}^{-1} \hess{\z\gamma}\right)^{-1} \hess{\gamma\t}.
%
\end{align*}

In fact, the former expression can be computed entirely using automatic
differentiation using the fact that $\etaoptz$ has an explicit closed form given
$\etaoptgamma$, as dicussed in \exref{qz_form}.  TODO: is this worth writing
here?

When even $\hess{\gamma\gamma}$ is too large to form in memory, one can
use the conjugate gradient algorithm.  TODO: fill in details.
