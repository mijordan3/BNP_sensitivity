Before computing any sensitivity measures, we first need to find $\etaopt(0)$
by optimizing the $\mathrm{KL}$ objective (\eqref{vb_optimization}).
In the optimization, we take advantage of the global/local structure in our models
(\secref{vb_conjugacy}).
Recall that we partitioned the variational parameter vector $\eta$
into global variational parameters $\etaglob$ and local variational
parameters $\etalocal$.
The global variational parameters govern the distribution on
latent variables shared across multiple data points,
such as stick-breaking proportions $\nu$ and component variables $\beta$;
the local variational parameters govern the collection of latent variables
unique to each data point, such as the cluster assignments $\z$.

In \secref{vb_conjugacy}, we noted that the optimal local variational parameters
$\etaoptlocal$ can be written
as a closed-form function of the global variational parameters $\etaglob$.
Let $\etaoptlocal(\eta_\gamma; \t)$ denote this mapping; that is,
\begin{align*}
  \etaoptlocal(\etaglob; \t) := \argmin_{\etalocal} \KL{(\eta_\gamma, \etalocal), \t}.
\end{align*}
With this minimizer available in closed-form, we can define
\begin{align}\eqlabel{kl_global}
\KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptlocal(\eta_\gamma; \t)), \t\Big),
\end{align}
which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum.

Rather than optimizing the $\mathrm{KL}$ over all variational paramters, both global and local,
we optimize $\KLglobal$, which is a function of global parameters only.
Minimizing $\KLglobal(\etaglob)$ keeps the
dimension of the optimization parameter independent of the number of data points.
After the optimzation terminates at an optimal $\etaoptglob$,
the optimal local parameters $\etaoptlocal$ can be set in closed form
to produce the entire vector of optimal variational parameters $\etaopt = (\etaoptglob, \etaoptlocal)$.
The construction of $\KLglobal$ will play a role in computing our sensitivity measures in practice,
as we detail below.


% Using the optimality of $\etaoptz(\eta_\gamma; \t)$ and applying the chain rule, one can check that
% \begin{align}\eqlabel{global_kl}
% \hessopt_{\gamma} &=
% \frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
% \KLglobal(\etaopt_\gamma, 0) \\
% & \mathwhere
% % \mathrm{KL}_{glob}(\eta_\gamma) := \evalat{\KL{\eta, \t}}{\eta = (\eta_\gamma, \etaoptz(\eta_\gamma; t))}.
% \KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptz(\eta_\gamma; \t)), \t\Big).
% \notag
% \end{align}

% the global latent variables $\gamma := (\nu, \beta)$.
% We also defined local variational parameters $\eta_\ell$ which parameterize
% the local latent variables $\z$ (or $(\z, \b)$ in the regression model).
% Instead of optimizing

\subsubsection*{the cross-hessian}

\todo{Move the discussion about the cross-hessian earlier: maybe after corollary 1.}

Define
\begin{align*}
\crosshessian :=   \expect{\q(\theta \vert \etaopt)}{
      \lqgradbar{\theta \vert \etaopt}
      \fracat{\partial \log \p(\theta \vert \t)}{\partial \t}{\t=0}
  }
\end{align*}

\todo{maybe define $\crosshessian$ in theorem 1?}

We assume that $\q(\theta \vert \eta)$ is normalized, so
$\lqgradbar{\theta \vert \etaopt} = \lqgrad{\theta \vert \etaopt}$.

In our case, only the prior on stick-breaking proportions
$\nu = (\nu_1, ..., \nu_{\kmax - 1})$ depends on $t$.
Because the elements of $\nu$ fully factorize
under both the prior and the variational distributions,
$\crosshessian$ decomposes as
\begin{align}
  \crosshessian &=
  \sum_{\k=1}^{\kmax - 1}
          \expect{\q(\nuk \vert \eta)}
                 {
                 \lqgrad{\nuk \vert \etaopt}
                 \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                 } \notag\\
  &= \sum_{\k=1}^{\kmax - 1}
         \evalat{\nabla_\eta \expect{\q(\nuk \vert \eta)}
                {
                \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                }}{\eta = \etaopt(0)}
\eqlabel{sens_mixed_partial}
\end{align}
\todo{say why can we exchange derivatives and expectations here}

We approximate the expectation using GH quadrature (\eqref{gh_integral}),
with
% $f(\nuk) = \frac{\partial}{\partial t}[\log \pstick(\nuk \vert \t)]\vert_{t =0}$.
$f(\nu_k) = \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}$.
In all the functional forms for
$\t \mapsto \pstick(\nuk \vert \t)$ considered below,
$f(\nu_k)$ can be provided in either closed-form or computed with automatic differentiation.
The resulting GH approximation is a deteriminstic function of $\eta$,
and thus the gradient in \eqref{sens_mixed_partial} can be computed
with another application of automatic differentiation.
Note that $\crosshessian$ is sparse in \eqref{sens_mixed_partial}:
it is zero for all entries of
$\eta$ other than those that parameterize the sticks.

\subsubsection*{The hessian}

Typically, it is the computation and inversion of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
the dimension of $\eta$, is large.
The dimension of $\etaglob$ scales with $\kmax$,
while the dimension of $\etalocal$ scales with $\kmax
\times\N$. (the scaling of the topic model is slightly different; but the point remains
that the dimension of $\etalocal$ grows faster than the dimension of $\etaglob$).
Depending on the size of $\eta$, instatiating the full Hessian in memory
may be impossible.

We again take advantage of the fact that the latent variables
factorize into a set of global and local latent variables.
For generic latent variables $a$ and $b$,
let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
\eta_b^T}{\etaopt(0), t=0}$, the Hessian with respect to the variational
parameters governing $a$ and $b$.
We decompose the Hessian matrix $\hessopt$ into four blocks:
%
\begin{align*}
%
\hessopt =
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \eta^T}
       {\etaopt(0), t= 0} ={}&
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right).
%
\end{align*}

Next, let $\crosshessian_\gamma$ be the components of
$\crosshessian$ corresponding to the variational parameters
$\etaglob$---that is, $\crosshessian_\gamma$ is given by replacing
the operator $\nabla_\eta$ with $\nabla_{\eta_\gamma}$ in \eqref{sens_mixed_partial}.
The analagous quantity for the local parameters, $\crosshessian_\ell$,
is zero, since no local variables enter the expectation in \eqref{sens_mixed_partial}.
We can thus write
\begin{align*}
  \crosshessian = \left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right).
  %
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right),
%
\end{align*}
%
and an application of the Schur complement gives
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\ell\ell}^{-1} \hess{\ell\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right)^{-1} \crosshessian_\gamma,
%
\end{align*}
where $I_{\gamma\gamma}$ is the identity matrix with
the same dimension as $\eta_\gamma$.

Specifically, observe that the sensitivity of the global parameters
is given by
\begin{align}\eqlabel{global_sens}
  \fracat{d \etaopt_\gamma(\t)}{d \t}{t = 0} &=
  - \hessopt_\gamma^{-1}\crosshessian_\gamma
  \mathwhere
  \hessopt_\gamma := \left(\hess{\gamma\gamma} -
        \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right).
\end{align}


Each term of $\hessopt_\gamma$ can be easily computed using automatic differentiation.
In fact, the $\hess{\ell\ell}$ is block-diagonal and has a closed form inverse.

Alteratively, using the optimality of $\etaoptz(\eta_\gamma; \t)$ and
applying the chain rule, one can check that
\begin{align}\eqlabel{global_kl}
\hessopt_{\gamma} &=
\frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
\KLglobal(\etaopt_\gamma, 0)
\end{align}

\eqref{global_kl} is how we calculate $\hessopt_\gamma$ in practice:
we implement $\mathrm{KL}_{glob}$, which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum;
we then use automatic differentiation to
compute the Hessian of $\mathrm{KL}_{glob}$
with respect to the global parameters $\eta_\gamma$.

Crucially, the size of $\hessopt_\gamma$ scales with $(\kmax)^2$,
while the full Hessian scales with both $(\kmax)^2$
and the squared number of data points $\N^2$
(in the topic model, the Hessians scale as $(\N\kmax)^2$ and $(\N\kmax\nloci)^2)$).
Hence, $\hessopt_\gamma$ is more memory efficient to store
and faster to invert
than the full Hessian of all the variational parameters.


Now that we can compute the sensitivity of the global parameters in
\eqref{global_sens}, we produce its linear approximation:
\begin{align}\eqlabel{global_lin_approx}
  \etalin_\gamma(\t) := \etaopt_\gamma +
  \fracat{d \etaopt_\gamma(\t)}{d \t}{\t=0} \t .
\end{align}

Finally, given a posterior quantity $\g$,
we again take advantage of the fact that the optimal
local parameters can be found in closed form given global parameters.
In the same way that $\KLglobal$ implicitly sets the local parameters at their optimum
and is a function of only global parameters and the prior parameter $\t$,
we can construct an analagous mapping for $\g$,
\begin{align}\eqlabel{g_as_global}
(\t, \etaglob) \mapsto g\Big(\big(\etaglob, \etaoptz(\etaglob, \t))\Big).
\end{align}

This mapping can be used for any posterior quantity.
Therefore, linearizing the global parameters using \eqref{global_sens, global_lin_approx} is sufficient;
we do not need to invert the full Hessian
and linearize the entire set of variational parameters, global and local.
As a shorthand, we will use $\g(\etalin_\gamma(t))$ to
denote the posterior computed under our linearized global parameters
at prior parameter $\t$ using the mapping \eqref{g_as_global}.


% We have the option of again taking advantage of the fact that the optimal
% local parameters can be found in closed form given global parameters
% to compute any posterior statistic $\g$.
% In the same way that
%
%
%
% We assume that $\etaoptz(\eta_\gamma; \t)$ is inexpensive to compute.
% Then, instead of linearizing both local and variational parameters,
% we linearize only the global parameters and approximate
% the full set of variational parameters at $\t \not= 0$ using
% %
% \begin{align}\eqlabel{global_lin_approx_all}
% \etalinglobal(\t) = \left(\begin{array}{c} \etalin_\gamma(\t) \\
% \etaoptz(\etalin_\gamma(\t); \t)
% \end{array}\right).
% \end{align}
%
% The advantage to $\etalinglobal$ is that the Hessian
% required for forming the approximation does not scale with $\N$.
% Note also that the $\mathrm{KL}$ evaluated at $\etalinglobal$
% will always be smaller than the $\mathrm{KL}$ evaluated at $\etalin$ obtained by
% linearizing all variational parameters,
% and thus $\etalinglobal$ may be a better approximator
% to the actual refitted variational parameters.
%
%
%


% \subsection*{old text}
%
%
%
%
%
% % Moreover, the $\hess{\z\z}$ term is block-diagonal with $N$ blocks of size $\kmax\times\kmax$;
% % the $n$-th block corresponds to the variational parameters governing $z_n$.
% % Each block, and resulting matrix $\hess{\z\z}$, has a closed form inverse.
%
%
% % However, this Hessian matrix does not depend on $\t$, and
% % so can be computed once and re-used for many different values of $\t$ or many
% % different classes of prior perturbations. In our case, w
%
%   Fortunately, the Hessian of the KL divergence
% is sparse and highly structured in $\etaz$.  In particular, it is
% block-diagonal, with
% %
% \begin{align*}
% %
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta_{\z_{n}} \partial \eta_{\z_{m}}^T}
%        {\etaopt(t = 0), \t_0} ={}& 0 \mathtxt{for}n\ne m.
% %
% \end{align*}
% %
% Recall from \exref{qz_unconstrained} that $\eta_{\z_\n}$ is of dimension $\kmax -
% 1$, and that the Hessian of $\expect{\q(\zeta \vert \eta)}{\logp(\x \vert
% \zeta)}$ with respect to $\etaz$ may be nonzero despite $\logp(\x \vert \zeta)$
% being linear in $\z$ due to the unconstrained parameterization of $\etaz$.
%
% We propose the following techniques for computing the inverse Hessian when the
% fully matrix is prohibitively large.  To describe the techniques it will be
% useful to use the following compact notation.  Let $\gamma = (\beta, \nu)$
% denote all the parameters besides $\z$, and for generic parameters $a$ and $b$
% let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
% \eta_b^T}{\etaopt(\t_0), \t_0}$, the Hessian with respect to the variational
% parameters governing $a$ and $b$.  Specifically:
% %
% \begin{align*}
% %
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta \partial \eta^T}
%        {\etaopt(\t_0), \t_0} ={}&
% \hess{\zeta\zeta} =
% \left(
% \begin{array}{cc}
%    \hess{\gamma\gamma} & \hess{\gamma\z} \\
%    \hess{\z\gamma}     & \hess{\z\z} \\
% \end{array}
% \right).
% %
% \end{align*}
% %
% The terms $\hess{\gamma\gamma}$ and $\hess{\z\gamma} = \hess{\gamma\z}^T$ are
% typically dense and easily computed using atuomatic differentiation, and the
% term $\hess{\z\z}$ is block diagonal with a closed-form inverse.
% %
% Analogously, let
% %
% \begin{align*}
% %
% \hess{\zeta\t} :=
% \fracat{\partial^2 \KL{\eta, \t}}
%        {\partial \eta \partial \t}
%        {\etaopt(\t_0), \t_0},
% %
% \end{align*}
% %
% and observe that
% %
% \begin{align*}
% %
% \hess{\zeta\t} ={}& \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right).
% %
% \end{align*}
%
% In this notation,
% %
% \begin{align*}
% %
% \fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
% -\left(
% \begin{array}{cc}
%    \hess{\gamma\gamma} & \hess{\gamma\z} \\
%    \hess{\z\gamma}     & \hess{\z\z} \\
% \end{array}
% \right)^{-1}
% \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right)
% %
% \end{align*}
% %
% We can then use the Schur complement to get the computationally tractable
% expression:
% %
% \begin{align*}
% %
% \fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
% -\left(\begin{array}{c}
% I_{\gamma\gamma} \\
% \hess{\z\z}^{-1} \hess{\z\gamma}
% \end{array}\right)
% \left(\hess{\gamma\gamma} -
%       \hess{\gamma\z} \hess{\z\z}^{-1} \hess{\z\gamma}\right)^{-1} \hess{\gamma\t}.
% %
% \end{align*}
%
% In fact, the former expression can be computed entirely using automatic
% differentiation using the fact that $\etaoptz$ has an explicit closed form given
% $\etaoptgamma$, as dicussed in \exref{qz_form}.  TODO: is this worth writing
% here?
%
% When even $\hess{\gamma\gamma}$ is too large to form in memory, one can
% use the conjugate gradient algorithm.  TODO: fill in details.
%
%
%
% %
% %  we can evaluate the mixed partial using the derivative of
% % \eqref{gh_integral}.  We will consider particular functional forms for
% % $\t \mapsto \pstick(\nuk \vert \t)$ in below.
% %
% %
% % In order to evaluate $d\etaopt(\t) / d\t$ as given by \eqref{vb_eta_sens}, we
% % need to solve a linear system involving the $\etadim \times \etadim$ Hessian of
% % the objective function $\KL{\eta, 0}$ and the $\etadim \times 1$ mixed
% % second-order partial derivative of $\KL{\eta, \t}$ with respect to $\eta$ and
% % $\t$.
% % \todo{Theorem 1 isn't in the form of a mixed partial; maybe this needs more explaining.
% % Perhaps we could put \eqref{sens_mixed_partial} and a discussion about this earlier?
% % e.g. after corollary 1?}
% % We will discuss these two tasks in turn.
% %
% % First, consider the mixed partial derivative $\partial^2 \KL{\eta, \t} /
% % \partial \eta \partial \t$.  In our case, only the priors $\log \pstick(\nuk
% % \vert \t)$ depend on $\t$, and so
% % %
% % \begin{align}\eqlabel{sens_mixed_partial}
% % %
% % \fracat{\partial^2 \KL{\eta, \t}}
% %        {\partial \eta \partial \t}
% %        {\etaopt(\t_0), \t_0} ={}&
% % \sum_{\k=1}^{\kmax}
% %     \evalat{
% %         \frac{\partial}{\partial\eta}
% %         \expect{\q(\nuk \vert \eta)}
% %                {\fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t_0}
% %                }
% %         }
% %         {\etaopt(\t_0)}.
% % %
% % \end{align}
% % %
% % Consequently, as long as we can explicitly evaluate $\fracat{\log \pstick(\nuk
% % \vert \t)}{\partial \t}{\t_0} $, either in closed form or with automatic
% % differentiation, we can evaluate the mixed partial using the derivative of
% % \eqref{gh_integral}.  We will consider particular functional forms for
% % $\t \mapsto \pstick(\nuk \vert \t)$ in below.
% %
% % Observe that \eqref{sens_mixed_partial} will be zero for all entries of
% % $\eta$ other than those that parameterize the sticks.
