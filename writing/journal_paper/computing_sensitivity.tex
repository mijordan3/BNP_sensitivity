
This section outlines practical considerations for computating
the local sensitivity in \thmref{etat_deriv}.
The cross-hessian term $\crosshessian$
\todo{is this actually called a cross hessian? need to give a name}
is inexpensive to compute using automatic differentiation.
Typically, it is the computation and inversion of the Hessian matrix that is
the most expensive step in computing local sensitivity,
especially when the dimension of $\eta$ is large.
To invert the Hessian, we will take advantage of the fact that
all the models we consider in \secref{results} have a global/local factorization.
This factorization will be useful for optimizing the $\mathrm{KL}$ objective
as well.

\subsection{The cross-hessian}

\todo{would this subsection make more sense immediately after \thmref{etat_deriv}?}

We are interested in sensitivity to the stick-breaking distribution,
so in our experiments (\secref{results}),
only the prior terms on stick-breaking proportions
$\nu = (\nu_1, ..., \nu_{\kmax - 1})$ will depend on $t$.
Because the elements of $\nu$ fully factorize
under both the prior and the variational distributions,
$\crosshessian$ decomposes as
\begin{align}
  \crosshessian &=
  \sum_{\k=1}^{\kmax - 1}
          \expect{\q(\nuk \vert \eta)}
                 {
                 \lqgrad{\nuk \vert \etaopt}
                 \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                 } \notag\\
  &= \sum_{\k=1}^{\kmax - 1}
         \evalat{\nabla_\eta \expect{\q(\nuk \vert \eta)}
                {
                \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                }}{\eta = \etaopt(0)},
\eqlabel{sens_mixed_partial}
\end{align}
where we assumed that $\q(\theta \vert \eta)$ is normalized, so
$\lqgradbar{\theta \vert \etaopt} = \lqgrad{\theta \vert \etaopt}$,
and that the assumptions of \thmref{etat_deriv} hold, so we
can freely exchange derivatives with expectations.

We approximate the expectation using GH quadrature (\appref{gh_quadrature}),
with the integrand $f$ in \eqref{gh_integral} being
$f(\nu_k) = \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}$.
In all the functional forms for
$\t \mapsto \pstick(\nuk \vert \t)$ considered below,
$f(\nu_k)$ can be provided in either closed-form or computed with automatic differentiation.
The resulting GH approximation is a deterministic function of $\eta$,
and thus the gradient in \eqref{sens_mixed_partial} can be computed
with another application of automatic differentiation.
Note that $\crosshessian$ is sparse in \eqref{sens_mixed_partial}:
it is zero for all entries of
$\eta$ other than those that parameterize the sticks.

\subsection{A global and local factorization}

To compute the local sensitivity in practice, we
make use of a factorization common to all the models we will consider:
the latent variables can be partitioned into a set of \textit{global}
variables and a set of \textit{local} latent variables.
The global variables are common to all data points, while the local variables
are unique to each data point.
In the GMM example, the global variables are $(\beta, \nu)$,
while $\z_1, ..., \z_\N$ are local variables.

Let $\gamma$ be the collection of global latent variables
and let $\etaglob$ be their variational parameters.
Similarly, let $\ell$ be the local latent variables and
let $\etalocal$ be the local variational parameters.
Notice that because the local latent variables are unique to each data point,
the dimension of $\etalocal$ scales with $\N$.
The global parameter $\etaglob$ does not scale with $\N$---in the GMM example,
$\etaglob$ scales only with $\kmax$.

In all models we will consider,
the optimal local variational parameters $\etaoptlocal$ can be written
as a closed-form function of the global variational parameters $\etaglob$.
Let $\etaoptlocal(\eta_\gamma; \t)$ denote this mapping; that is,
\begin{align}\eqlabel{local_eta_optim}
  \etaoptlocal(\etaglob; \t) := \argmin_{\etalocal} \KL{(\eta_\gamma, \etalocal), \t}.
\end{align}

The next example details this mapping for the Gaussian mixture model.

\begin{ex}[Optimalility of $\etalocal$ in a GMM]\exlabel{qz_optimality}
Recall that under our truncated variational approximation,
the cluster assignment $\z_\n$ is a discrete random variable
over $\kmax$ categories.

Let $\eta_{\z_\n}$ be the categorical parameters in its
exponential family natural parameterization.
That is, we let $\eta_{\z_\n} = (\rho_{\n1}, \rho_{\n2}, ..., \rho_{\n(\kmax-1)})$
be an unconstrained vector in $\mathbb{R}^{\kmax-1}$;
in this parameterization, the assignment probabilities are
%
\begin{align*}
  p_{\n\k} := \expect{\q(\z_\n \vert \etaz)}{\z_{\n\k}} =
  \frac{\exp(\rho_{\n\k})}{1 + \sum_{\k'=1}^{\kmax-1}\exp(\rho_{\n\k})}
\end{align*}
%
We use the exponential family parameterization because
we require the optimal variational parameters $\etaopt$
to be interior to $\etadom$ in \thmref{etat_deriv}.
In the mean parameterization,
$\sum_{\k=1}^\kmax p_{\n\k} = 1$, so the
optimal mean parameters $\hat p_{\n}$ cannot be
interior to $\Delta^{\kmax - 1}$.
On the other hand, $\eta_{\z_\n}$ as defined
is unconstrained in $\mathbb{R}^{\kmax - 1}$.

Fixing $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$,
the optimal $\etaopt_{\z_\n}$ must satisfy
%
\begin{align*}
& \q(\z_\n | \etaopt_{\z_\n}) \propto \exp\left(\tilde \rho_{\n\k}\right)\\
& \mathwhere \tilde \rho_{\n\k} := \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
See \citet{bishop:2006:PRML} and \citet{blei:2017:vi_review} for details.
To satisfy this optimality condition,
we set the optimal $\etaopt_{\z_\n}$ to be
%
\begin{align*}
%
\etaopt_{\z_\n} = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}},
\log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots,
\log\frac{\tilde\rho_{\n(\kmax-1)}}{\tilde\rho_{\n\kmax}}\right).
%
\end{align*}
%
Thus, as long as the expectation $\tilde\rho_{\n\k}$ has a closed-form as a function of
$(\etabeta, \etanu)$, the optimal $\etaopt_{\z_\n}$ can be also be set in closed-form as
a function of $(\etabeta, \etanu)$.
%
\end{ex}

The availability of the minimizer \eqref{local_eta_optim}
in closed-form will be useful for both optimizing the variational parameters
and for computing local sensitivity.
Define
\begin{align}\eqlabel{kl_global}
\KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptlocal(\eta_\gamma; \t)), \t\Big),
\end{align}
which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum.

Rather than optimizing the $\mathrm{KL}$ over all variational parameters, both global and local,
we optimize $\KLglobal$ (\eqref{kl_global}), which is a function of global parameters only.
Minimizing $\KLglobal(\etaglob)$ keeps the
dimension of the optimization parameter independent of the number of data points.
To minimize $\KLglobal(\etaglob)$ in practice,
we run the BFGS algorithm with a loose convergence tolerance
followed by trust-region Newton conjugate gradient \citep[Chapter~7]{nocedal:2006:numerical}
to find a high-quality optimum.
After the optimization terminates at an optimal $\etaoptglob$,
the optimal local parameters $\etaoptlocal$ can be set in closed form
to produce the entire vector of optimal variational parameters $\etaopt = (\etaoptglob, \etaoptlocal)$.

The construction of $\KLglobal$ will also play an
important role in computing the local sensitivity in practice,
which we now detail.


% Using the optimality of $\etaoptz(\eta_\gamma; \t)$ and applying the chain rule, one can check that
% \begin{align}\eqlabel{global_kl}
% \hessopt_{\gamma} &=
% \frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
% \KLglobal(\etaopt_\gamma, 0) \\
% & \mathwhere
% % \mathrm{KL}_{glob}(\eta_\gamma) := \evalat{\KL{\eta, \t}}{\eta = (\eta_\gamma, \etaoptz(\eta_\gamma; t))}.
% \KLglobal(\eta_\gamma, \t) := \mathrm{KL}\Big((\eta_\gamma, \etaoptz(\eta_\gamma; \t)), \t\Big).
% \notag
% \end{align}

% the global latent variables $\gamma := (\nu, \beta)$.
% We also defined local variational parameters $\eta_\ell$ which parameterize
% the local latent variables $\z$ (or $(\z, \b)$ in the regression model).
% Instead of optimizing


\subsection{The Hessian inversion}

Typically, it is the computation and inversion of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
the dimension of $\eta$, is large.
For especially high-dimensional $\eta$,
instantiating the full Hessian in memory
may be impossible.

To invert the Hessian, we take advantage of the fact that the latent variables
factorize into a set of global and local latent variables.
For generic latent variables $a$ and $b$,
let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
\eta_b^T}{\etaopt(0), t=0}$, the Hessian with respect to the variational
parameters governing $a$ and $b$.
We decompose the Hessian matrix $\hessopt$ into four blocks:
%
\begin{align*}
%
\hessopt =
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \eta^T}
       {\etaopt(0), t= 0} ={}&
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right).
%
\end{align*}

Next, let $\crosshessian_\gamma$ be the components of
$\crosshessian$ corresponding to the variational parameters
$\etaglob$---that is, $\crosshessian_\gamma$ is given by replacing
the operator $\nabla_\eta$ with $\nabla_{\eta_\gamma}$ in \eqref{sens_mixed_partial}.
The analogous quantity for the local parameters, $\crosshessian_\ell$,
is zero, since no local variables enter the expectation in \eqref{sens_mixed_partial}.
We can thus write
\begin{align*}
  \crosshessian = \left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right).
  %
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\ell} \\
   \hess{\ell\gamma}     & \hess{\ell\ell} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \crosshessian_\gamma \\ 0 \end{array}\right),
%
\end{align*}
%
and an application of the Schur complement gives
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{t = 0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\ell\ell}^{-1} \hess{\ell\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right)^{-1} \crosshessian_\gamma,
%
\end{align*}
where $I_{\gamma\gamma}$ is the identity matrix with
the same dimension as $\eta_\gamma$.

Specifically, observe that the sensitivity of the global parameters
is given by
\begin{align}\eqlabel{global_sens}
  \fracat{d \etaopt_\gamma(\t)}{d \t}{t = 0} &=
  - \hessopt_\gamma^{-1}\crosshessian_\gamma
  \mathwhere
  \hessopt_\gamma := \left(\hess{\gamma\gamma} -
        \hess{\gamma\ell} \hess{\ell\ell}^{-1} \hess{\ell\gamma}\right).
\end{align}
Each term of $\hessopt_\gamma$ can be easily computed using automatic differentiation.

Alternatively, using the optimality of $\etaoptlocal(\eta_\gamma; \t)$ and
applying the chain rule, one can check that
\begin{align}\eqlabel{global_kl}
\hessopt_{\gamma} &=
\frac{\partial^2}{\partial\eta_\gamma\partial\eta_\gamma^T}
\KLglobal(\etaopt_\gamma, 0)
\end{align}

\eqref{global_kl} is how we calculate $\hessopt_\gamma$ in practice:
we implement $\mathrm{KL}_{glob}$, which returns the $\mathrm{KL}$ as a function of only global parameters
by implicitly setting local parameters at their optimum;
we then use automatic differentiation to
compute the Hessian of $\mathrm{KL}_{glob}$.

Crucially, the size of $\hessopt_\gamma$ scales with the squared dimension
of $\etaglob$---and recall that the unlike $\etalocal$, the dimension of
$\etaglob$ does not scale with the number of data points $\N$.
Hence, on large data sets, $\hessopt_\gamma$ will be much more memory efficient
to store and faster to invert than the full Hessian.

Now that we can compute the sensitivity of the global parameters in
\eqref{global_sens}, we can produce its linear approximation:
\begin{align}\eqlabel{global_lin_approx}
  \etalin_\gamma(\t) := \etaopt_\gamma +
  \fracat{d \etaopt_\gamma(\t)}{d \t}{\t=0} \t .
\end{align}

Finally, given a posterior quantity $\g$,
we again take advantage of the fact that the optimal
local parameters can be found in closed form given global parameters.
In general, $\g$ will be a function of the entire vector of variational parameters.
However, in the same way that $\KLglobal$ implicitly sets the local parameters at their optimum
and is a function of only global parameters and the prior parameter $\t$,
we can construct an analogous mapping for $\g$,
\begin{align}\eqlabel{g_as_global}
(\t, \etaglob) \mapsto g\Big(\big(\etaglob, \etaoptz(\etaglob, \t))\Big).
\end{align}

This mapping can be used for any posterior quantity.
Therefore, linearizing the global parameters using \eqref{global_sens, global_lin_approx} is sufficient:
we do not need to invert the full Hessian
and linearize the entire set of variational parameters, global and local.
As a shorthand, we will use $\g(\etalin_\gamma(t))$ to
denote the posterior computed under our linearized global parameters
at prior parameter $\t$ using the mapping \eqref{g_as_global}.

In practice, we solved the linear system in~\eqref{global_sens} using
the conjugate gradient (CG) algorithm, which requires only Hessian-vector products;
this avoids instatiating the Hessian matrix in memory.
We could have also computed the Hessian and
either factorized it (e.g. with a Cholesky decomposition) or found its inverse directly.
Then, the Hessian inverse (or its Cholesky decomposition) can
be re-used for different perturbations.
In our case, we need to re-solve the linear system (\eqref{global_sens}) with
CG for each choice of prior perturbation (since different choices of prior perturbation
require solving different $\crosshessian$).
On our applications in \secref{results}, the conjugate gradient algorithm was at least
an order of magnitude faster than refitting, and did not pose a meaningful bottleneck
to exploring different perturbations.

The conjugate gradient algorithm along with
the optimizers BFGS and Newton conjugate gradient
are implemented in SciPy~\citep{scipy}.
All necessary derivatives were computed using the Python
automatic differentiation libarary Jax~\citep{jax2018github}.
