We would like to understand how our quantity of interest $g(\etaopt)$ changes
when the concentration parameter or, more generally, the stick-breaking density
$\pstick$ changes. To efficiently compute these changes, we use a first-order
Taylor series approximation in the optimal VB parameters. In this section, we
first give the Taylor series and then show how to compute its terms.

%%
\noindent \textbf{Sensitivity to the concentration parameter.} First, we show
how to approximate the sensitivity of $g(\etaopt)$ to the choice of
concentration parameter $\alpha$. Let $\etaopt(\alpha)$ represent the value of
$\etaopt$ for a particular choice of $\alpha$. For our approximation, we choose
some initial value $\alpha_0$ of the concentration parameter and solve the
optimization problem to compute $\etaopt(\alpha_0)$. We then approximate
$\etaopt(\alpha)$ with the linear approximation $\etalin(\alpha)$, and in turn
approximate $g(\etaopt(\alpha))$ with $g(\etalin(\alpha))$:
%
\begin{align*}
%
\etalin(\alpha) :=
    \etaopt(\alpha_0) +
    \fracat{d\etaopt(\alpha)}{d\alpha}{\alpha_0} (\alpha - \alpha_0)
\mathand
g(\etaopt(\alpha)) \approx g(\etalin(\alpha)).
%
\end{align*}
%
If $\alpha \mapsto \etaopt(\alpha)$ is continuously differentiable, and $g$ is
sufficiently smooth, then we expect $g(\etaopt(\alpha)) \approx
g(\etalin(\alpha))$ when $\abs{\alpha - \alpha_0}$ is small. We will show in
\thmref{bnp_deriv} below that the map $\alpha \mapsto \etaopt(\alpha)$ is
continuously differentiable.
% We show (\thmref{etat_deriv} in \appref{diffable_concentration}) that the map
% $\alpha \mapsto \etaopt(\alpha)$ is continuously differentiable for the VB
% approximation given in \secref{model_vb}.

%%
\noindent \textbf{Sensitivity to the stick-breaking density.} Next, we show how
to approximate the sensitivity of $g(\etaopt)$ to the choice of concentration
stick distribution $\pstick$. Technically, perturbations of $\alpha$ are
perturbations of $\pstick$. But here we consider more general perturbations of
the form of $\pstick$, potentially outside the beta class. To define our
perturbations, let $\ptil$ represent a potentially unnormalized (but
normalizable) density with respect to Lebesgue measure; the same notation
without the tilde will give the normalized density. Now start from an initial
setting of $\pstick$ at $\pbase$; we will typically start from Dirichlet-process
stick-breaking, i.e.\ $\pbase = \betadist{1,\alpha_0}$. Then take any
Lebesgue-measurable function $\phi(\cdot)$ on $[0,1]$. We consider a range of
alternative (potentially unnormalized) stick-breaking forms $\ptil(\cdot \vert
\t)$ defined by
%
\begin{equation} \eqlabel{mult_perturbation}
	\log \ptil(\cdot \vert \t) = \log \pbase(\cdot) + \t \phi(\cdot)
\end{equation}
%
on $[0,1]$. Note that the perturbation applies equally to every stick break
$\nuk$. This style of multiplicative functional perturbation was proposed by
\citet{gustafson:1996:local}; we deviate from \citet{gustafson:1996:local} by
allowing $\phi$ to take on negative values and by considering VB (rather than
MCMC) approximations.

If we now let $\etaopt(t)$ represent the value of $\etaopt$ for a particular
choice of $\ptil(\cdot \vert \t)$, we can form an analogous approximation to the
$\alpha$ case above:
%
\begin{align} \eqlabel{taylor_series_t}
%
% \etaopt(\alpha) \approx
\etalin(\t) :=
    \etaopt(0) +
    \fracat{d\etaopt(t)}{dt}{\t=0} (\t - 0)
\mathand
g(\etaopt(\t)) \approx g(\etalin(\t)).
%
\end{align}
%
As in the $\alpha$ case, \eqref{taylor_series_t} is useful only if the map $t
\mapsto \etaopt(t)$ is continuously differentiable for the chosen $\phi$.  As we
will show in \thmref{bnp_deriv} below, a simple sufficient condition for
differentiability is given in terms of the following norm on the perturbation
$\phi$.

%\noindent \textbf{An infinity norm.}
%First, we define $\norminf{\cdot}$ and a corresponding ball.
%Let $\mu$ be a probability measure on $[0,1]$. Then
\begin{equation} \eqlabel{infty_norm}
    \textrm{Define }
	\norminf{\phi} := \esssup_{\nu_0 \sim \pbase} \abs{\phi(\nu_0)}
    \textrm{ and }
	\quad \ball_\phi(\delta) := \left\{ \phi: \norminf{\phi} <
\delta \right\}.
\end{equation}
%
The set of priors that arise by considering functional perturbations $\phi \in
\ball_\phi(\delta)$ live in a multiplicative band around the original prior,
$\pbase$, as shown in \figref{linf_examples}.  \Thmref{bnp_deriv} below states
that $\t \mapsto \etaopt(\t)$ is continuously differentiable whenever
$\norminf{\phi} < \infty$.  So, for sufficiently smooth $\g$, we expect the
approximation \eqref{taylor_series_t} to be good for small $\t$, given a
particular choice of $\phi$ with $\norminf{\phi} < \infty$.

We next provide some additional justification for why the functional
perturbation given in \eqref{mult_perturbation} is useful. Note first that if we
consider any other distribution $\palt$ for $\pstick$, we can continuously warp
$\pbase$ to $\palt$ by setting $\phi(\cdot) = \log \left( \palt(\cdot) /
\pbase(\cdot) \right)$ so long as $\palt \ll \pbase$, i.e.\ $\palt$ is
absolutely continuous with respect to $\pbase$.
%
However, restricting to $\norminf{\phi} < \infty$ limits the kinds of
alternative priors $\palt$ that can be formed using \eqref{mult_perturbation}.
We show in \lemref{pert_invariance} of \appref{diffable_nonparametric} that
functional perturbations with $\norminf{\phi} < \infty$ yield valid priors.
However, the converse is not true. For instance, perturbing the beta
stick-breaking form by changing $\alpha$ provides a counterexample; see
\exref{beta_inf_norm} of \appref{diffable_nonparametric} for more details.
\todo{Make sure this works with the corrected \defref{prior_nl_pert} }

Next, we will see in \secref{influence_function} that we can compute an
\emph{influence function} to provide an interpretable summary of the effect of
arbitrary changes $\phi$.  Using the influence function and the
$\norminf{\cdot}$ norm, we are able to find a worst-case $\phi$ in
$\ball_\phi(\delta)$.

Finally, while the perturbations above are essentially multiplicative relative
to $\pbase$, one might wonder whether additive perturbations (subject to
renormalization) or other perturbations would work instead; indeed
\citet{gustafson:1996:local} proposes a number of alternative perturbations. We
show in \secref{influence_function} that, among a class of potential functional
perturbations, the one we consider here is the only one that is {\em Fr{\'e}chet
differentiable} -- and thus can be used to safely reason about worst-case
$\phi$.

%%
\noindent \textbf{Computing the terms in the Taylor series.}  It remains to show
that $\alpha \mapsto \etaopt(\alpha)$ and $\t \mapsto \etaopt(\t)$ are
continuously differentiable, and to provide a computable formula for the
derivative.
%
Differentiability naturally requires some regularity conditions on the VB
parameterization and on the optimum.  We state sufficient conditions in the
following \assuref{kl_opt_ok}, which is satisfied for any local optimum of a
smooth, unconstrained parameterization of the variational approximation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assu}\assulabel{kl_opt_ok}
%
Assume that:
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate*}[label=(\arabic*)]
%
    \item \itemlabel{kl_diffable} The map $\eta \mapsto \KL{\eta}$ is twice
    continuously differentiable at $\etaopt$;

    \item\itemlabel{kl_hess} The Hessian matrix $\fracat{\partial^2 \KL{\eta}}
    {\partial \eta \partial \eta^T} {\etaopt}$ is non-singular; and

    \item \itemlabel{kl_opt_interior} There exists an open ball $\ball_\eta
    \subseteq \mathbb{R}^\etadim$ such that $\etaopt \in \ball_\eta \subseteq
    \etadom$.
%
\end{enumerate*}
%
\end{assu}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Our next result gives differentiability as well as a computable formula for the
derivative.
%
\begin{thm}\thmlabel{bnp_deriv}
%
Under the VB approximation given in \secref{model_vb}, let \assuref{kl_opt_ok}
hold.  Take the perturbation given by \eqref{mult_perturbation} with
$\norminf{\phi} < \infty$, or the concentration parameter $\alpha$, identifying
$\varepsilon$ with $\t$ or $\alpha - \alpha_0$, respectively.  Then the map
$\varepsilon \mapsto \etaopt(\varepsilon)$ is continuously differentiable at
$\varepsilon = 0$ with derivative
%
\begin{align}
%
\eqlabel{bnp_vb_eta_sens}
\fracat{d \etaopt(\varepsilon)}{d \varepsilon}{\varepsilon=0} ={}&
    - \hessopt^{-1} \crosshessian, \mathwhere
    \rho_\k(\nuk) := \fracat{\partial \log \ptil(\nu_k \vert \varepsilon)}
            {\partial \varepsilon}{\varepsilon=0}
            ,
\\
%
\hessopt :={}& \fracat{\partial^2 \KL{\eta}}
                      {\partial \eta \partial \eta^T}
                      {\eta = \etaopt},
\quad \lqgradbar{\zeta \vert \eta} :={}
\fracat{\partial \log \q(\zeta \vert \eta)}{\partial \eta}{\eta}, \textrm{ and}\\
    % \lqgrad{\zeta \vert \eta} -
    % \expect{\q(\zeta \vert \eta)}{\lqgrad{\zeta \vert \eta}}, \\
%
\eqlabel{bnp_vb_crosshessian}
\crosshessian :={}&
    \frac{\partial}{\partial\eta}
    \expect{\q(\zeta \vert \eta)}{
        \sum_{k=1}^{\kmax-1}
        \rho_\k(\nuk)
    }
    \Bigg|_{\eta = \etaopt}
={}
    \expect{\q(\zeta \vert \etaopt)}{
          \lqgradbar{\zeta \vert \etaopt}
          \sum_{k=1}^{\kmax-1}
          \rho_\k(\nuk)}.
%
\end{align}
%
\end{thm}
%
\begin{proof}
%
The result follows from \thmref{etat_deriv} of \appref{diffable_parametric},
which states general conditions for the differentiability of VB optima.  We show
in \appref{diffable_concentration, diffable_nonparametric} that the conditions
of \thmref{etat_deriv} are satisfied in the case of our present BNP problem. The
equivalence of the expressions for $\crosshessian$ follow by differentiating
through the expectation; see \appref{proofs} for more details.
% The proof of \thmref{etat_deriv} relies on the differentiability of the
% log densities, the ability to exchange the order of integration and
% differentiation using the dominated convergence theorem, and the implicit
% function theorem applied to the the first order condition of the VB
% objective function.
%
\end{proof}
%

\eqref{vb_eta_sens} requires computation of two terms: $\hessopt^{-1}$ and
$\crosshessian$.  Typically, $\crosshessian$, which is a derivative of a
variational expectation, is straightforward to evaluate: the requisite
expectation is evaluated either in closed form or approximated numerically;
then, in either case, an application of automatic differentiation provides the
gradient. Forming and inverting or factorizing $\hessopt$ can present a
challenge due to its high dimensionality. $\hessopt$ has dimensions $\etadim
\times \etadim$, where $\etadim$ is the dimension of $\eta$. There are
variational parameters in $\eta$ corresponding to each parameter $\z_\n$. Also,
the truncation level $\kmax$ typically needs to grow with $N$ to keep pace with
the growing number of clusters -- though this growth is sublinear. It follows
from these observations that $\etadim$ can grow linearly with $N$ and thus
become very large.  However, in many cases -- including our BNP problem here --
we can take advantage of model sparsity to efficiently compute
\eqref{vb_eta_sens}. We show how to do so in \secref{computing_sensitivity}. And
our experiments confirm that we can compute $\fracat{d \etaopt(\t)}{d \t}{\t=0}$
much more efficiently than re-optimizing the VB objective directly
(\secref{compute_time}). Moreover, we will see that most of the cost of using
the Taylor series approximation can be shared across multiple values of $t$, so
the savings increases dramatically when we are interested in a range of $t$
values (\secref{compute_time}).

%%
\noindent \textbf{More on accuracy and speed trade-offs in the Taylor series.}
One might save even more compute time, though likely at the cost of some
accuracy, by extending the approximation to $\g$ as well. If the cost of
computing $\g(\etalin(t))$ is prohibitive and if $\g(\eta)$ is itself
continuously differentiable, we may employ an additional first-order expansion
of $\g$ to further approximate $\g(\etalin(t))$. In all of our experiments,
computing $\g(\etalin(t))$ is not prohibitive, so we do not employ this
additional approximation when evaluating the accuracy of our results. However,
as we discuss in \secref{influence_function} below, by linearizing $\g$, we can
construct {\em influence functions} which succinctly summarize the effect of
different choices of $\phi$.  In our experiments, we use the influence function
based on a linearization of $\g$ to choose prior perturbations, whose accuracy
we evaluate with the non-linearized $\g(\etalin(t))$.

Finally, we note that even when the approximation $\etalin(t)$ is an imperfect
substitute for exact re-optimization, the derivative can be a useful guide for
what sorts of prior perturbations might be problematic and thereby inform
further exploration based on re-optimizing; see \secref{results_structure} for
an example and discussion.
