Consider the optimization problem \eqref{vb_optimization}, but now let the
priors $\pstick(\nu_\k)$ which enter the term $\logp(\zeta)$ depend on a
real-valued parameter, $\t \in \tdom \subseteq \mathbb{R}$, writing
$\pstick(\nu_\k \vert \t)$.  For example, $\t$ might represent the concentration
parameter $\alpha$ in the $\mathrm{GEM}(\alpha)$ distribution, or it might
represent some more general class of perturbations.  In any case, the prior on
$\zeta$ and posterior in turn depend on $\t$, which we write $p(\zeta \vert \x,
\t)$.

The $\t$ dependence propagates to the optimal
variational parameters as well.  Define the shorthand notation
%
\begin{align*}
%
\KL{\eta, \t} := \KL{\q(\theta \vert \eta) || p(\x \vert \theta, \t)}.
%
\end{align*}
%
We can then write
%
\begin{align*}
%
\etaopt(\t) :={}& \argmin_{\eta \in \etadom} \KL{\eta, \t},
%
\end{align*}
%
where we write $\etaopt(\t)$ to emphasize the dependence of the optimum on $\t$.

Assuming that, for all $\t \in \tdom$, $\etaopt(\t)$ is interior to $\etadom$,
and that the first derivative $\partial \KL{\eta, \t}/ \partial \eta$ is
continuous, then $\etaopt(\t)$ satisfies the first-order condition
%
\begin{align}\eqlabel{vb_first_order_condition}
%
\fracat{\partial \KL{\eta, \t}}{ \partial \eta}{\etaopt(\t), \t} = 0.
%
\end{align}
%
Recall that the intractable $\logp(\x \vert \t)$ does not depend on
$\eta$, and so vanishes in \eqref{vb_first_order_condition}, and in all
expressions involving partial $\eta$ derivatives.

If the second derivative $\partial^2 \KL{\eta, \t}/ \partial \eta \partial
\eta^T$ is positive definite in a neighborhood of $\etaopt(\t)$ and $\t$, then
$\etaopt(\t)$ is a differentiable function of $\t$ by the implicit function
thoerem.  We can then differentiate \eqref{vb_first_order_condition} to get
%
\begin{align}\eqlabel{vb_eta_sens}
%
\fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
    - \left( \fracat{\partial^2 \KL{\eta, \t}}
                    {\partial \eta \partial \eta^T}
                    {\etaopt(\t_0), \t_0} \right)^{-1}
    \fracat{\partial^2 \KL{\eta, \t}}
           {\partial \eta \partial \t}
           {\etaopt(\t_0), \t_0}.
%
\end{align}

For a general $\t$, we can form a Taylor series approximation to $\etaopt(\t)$
from a solution to \eqref{vb_first_order_condition} for some $\t_0$ and the
result \eqref{vb_eta_sens}.  Specifically, we define
%
\begin{align*}
%
\etalin(\t) := \etaopt(\t_0) + \fracat{d \etaopt(\t)}{d \t^T}{\t_0} (\t - \t_0).
%
\end{align*}
%
Evaluating $\etaopt(\t)$ requires solving a new optimization problem, but, given
$d\etaopt(\t) / d\t | \t_0$, evaluating $\etalin(\t)$ involves only
multiplication and addition.  When $|\t - \t_0|$ is small, we might hope
that $\etaopt(\t) \approx \etalin(\t)$, and so we can use $\etalin(\t)$
to quickly approximate a time-consuming optimization problem.

For functions of interest $\g(\eta)$ which are themselves differentiable,
we can use the chain rule to compute
%
\begin{align*}
%
\fracat{d g(\etaopt(\t))}{d\t} ={}&
    \fracat{\partial g(\eta)}{\partial \eta^T}{\etaopt(\t_0)}
    \fracat{d \etaopt(\t)}{d \t}{\t_0} \\
\glin(\t) :={}& \g(\etaopt(\t_0)) + \fracat{d g(\etaopt(\t))}{d\t}{\t_0} (\t - \t_0).
%
\end{align*}
%
For non-differentiable functions of $\eta$, we can still form the approximation
%
\begin{align*}
%
\gapprox(\t) :={}& \g(\etalin(\t)).
%
\end{align*}
%
The advantage of $\glin(\t)$ relative to $\gapprox(\t)$ is that for the former
we can compute influence functions and worst-case perturbations, as we discuss
below in SECREF.  Converse, $\gapprox(\t)$ may be expected to provide a better
approximation in some cases since it retains non-linearities in the map $\eta
\mapsto \g(\eta)$, linearizing only the computationally intensive map $\t
\mapsto \etaopt(\t)$.


\subsection{Evaluating $d\etaopt(\t) / d\t$}

In order to evaluate $d\etaopt(\t) / d\t$ as given by \eqref{vb_eta_sens}, we
need to solve a linear system involving the $\etadim \times \etadim$ Hessian of
the objective function $\KL{\eta, \t_0}$ and the $\etadim \times 1$ mixed
second-order partial derivative of $\KL{\eta, \t}$ with respect to $\eta$ and
$\t$.  We will discuss these two tasks in turn.

First, consider the mixed partial derivative $\partial^2 \KL{\eta, \t} /
\partial \eta \partial \t$.  In our case, only the priors $\log \pstick(\nu_\k
\vert \t)$ depend on $\t$, and so
%
\begin{align}\eqlabel{sens_mixed_partial}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \t}
       {\etaopt(\t_0), \t_0} ={}&
\sum_{\k=1}^{\kmax}
    \evalat{
        \frac{\partial}{\partial\eta}
        \expect{\q(\nu_\k \vert \eta)}
               {\fracat{\log \pstick(\nu_\k \vert \t)}{\partial \t}{\t_0}
               }
        }
        {\etaopt(\t_0)}.
%
\end{align}
%
Consequently, as long as we can explicitly evaluate $\fracat{\log \pstick(\nu_\k
\vert \t)}{\partial \t}{\t_0} $, either in closed form or with automatic
differentiation, we can evaluate the mixed partial using the derivative of
\eqref{gh_integral}.  We will consider particular functional forms for
$\t \mapsto \pstick(\nu_\k \vert \t)$ in \secref{prior_perturbations}.

Observe that \eqref{sens_mixed_partial} will be zero for all entries of
$\eta$ other than those that parameterize the sticks.

Typically, it is the computation and factorization of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
$\etadim$ is large.  However, this Hessian matrix does not depend on $\t$, and
so can be computed once and re-used for many different values of $\t$, or many
different classes of prior perturbations.  In our case, we have written
$\KL{\eta, \t_0}$ as an explicit function of $\eta$.  Consequently, we can
compute either the Hessian itself or Hessian-vector products using automatic
differentiation CITE.

The Hessian matrix may be quite large, since it involves the VB parameters for
all the random effects.  In particular, though the dimensions of $\etanu$ and
$\etatheta$ are of order $\kmax$, the dimension of $\etaz$ is of order $\kmax
\N$.  For large $\N$, it may not be possible to instantiate the fully dense
Hessian matrix in memory.  Fortunately, the Hessian of the KL divergence is
sparse and highly structured in $\etaz$.

%
