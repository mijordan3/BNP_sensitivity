Consider the optimization problem \eqref{vb_optimization}, but now let the
priors $\pstick(\nuk)$ which enter the term $\logp(\zeta)$ depend on a
real-valued parameter, $\t \in \tdom \subseteq \mathbb{R}$, writing
$\pstick(\nuk \vert \t)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}\exlabel{alpha_perturbation}
%
When drawing from the classical $\mathrm{GEM}(\alpha)$ distribution, we
model
%
\begin{align*}
%
\pstick(\nuk \vert \alpha) ={}&
    \betadist{\nuk \vert 1, \alpha} \Rightarrow\\
\log \pstick(\nuk \vert \alpha) ={}&
    (\alpha - 1) \log(1 - \nuk) + \const. &
    \constdesc{\nuk}
%
\end{align*}
%
Fix some ``original'' $\alpha_0$.  In this case, we represent deviations from the
choice $\alpha_0$ by identifying $\t$ with $\alpha - \alpha_0$:
%
\begin{align*}
%
\log \pstick(\nuk \vert \t) ={}&
    (\t + \alpha_0 - 1) \log(1 - \nuk) + \const. &
    \constdesc{\nuk}
%
\end{align*}
%
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \secref{prior_perturbations}, we will consider a more general class of
perturbations than \exref{alpha_perturbation}.

In any case, the prior on $\zeta$ and posterior in turn depend on $\t$, which we
write $p(\zeta \vert \x, \t)$. The $\t$ dependence propagates to the optimal
variational parameters as well through the dependence of the KL divergence on
$\t$.  Define the shorthand notation
%
\begin{align}\eqlabel{kl_shorthand}
%
\KL{\eta, \t} := \KL{\q(\zeta \vert \eta) || p(\x \vert \theta, \t)}
\mathand
\etaopt(\t) := \argmin_{\zeta \in \etadom} \KL{\eta, \t},
%
\end{align}
%
where we write $\etaopt(\t)$ to emphasize the dependence of the optimum on $\t$.
Without loss of generality, we will continue to use $\etaopt$ with no argument
to refer to $\etaopt(0)$.  That is, we take $\t = 0$ at the ``original''
problem, \eqref{vb_optimization}.

Assuming that, for all $\t \in \tdom$, $\etaopt(\t)$ is interior to $\etadom$,
and that the first derivative $\partial \KL{\eta, \t}/ \partial \eta$ is
continuous, then $\etaopt(\t)$ satisfies the following first-order condition for
all $\t$:
%
\begin{align}\eqlabel{vb_first_order_condition}
%
\fracat{\partial \KL{\eta, \t}}{ \partial \eta}{\etaopt(\t), \t} = 0.
%
\end{align}
%
Recall that the intractable $\logp(\x \vert \t)$ does not depend on $\eta$, and
so vanishes in \eqref{vb_first_order_condition}, and in all expressions
involving partial $\eta$ derivatives of the KL divergence.

If the map $\t \mapsto \etaopt(\t)$ is continuously differentiable, and we have
already computed the solution $\etaopt$ to the ``original'' problem
\eqref{vb_optimization}, then we can form a Taylor series
approximation to $\etaopt(\t)$.  Specifically, we define
%
\begin{align*}
%
\etalin(\t) := \etaopt + \fracat{d \etaopt(\t)}{d \t^T}{\t=0} \t .
%
\end{align*}
%
Evaluating $\etaopt(\t)$ requires solving a new optimization problem, but, given
$d\etaopt(\t) / d\t | 0$, evaluating $\etalin(\t)$ involves only
multiplication and addition.  When $|\t|$ is small, by continuous
differentiability of $\etaopt(\t)$, we might hope that $\etaopt(\t) \approx
\etalin(\t)$, and so we can use $\etalin(\t)$ to quickly approximate a
time-consuming optimization problem.

Futhermore, or functions of interest $\g(\eta)$ which are themselves
differentiable, we can use the chain rule to compute
%
\begin{align}\eqlabel{vb_g_sens}
%
\fracat{d g(\etaopt(\t))}{d\t} ={}&
    \fracat{\partial g(\eta)}{\partial \eta^T}{\etaopt(\t_0)}
    \fracat{d \etaopt(\t)}{d \t}{\t_0} \\
\glin(\t) :={}& \g(\etaopt(\t_0)) + \fracat{d g(\etaopt(\t))}{d\t}{\t_0} (\t - \t_0).
%
\end{align}
%
For non-differentiable functions of $\eta$, we can still form the approximation
%
\begin{align*}
%
\gapprox(\t) :={}& \g(\etalin(\t)).
%
\end{align*}
%
The advantage of $\glin(\t)$ relative to $\gapprox(\t)$ is that for the former
we can compute influence functions and worst-case perturbations, as we discuss
below in \secref{prior_perturbations}.  Converse, $\gapprox(\t)$ may be expected
to provide a better approximation in some cases since it retains non-linearities
in the map $\eta \mapsto \g(\eta)$, linearizing only the computationally
intensive map $\t \mapsto \etaopt(\t)$.

When, then, is $\etaopt(\t)$ continuously differentiable?  We now state some
sufficient conditions which will allow us to prove that $\etaopt(\t)$ is
continuously differentiable via the implicit function theorem
(e.g., \citet{krantz:2012:implicit}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assu}[The VB approximation is nice]\assulabel{q_regular}
%
Define
%
\begin{align*}
%
\lqgrad{\zeta \vert \eta} :={}&
    \fracat{\partial \log \q(\zeta \vert \eta)}{\partial \eta}{\eta}
    \mathtxt{and}
%
\lqhess{\zeta \vert \eta} :={}&
    \fracat{\partial^2 \log \q(\zeta \vert \eta)}
           {\partial \eta \partial \eta^T}{\eta}.
%
\end{align*}
%
We say that a variational density $\q(\zeta \vert \eta)$ defined relative to the
dominating measure $\lambda(\cdot)$ is ``regular'' if the following conditions
hold in a neighborhood $\ball_\eta$ of $\eta_0$:
%
\begin{itemize}
%
\item  The map $\eta \mapsto \log \q(\zeta \vert \eta)$ is twice continuously
differentiable.
%
\item  There exists an envelope function $M(\zeta)$, integrable with respect to
$\lambda$ with $\int M(\zeta) \lambda(d\zeta) < \infty$, such that
all of $\q(\zeta \vert \eta)$, $\q(\zeta \vert \eta) \norm{\lqgrad{\zeta \vert \eta}}_2$,
and $\q(\zeta \vert \eta) \norm{\lqhess{\zeta \vert \eta}}_2$ are
bounded by $M(\zeta)$ uniformly in $\zeta$.
%
\item We say that $\q(\zeta \vert \eta)$ is regular for a function $\psi(\zeta):
\mathbb{R}^\zetadim \mapsto \mathbb{R}$ if $\q(\zeta \vert \eta)
\abs{\psi(\zeta)}$ and $\q(\zeta \vert \eta) \norm{\lqgrad{\zeta \vert
\eta}}_2^2 \abs{\psi(\zeta)}$ are also bounded by $M(\zeta)$ uniformly in
$\zeta$.
%
\end{itemize}
%
%
\end{assu}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Assuref{q_regular} states conditions under which we interchange limits with
variational  expectations, giving continuity of variational expectations and
explicit forms for their derivatives (see, e.g., \lemref{logq_derivs,
logq_continuous} in \appref{proofs}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assu}\assulabel{kl_opt_ok}
%
Let the following assumptions hold for some neighborhood $\ball_\eta$
of $\etaopt$, and, for $\eta \in \ball_\eta$:
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
%
    \item \itemlabel{kl_diffable}
    The map $\eta \mapsto \KL{\eta, 0}$ is twice
    continuously differentiable.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item\itemlabel{kl_hess}
    The Hessian matrix $\fracat{\partial^2 \KL{\eta, 0}}
                    {\partial \eta \partial \eta^T}
                    {\eta}$ is positive definite.%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item\itemlabel{q_stick_regular} The variational approximations $\q(\nu
\vert \t)$ to the stick-breaking posteriors satisfy \assuref{q_regular}
with $\eta_0 = \etaopt$.
%
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\end{assu}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thm}\thmlabel{etat_deriv}
%
Under \assuref{kl_opt_ok}, the map $\t \mapsto \etaopt(\t)$ is continuously
differentiable with derivative
%
\begin{align}\eqlabel{vb_eta_sens}
%
\fracat{d \etaopt(\t)}{d \t}{0} ={}&
    - \left( \fracat{\partial^2 \KL{\eta, \t}}
                    {\partial \eta \partial \eta^T}
                    {\etaopt, 0} \right)^{-1}
    \fracat{\partial^2 \KL{\eta, \t}}
           {\partial \eta \partial \t}
           {\etaopt, 0}.
%
\end{align}
%
\begin{proof}
%
The result follows from \citet[Theorem 3.3.1]{krantz:2012:implicit}.
%
\end{proof}
%
\end{thm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Evaluating $d\etaopt(\t) / d\t$}

In order to evaluate $d\etaopt(\t) / d\t$ as given by \eqref{vb_eta_sens}, we
need to solve a linear system involving the $\etadim \times \etadim$ Hessian of
the objective function $\KL{\eta, 0}$ and the $\etadim \times 1$ mixed
second-order partial derivative of $\KL{\eta, \t}$ with respect to $\eta$ and
$\t$.  We will discuss these two tasks in turn.

First, consider the mixed partial derivative $\partial^2 \KL{\eta, \t} /
\partial \eta \partial \t$.  In our case, only the priors $\log \pstick(\nuk
\vert \t)$ depend on $\t$, and so
%
\begin{align}\eqlabel{sens_mixed_partial}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \t}
       {\etaopt(\t_0), \t_0} ={}&
\sum_{\k=1}^{\kmax}
    \evalat{
        \frac{\partial}{\partial\eta}
        \expect{\q(\nuk \vert \eta)}
               {\fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t_0}
               }
        }
        {\etaopt(\t_0)}.
%
\end{align}
%
Consequently, as long as we can explicitly evaluate $\fracat{\log \pstick(\nuk
\vert \t)}{\partial \t}{\t_0} $, either in closed form or with automatic
differentiation, we can evaluate the mixed partial using the derivative of
\eqref{gh_integral}.  We will consider particular functional forms for
$\t \mapsto \pstick(\nuk \vert \t)$ in \secref{prior_perturbations}.

Observe that \eqref{sens_mixed_partial} will be zero for all entries of
$\eta$ other than those that parameterize the sticks.

Typically, it is the computation and factorization of the Hessian matrix that is
the most computationally intensive part of \eqref{vb_eta_sens}, especially when
$\etadim$ is large.  However, this Hessian matrix does not depend on $\t$, and
so can be computed once and re-used for many different values of $\t$, or many
different classes of prior perturbations.  In our case, we have written
$\KL{\eta, \t_0}$ as an explicit function of $\eta$.  Consequently, we can
compute either the Hessian itself or Hessian-vector products using automatic
differentiation CITE.

The Hessian matrix may be quite large, since it involves the VB parameters for
all the random effects.  In particular, though the dimensions of $\etanu$ and
$\etatheta$ are of order $\kmax$, the dimension of $\etaz$ is of order $\kmax
\times\N$.  For large $\N$, it may not be possible to instantiate the fully
dense Hessian matrix in memory.  Fortunately, the Hessian of the KL divergence
is sparse and highly structured in $\etaz$.  In particular, it is
block-diagonal, with
%
\begin{align*}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta_{\z_{n}} \partial \eta_{\z_{m}}^T}
       {\etaopt(\t_0), \t_0} ={}& 0 \mathtxt{for}n\ne m.
%
\end{align*}
%
Recall from \exref{qz_unconstrained} that $\eta_{\z_\n}$ is of dimension $\kmax -
1$, and that the Hessian of $\expect{\q(\zeta \vert \eta)}{\logp(\x \vert
\zeta)}$ with respect to $\etaz$ may be nonzero despite $\logp(\x \vert \zeta)$
being linear in $\z$ due to the unconstrained parameterization of $\etaz$.

We propose the following techniques for computing the inverse Hessian when the
fully matrix is prohibitively large.  To describe the techniques it will be
useful to use the following compact notation.  Let $\gamma = (\theta, \nu)$
denote all the parameters besides $\z$, and for generic parameters $a$ and $b$
let $\hess{ab}$ denote $\evalat{\partial^2 \KL{\eta, \t} / \partial \eta_a
\eta_b^T}{\etaopt(\t_0), \t_0}$, the Hessian with respect to the variational
parameters governing $a$ and $b$.  Specifically:
%
\begin{align*}
%
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \eta^T}
       {\etaopt(\t_0), \t_0} ={}&
\hess{\zeta\zeta} =
\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\z} \\
   \hess{\z\gamma}     & \hess{\z\z} \\
\end{array}
\right).
%
\end{align*}
%
The terms $\hess{\gamma\gamma}$ and $\hess{\z\gamma} = \hess{\gamma\z}^T$ are
typically dense and easily computed using atuomatic differentiation, and the
term $\hess{\z\z}$ is block diagonal with a closed-form inverse.
%
Analogously, let
%
\begin{align*}
%
\hess{\zeta\t} :=
\fracat{\partial^2 \KL{\eta, \t}}
       {\partial \eta \partial \t}
       {\etaopt(\t_0), \t_0},
%
\end{align*}
%
and observe that
%
\begin{align*}
%
\hess{\zeta\t} ={}& \left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right).
%
\end{align*}

In this notation,
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
-\left(
\begin{array}{cc}
   \hess{\gamma\gamma} & \hess{\gamma\z} \\
   \hess{\z\gamma}     & \hess{\z\z} \\
\end{array}
\right)^{-1}
\left( \begin{array}{c} \hess{\gamma\t} \\ 0 \end{array}\right)
%
\end{align*}
%
We can then use the Schur complement to get the computationally tractable
expression:
%
\begin{align*}
%
\fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
-\left(\begin{array}{c}
I_{\gamma\gamma} \\
\hess{\z\z}^{-1} \hess{\z\gamma}
\end{array}\right)
\left(\hess{\gamma\gamma} -
      \hess{\gamma\z} \hess{\z\z}^{-1} \hess{\z\gamma}\right)^{-1} \hess{\gamma\t}.
%
\end{align*}

In fact, the former expression can be computed entirely using automatic
differentiation using the fact that $\etaoptz$ has an explicit closed form given
$\etaoptgamma$, as dicussed in \exref{qz_form}.  TODO: is this worth writing
here?

When even $\hess{\gamma\gamma}$ is too large to form in memory, one can
use the conjugate gradient algorithm.  TODO: fill in details.
