Suppose we have a dataset $\x$, a real-valued parameter $\theta \in \thetadom
\subseteq \mathbb{R}^{\thetadim}$, and a log likelihood $\logp(\x | \theta)$.
Let us take a family of log prior densities, $\ell(\theta \vert \t)$,
parameterized by some scalar $\t \in \tdom \subseteq \mathbb{R}$, defined
relative to some dominating measure, $\lambda$.  We can write the posterior
density (relative to $\lambda$) and expectations as
%
\begin{align*}
%
p(\theta \vert \x, \t) :={}&
    \frac{p(\x \vert \theta) p(\theta \vert \t)}
         {\int_{\thetadom} p(\x \vert \tilde{\theta}) p(\tilde{\theta} \vert \t)
               \lambda(d \tilde{\theta})}
= \frac{p(\x \vert \theta) p(\theta \vert \t)}{p(\x \vert \t)} \\
\expect{p(\theta \vert \x, \t)}{\g(\theta)} :={}&
    \int_{\thetadom} p(\theta \vert \x, \t) g(\theta) \lambda{d\theta}.
%
\end{align*}
%

Suppose also we have a family of variational distributions
expressed as densities with respect to the same dominating measure, $q(\theta
\vert \eta)$, parameterized by a variational parameter $\eta \in \etadom
\subseteq \mathbb{R}^{\etadim}$.

We will form the variational approximation by solving
%
\begin{align*}
%
\etaopt(\t) :={}&
    \argmin_{\eta \in \etadom}
        \KL{\q(\theta \vert \eta) || p(\x \vert \theta, \t)} \mathwhere \\
\KL{\q(\theta \vert \eta) || p(\x \vert \theta, \t)}
={}&    \expect{\q(\theta \vert \eta)}{
        \log \q(\theta \vert \eta) - \logp(\x \vert \theta) -
        \logp(\theta \vert \t)} + \logp(\x \vert \t).
%
\end{align*}
%
That is, we wish to find the variational distribution $\q(\theta \vert
\etaopt(\t))$ that is closest in KL-divergence to $p(\theta \vert \x, \t)$.  We
have emphasized in the notation $\etaopt(\t)$ that this optimum depends on the
prior hyperparameter $\t$.  For a given $\t$, we use $\q(\theta \vert
\etaopt(\t))$ to approximate the posterior expectation of interest:

%
\begin{align*}
%
\expect{p(\theta \vert \x, \t)}{\g(\theta)} \approx
    \expect{\q(\theta \vert \etaopt(\t))}{\g(\theta)}.
%
\end{align*}
%

Define the shorthand notation
%
\begin{align*}
%
\KL{\eta, \t} := \KL{\q(\theta \vert \eta) || p(\x \vert \theta, \t)}.
%
\end{align*}
%
Assuming that, for all $\t \in \tdom$, $\etaopt(\t)$ is interior to $\etadom$,
and that the first derivative $\partial \KL{\eta, \t}/ \partial \eta$ is
continuous, then $\etaopt(\t)$ satisfies the first-order condition
%
\begin{align}\eqlabel{vb_first_order_condition}
%
\fracat{\partial \KL{\eta, \t}}{ \partial \eta}{\etaopt(\t), \t} = 0.
%
\end{align}
%
In practice, it is typically difficult to evaluate $\KL{\eta, \t}$ due to the
intractable $\logp(\x \vert \t)$ term.  However, since $\logp(\x \vert \t)$ does
not depend on $\eta$, we can evaluate the derivatives $\partial \KL{\eta, \t} /
\partial \eta$, as well as higher order derivatives involving at least one
$\eta$ partial derivative.  For this reason, we can tractably find local
solutions to \eqref{vb_first_order_condition} even though we cannot evaluate the
objective function $\KL{\eta, \t}$ explicitly.

If the second derivative $\partial^2 \KL{\eta, \t}/ \partial \eta \partial
\eta^T$ is positive definite in a neighborhood of $\etaopt(\t)$ and $\t$, then
$\etaopt(\t)$ is a differentiable function of $\t$ by the implicit function
thoerem.  We can then differentiate \eqref{vb_first_order_condition} to get
%
\begin{align}\eqlabel{vb_eta_sens}
%
\fracat{d \etaopt(\t)}{d \t}{\t_0} ={}&
    - \left( \fracat{\partial^2 \KL{\eta, \t}}
                    {\partial \eta \partial \eta^T}
                    {\etaopt(\t_0), \t_0} \right)^{-1}
    \fracat{\partial^2 \KL{\eta, \t}}
           {\partial \eta \partial \t}
           {\etaopt(\t_0), \t_0}.
%
\end{align}

For a general $\t$, we can form a Taylor series approximation to $\etaopt(\t)$
from a solution to \eqref{vb_first_order_condition} for some $\t_0$ and the
result \eqref{vb_eta_sens}.  Specifically, we define
%
\begin{align*}
%
\etalin(\t) := \etaopt(\t_0) + \fracat{d \etaopt(\t)}{d \t^T}{\t_0} (\t - \t_0).
%
\end{align*}
%
Evaluating $\etaopt(\t)$ requires solving a new optimization problem, but, given
$d\etaopt(\t) / d\t | \t_0$, evaluating $\etalin(\t)$ involves only
multiplication and addition.  When $|\t - \t_0|$ is small, we might hope
that $\etaopt(\t) \approx \etalin(\t)$, and so we can use $\etalin(\t)$
to quickly approximate a time-consuming optimization problem.

To evaluate the right-hand side of \eqref{vb_eta_sens}, we need to solve a
linear system involving the $\etadim \times \etadim$ Hessian of the objective
function $\KL{\eta, \t}$ and the $\etadim \times 1$ mixed second-order partial
derivative of $\KL{\eta, \t}$ with respect to $\eta$ and $\t$.  Typically, it is
the computation and factoization of the Hessian matrix that is the most
computationally intensive part of \eqref{vb_eta_sens}, especially when $\etadim$
is large.  However, this Hesian matrix does not depend on $\t$, and so can be
computed once and re-used for many different values of $\t$, or many different
classes of prior perturbations.

%
