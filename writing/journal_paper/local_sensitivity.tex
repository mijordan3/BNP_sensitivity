We would like to understand how our quantity of interest $g(\etaopt)$ changes
when the concentration parameter or, more generally, the stick-breaking density
$\pstick$ changes. To efficiently compute these changes, we use a first-order
Taylor series approximation in the optimal VB parameters. In this section, we
first give the Taylor series and then show how to compute its terms.

%%
\noindent \textbf{Sensitivity to the concentration parameter.} First, we show
how to approximate the sensitivity of $g(\etaopt)$ to the choice of
concentration parameter $\alpha$. Let $\etaopt(\alpha)$ represent the value of
$\etaopt$ for a particular choice of $\alpha$. For our approximation, we choose
some initial value $\alpha_0$ of the concentration parameter and solve the
optimization problem to compute $\etaopt(\alpha_0)$. We then approximate
$\etaopt(\alpha)$ with the linear approximation $\etalin(\alpha)$, and in turn
approximate $g(\etaopt(\alpha))$ with $g(\etalin(\alpha))$:
%
\begin{align*}
%
\etalin(\alpha) :=
    \etaopt(\alpha_0) +
    \fracat{d\etaopt(\alpha)}{d\alpha}{\alpha_0} (\alpha - \alpha_0)
\mathand
g(\etaopt(\alpha)) \approx g(\etalin(\alpha)).
%
\end{align*}
%
If $\alpha \mapsto \etaopt(\alpha)$ is continuously differentiable, and $g$ is
sufficiently smooth, then we expect $g(\etaopt(\alpha)) \approx
g(\etalin(\alpha))$ when $\abs{\alpha - \alpha_0}$ is small.

We show (\thmref{etat_deriv} in \appref{diffable_concentration}) that the map
$\alpha \mapsto \etaopt(\alpha)$ is continuously differentiable for the VB
approximation given in \secref{model_vb}.

%%
\noindent \textbf{Sensitivity to the stick-breaking density.} Next, we show how
to approximate the sensitivity of $g(\etaopt)$ to the choice of concentration
stick distribution $\pstick$. Technically, perturbations of $\alpha$ are
perturbations of $\pstick$. But here we consider more general perturbations of
the form of $\pstick$, potentially outside the beta class. To define our
perturbations, let $\ptil$ represent a potentially unnormalized (but
normalizable) density with respect to Lebesgue measure; the same notation
without the tilde will give the normalized density. Now start from an initial
setting of $\pstick$ at $\pbase$; we will typically start from Dirichlet-process
stick-breaking, i.e.\ $\pbase = \betadist{1,\alpha_0}$. Then take any
Lebesgue-measurable function $\phi(\cdot)$ on $[0,1]$. We consider a range of
alternative (potentially unnormalized) stick-breaking forms $\ptil(\cdot \vert
\t)$ defined by
%
\begin{equation} \eqlabel{mult_perturbation}
	\log \ptil(\cdot \vert \t) = \log \pbase(\cdot) + \t \phi(\cdot)
\end{equation}
%
on $[0,1]$. Note that the perturbation applies equally to every stick break
$\nuk$. This style of multiplicative functional perturbation was proposed by
\citet{gustafson:1996:local}; we deviate from \citet{gustafson:1996:local} by
allowing $\phi$ to take on negative values and by considering VB (rather than
MCMC) approximations.

If we now let $\etaopt(t)$ represent the value of $\etaopt$ for a particular
choice of $\ptil(\cdot \vert \t)$, we can form an analogous approximation to the
$\alpha$ case above:
%
\begin{align} \eqlabel{taylor_series_t}
%
% \etaopt(\alpha) \approx
\etalin(\t) :=
    \etaopt(0) +
    \fracat{d\etaopt(t)}{dt}{\t=0} (\t - 0)
\mathand
g(\etaopt(\t)) \approx g(\etalin(\t)).
%
\end{align}

As in the $\alpha$ case, \eqref{taylor_series_t} is useful only if the map $t
\mapsto \etaopt(t)$ is continuously differentiable for the chosen $\phi$. A
simple sufficient condition for differentiability is given in terms of the
following norm on the perturbation $\phi$.

\noindent \textbf{An infinity norm.}
%First, we define $\norminf{\cdot}$ and a corresponding ball.
Let $\mu$ be a probability measure on $[0,1]$. Then
\begin{equation} \eqlabel{infty_norm}
	\norminf{\phi} := \esssup_{\nu_0 \sim \mu} \abs{\phi(\nu_0)},
	\quad \ball_\phi(\delta) := \left\{ \phi: \norminf{\phi} <
\delta \right\}.
\end{equation}
%
In \coryref{etafun_deriv_form} of \appref{diffable_nonparametric}, we show that
$\t \mapsto \etaopt(\t)$ is continuously differentiable whenever $\norminf{\phi} <
\infty$.  So, for sufficiently smooth $g$, we expect the approximation
\eqref{taylor_series_t} to be good for small $t$ when $\norminf{\phi} < \infty$.
The $\norminf{\cdot}$ norm has further connections to multiplicative
perturbations, as discussed in \citep{gustafson:1996:marginal} and later in
\secref{influence_function}.

We next provide some additional justification for why this particular functional
perturbation is useful. Note first that if we consider any other distribution
$\palt$ for $\pstick$, we can continuously warp $\pbase$ to $\palt$ by setting
$\phi(\cdot) = \log \left( \palt(\cdot) / \pbase(\cdot) \right)$ so long as
$\palt \ll \pbase$, i.e.\ $\palt$ is absolutely continuous with respect to
$\pbase$. Moreover, we will see in \secref{influence_function} that we can
compute an \emph{influence function} to provide an interpretable summary of the
effect of arbitrary changes $\phi$ and also find a worst-case $\phi$ in
$\ball_\phi(\delta)$. Second, while the perturbations above are essentially
multiplicative relative to $\pbase$, one might wonder whether additive
perturbations (subject to renormalization) or other perturbations would work
instead; indeed \citet{gustafson:1996:local} proposes a number of alternative
perturbations. We show in \secref{influence_function} that, among a class of
potential functional perturbations, the one we consider here is the only one
that is {\em Fr{\'e}chet differentiable} -- and thus can be used to safely
reason about worst-case $\phi$.

%%
\noindent \textbf{Computing the terms in the Taylor series.}  It remains to show
that $\alpha \mapsto \etaopt(\alpha)$ and $\t \mapsto \etaopt(\t)$ are
continuously differentiable, and to provide a computable formula for the
derivative.
%
Differentiability naturally requires some regularity conditions on the VB
parameterization and on the optimum.  We state sufficient conditions in the
following \assuref{kl_opt_ok}, which is satisfied for any local optimum of a
smooth, unconstrained parameterization of the variational approximation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assu}\assulabel{kl_opt_ok}
%
Assume that:
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate*}[label=(\arabic*)]
%
    \item \itemlabel{kl_diffable} The map $\eta \mapsto \KL{\eta}$ is twice
    continuously differentiable at $\etaopt$;

    \item\itemlabel{kl_hess} The Hessian matrix $\fracat{\partial^2 \KL{\eta}}
    {\partial \eta \partial \eta^T} {\etaopt}$ is non-singular; and

    \item \itemlabel{kl_opt_interior} There exists an open ball $\ball_\eta
    \subseteq \mathbb{R}^\etadim$ such that $\etaopt \in \ball_\eta \subseteq
    \etadom$.
%
\end{enumerate*}
%
\end{assu}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% First observe that
% we can write the $\alpha$ perturbation using the same notation as the stick-form
% perturbation by setting $t = \alpha - \alpha_0$. Then, as before, let
% $\ptil(\cdot \vert \t)$ be the corresponding (potentially unnormalized)
% distribution on a stick-breaking parameter, and let $\etaopt(t)$ be the value of
% $\etaopt$ for $\ptil(\cdot \vert \t)$. It follows that the concentration
% parameter perturbation is also given by \eqref{taylor_series_t}. Whether we
% perturb the concentration parameter or stick-breaking form, $\etaopt(0)$ is the
% VB optimum for the original, unperturbed problem, and $1-0$ is trivial to
% compute. It remains to provide a formula for the derivative
% $\fracat{d\etaopt(t)}{dt}{\t=0}$.
%
% \todo{Something may be confusing to the reader here.  We have observed that the
% $\alpha$ perturbation could have been represented with a particular choice of
% $\phi$.  Now we are suggesting replacing $\alpha$ with $\t$. We should be clear
% that we are not suggesting using the functional parameterization for all
% hyperparameter perturbations.  The two happen to be equivalent in this case
% because the beta prior is log-linear in $\alpha$, but in general, the two
% parameterizations are not equivalent.}

Our next result gives differentiability as well as a computable formula for the
derivative.
% In what follows,
% we write $\qtil$ for a potentially unnormalized (but normalizable) approximation
% distribution; as with $\ptil$ and $\p$, we again assume $\q$ is the normalized
% form of $\qtil$.
%
\begin{thm}\thmlabel{bnp_deriv}
%
Assume that we are using the VB approximation given in \secref{model_vb} and
that \assuref{kl_opt_ok} holds.  Take the perturbation given by
\eqref{mult_perturbation} with $\norminf{\phi} < \infty$, or the concentration
parameter with $\varepsilon = \alpha$, identifying $\varepsilon$ with $\t$ or
$\alpha - \alpha_0$, respectively.  Then the map $\varepsilon \mapsto
\etaopt(\varepsilon)$ is continuously differentiable at $\varepsilon = 0$ with
derivative
%
\begin{align}
%
\eqlabel{bnp_vb_eta_sens}
\fracat{d \etaopt(\varepsilon)}{d \varepsilon}{\varepsilon=0} ={}&
    - \hessopt^{-1} \crosshessian, \mathwhere
    \rho_\k(\nuk) := \fracat{\partial \log \ptil(\nu_k \vert \varepsilon)}
            {\partial \varepsilon}{\varepsilon=0}
            ,
\\
%
\hessopt :={}& \fracat{\partial^2 \KL{\eta}}
                      {\partial \eta \partial \eta^T}
                      {\eta = \etaopt},
\quad \lqgradbar{\zeta \vert \eta} :={}
\fracat{\partial \log \q(\zeta \vert \eta)}{\partial \eta}{\eta}, \textrm{ and}\\
    % \lqgrad{\zeta \vert \eta} -
    % \expect{\q(\zeta \vert \eta)}{\lqgrad{\zeta \vert \eta}}, \\
%
\eqlabel{bnp_vb_crosshessian}
\crosshessian :={}&
    \frac{\partial}{\partial\eta}
    \expect{\q(\zeta \vert \eta)}{
        \sum_{k=1}^{\kmax-1}
        \rho_\k(\nuk)
    }
    \Bigg|_{\eta = \etaopt}
={}
    \expect{\q(\zeta \vert \etaopt)}{
          \lqgradbar{\zeta \vert \etaopt}
          \sum_{k=1}^{\kmax-1}
          \rho_\k(\nuk)}.
%
\end{align}
%
\end{thm}
%
\begin{proof}
%
The result follows from \thmref{etat_deriv} of \appref{diffable_parametric},
which states general conditions for the differentiability of VB optima.  We show
in \appref{diffable_concentration, diffable_nonparametric} that the conditions
of \thmref{etat_deriv} are satisfied in the case of our present BNP problem. The
equivalence of the expressions for $\crosshessian$ follow by differentiating
through the expectation; see \appref{proofs} for more details.
% The proof of \thmref{etat_deriv} relies on the differentiability of the
% log densities, the ability to exchange the order of integration and
% differentiation using the dominated convergence theorem, and the implicit
% function theorem applied to the the first order condition of the VB
% objective function.
%
\end{proof}
%

\eqref{vb_eta_sens} requires computation of two terms: $\hessopt^{-1}$ and
$\crosshessian$.  Typically, $\crosshessian$, which is a derivative of a
variational expectation, is straightforward to evaluate: the requisite
expectation is evaluated either in closed form or approximated numerically;
then, in either case, an application of automatic differentiation provides the
gradient. Forming and inverting or factorizing $\hessopt$ can present a
challenge due to its high dimensionality. $\hessopt$ has dimensions $\etadim
\times \etadim$, where $\etadim$ is the dimension of $\eta$. There are
variational parameters in $\eta$ corresponding to each parameter $\z_\n$. Also,
the truncation level $\kmax$ typically needs to grow with $N$ to keep pace with
the growing number of clusters -- though this growth is sublinear. It follows
from these observations that $\etadim$ can grow linearly with $N$ and thus
become very large.  However, in many cases -- including our BNP problem here --
we can take advantage of model sparsity to efficiently compute
\eqref{vb_eta_sens}. We show how to do so in \secref{computing_sensitivity}. And
our experiments confirm that we can compute $\fracat{d \etaopt(\t)}{d \t}{\t=0}$
much more efficiently than re-optimizing the VB objective directly
(\secref{compute_time}). Moreover, we will see that most of the cost of using
the Taylor series approximation can be shared across multiple values of $t$, so
the savings increases dramatically when we are interested in a range of $t$
values (\secref{compute_time}).

%%
\noindent \textbf{More on accuracy and speed trade-offs in the Taylor series.}
One might save even more compute time, though likely at the cost of some
accuracy, by extending the approximation to $g$ as well. If the cost of
computing $g(\etalin(t))$ is prohibitive and if $g(\eta)$ is itself continuously
differentiable, we may employ an additional first-order expansion of $g$ to
further approximate $g(\etalin(t))$. In all of our experiments, computing
$g(\etalin(t))$ is not prohibitive, so we do not employ this additional
approximation.
\todo{The other big reason to linearize $g$ is for the influence function.}

Finally, we note that even when the approximation $\etalin(t)$ is an imperfect
substitute for exact re-optimization, the derivative can be a useful guide for
what sorts of prior perturbations might be problematic and thereby inform
further exploration based on re-optimizing; see \secref{results_structure} for
an example and discussion.
