A discrete Bayesian nonparametric (BNP) generative model draws data points
$\x_n$ from one of an infinite number of components indexed by $\k = 1, 2,
\ldots \infty$. Each component is characterized by a vector $\beta_\k \in
\betadom \subseteq \mathbb{R}^{\betadim}$, with $\p(\x_n \vert \beta_\k)$
denoting the distribution of data arising from  component $\k$. We model the
$\beta_\k$ as arising IID from a known prior, or \textit{base distribution},
denoted $\pbetaprior(\beta_\k)$, and write $\beta = (\beta_1, \beta_2, \ldots)$.

Assignment of data point $\n$ to a mixture component is represented by an
(infinite dimensional) vector $\z_\n = (\z_{\n1}, \z_{\n2}, \ldots)$ whose
elements $\z_{\n\k} = 1$ for exactly one $\k$ and $0$ otherwise. With $\z_\n$
defined in this way, we can write
%
\begin{align*}
%
\p(\x_n \vert \z_\n, \beta) =
    \prod_{k=1}^\infty \p(\x_n \vert \beta_\k)^{\z_{\n\k}}.
%
\end{align*}

The prior probabilities of assignments $\z_{\n}$ are generated according to the
following ``stick-breaking'' process. Fix a density $\pstick(\cdot)$, with
respect to the Lebesgue measure on $[0,1]$, over stick-breaking proportions
$\nuk \in (0, 1)$ and draw $\nuk\iid\pstick(\nuk)$ for $\k=1,2,\ldots\infty$.
Given these stick lengths, construct probabilities using the following formula:
%
\begin{align}\eqlabel{stick_breaking}
%
\pi_\k := \nuk \prod_{\k' < \k} (1 - \nu_{\k'}),
%
\end{align}
%
where the empty product is taken to be equal to $1$. By construction,
$\sum_{\k=1}^{\infty} \pi_\k = 1$. Given the probability vector $\pi := (\pi_1,
\pi_2, \ldots)$, the $\z_\n$ are drawn according to
%
\begin{align*}
%
\p(\z_\n \vert \pi) ={}&
   \prod_{k=1}^{\infty} \pi_\k^{\z_{\n\k}}.
%
\end{align*}
%
Since $\pi$ is a deterministic function of the stick-breaking proportions $\nu :=
(\nu_1, \nu_2, \ldots)$, we can also write $\p(\z_\n \vert \nu)$ with no
ambiguity.

The stick-breaking density $\pstick$ can thus be thought of as inducing a a
distribution on the vector of probabilities $\pi$.  Different choices of
$\pstick$ will favor different assignment probabilities, each with different
implied prior degrees of concentration.
%
Typically, many different choices for $\pstick$ may be {\em a priori}
reasonable.  A particularly common set of candidates for $\pstick$ is the class
of densities given by $\mathrm{Beta}(\nuk \vert 1, \alpha)$ density, which we
write as
%
\begin{align}\eqlabel{beta_density}
%
\pstick(\nuk \vert \alpha) :=
\mathrm{Beta}(\nuk \vert 1, \alpha) =
    \frac{\Gamma(1 + \alpha) (1 - \nuk)^{\alpha - 1}}
         {\Gamma(\alpha)},
%
\end{align}
%
When $\pstick$ is $\mathrm{Beta}(\nuk \vert 1, \alpha)$, the resulting
distribution on $\pi$ is known as the $\textit{GEM distribution}$, and we write
$\pi \sim \mathrm{GEM}(\alpha)$.
%
The GEM distribution is closely related to the Dirichlet process (DP).
Define a measure on $\betadom$ as
%
\begin{align*}
  \mathcal{M} = \sum_{\k = 1}^\infty \pi_\k\delta_{\beta_\k},
\end{align*}
%
which places atoms at points $\beta_k$ with weight $\pi_\k$. When $\pi \sim
\mathrm{GEM}(\alpha)$ and $\beta_\k \iid \pbetaprior(\beta_\k)$, $\mathcal{M}$
is a random measure is distributed according to Dirichlet process with
concentration parameter $\alpha$ and base measure $\pbetaprior$
\citep{ferguson:1973:bayesian, sethuraman:1994:constructivedp}. Below, we will
give special attention to stick-breaking densities in the Beta family, though we
will also consider deviations from the Beta family to generic densities.

In the generative process we have just outlined, the joint distribution of the
observed data and latent variables in a basic BNP mixture model
%
\footnote{ Our other experiments on real data sets in \secref{results_mice,
results_structure} require slightly different modeling considerations, and we
adjust the factorization in \eqref{bnp_model} to suit the model needs of the
particular problem.  However, the basic case of \eqref{bnp_model} will suffice
to motivate our approach, and we defer discussions of the variants until they
are needed.}
%
is given by
%
\begin{align}\eqlabel{bnp_model}
%
\logp(\x, \beta, \z, \nu) =&
\sum_{n=1}^N \sum_{k=1}^{\infty}
    \z_{\n\k} \left(
        \logp(\x_n \vert \beta_\k) + \log \pi_\k
    \right)
\nonumber\\
   & +
    \sum_{k=1}^{\infty} \left(
        \log \pstick(\nuk) + \log \pbetaprior(\beta_\k)
    \right).
%
\end{align}
%
Different choices of $\pstick$ will thus, in general, lead to different
posterior distributions via the application of Bayes' rule to \eqref{bnp_model},
and so lead to different values for posterior expectations of quantities of
interest.

For concreteness, let us consider the following motivational example
based on the Fisher iris datset \citep{anderson:1936:iris, fisher:1936:iris}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}[Gaussian mixture model and iris data]\exlabel{iris_bnp_process}
%
The observations are vectors $\x_\n \in \mathbb{R}^\d$, and we model each
component with a multivariate Gaussian. In this model, $\beta_\k = (\mu_k,
\Lambda_\k)$, where $\mu_\k \in \mathbb{R}^\d$, $\Lambda_\k$ is a $\d\times\d$
positive definite information matrix, and
%
\begin{align*}
%
\p(\x_\n \vert \beta_\k) ={}& \normdist{\x_n \vert \mu_\k, \Lambda_\k^{-1}} \\
\logp(\x_\n \vert \beta_\k) ={}&
    -\frac{1}{2}(\x_n - \mu_k)^T \Lambda_\k (\x_n - \mu_k)
    + \frac{1}{2} \log |\Lambda_\k| + \const.\\
    & \constdesc{\beta_\k}
%
\end{align*}
We let $\pbetaprior(\beta_\k)$ be the conjugate prior, which in this case is normal-Wishart:
\begin{align*}
  \pbetaprior(\beta_\k) &= \normalwishart{\beta_\k \vert \tau_0, n_0, p_0, V_0}\\
  \log\pbetaprior(\beta_\k) &=
      -\frac{\tau_0}{2}(\mu_\k - \mu_0)^T \Lambda_\k (\mu_\k - \mu_0)\\
      &{} + \frac{n_0 - p_0 - 1}{2} \log |\Lambda_\k| -
      \frac{1}{2} \textrm{Tr}(V_0 \Lambda_\k) + \const,
\end{align*}
where $(\tau_0, n_0, p_0, V_0)$ are fixed prior parameters.
%
For a choice of $\pstick$, and for $\pbetaprior(\beta_\k)$ and $\p(\x_\n \vert
\beta_\k)$ as given above, the posterior $\p(\beta, \z, \nu \vert \x)$ can in
principle be computed by applying Bayes' rule to \eqref{bnp_model}.

In \secref{results}, we fit a GMM to Fisher's iris data set and cluster irises
into latent species based on morphological measurements.  In that case, $\k$
indexes distinct species, $\beta_{\k}$ characterizes the distribution of
morphological measurements for species $\k$, and $\z_{\n\k} = 1$ when
observation $\n$ is a member of species $\k$.
%
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{ex}[A posterior quantity of interest]\exlabel{insample_nclusters_simple}

One might ask, \textit{how many clusters are present in the data set}? To answer
this question in the context of a BNP model, define the
following ``quantity of interest'':
%
\begin{align*}
  \nclusters_0(\z) := \sum_{k=1}^\kmax \ind{ \left(\sum_{n=1}^{N}
  \z_{\n\k}\right) > 0}
  = \sumk \left(1 -  \prod_{\n=1}^\N (1 - \z_{\n\k})\right),
\end{align*}
%
where $\ind{\cdot}$ is the indicator function taking value $1$ when the argument
is true and $0$ otherwise, and the second equality follows from the fact that
each $\z_{\n\k}$ is either zero or one, so $\sum_{\n=1}^N \z_{\n\k} > 0$ if and
only if $\prod_{\n=1}^\N (1 - \z_{\n\k}) = 0$.

$\nclusters_0$ counts the number of clusters with at
least one observations in a set of assignments $\z$.  The posterior expected
number of clusters is thus given by $\expect{\p(\z\vert\x)}{\nclusters_0(\z)}$.
For example, in the iris data set of \exref{iris_bnp_process},
$\expect{\p(\z\vert\x)}{\nclusters_0(\z)}$ is the posterior expectation of the
number of distinct iris species present in the observed data.
%
Again, given a choice of $\pstick$ and the definitions in
\exref{iris_bnp_process}, $\expect{\p(\z\vert\x)}{\nclusters_0(\z)}$ can in
principle be computed from the posterior $\p(\z \vert \x)$.
% We consider generalizations of this and other posterior quantities of interest
% in \secref{posterior_quantities} below.
%
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
