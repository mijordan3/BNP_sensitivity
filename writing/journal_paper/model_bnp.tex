We will consider discrete Bayesian nonparametric (BNP) generative models which
draw data points $\x_n$ from one of an infinite number of components indexed by
$\k = 1, 2, \ldots$.
Each component is characterized by a vector $\beta_\k \in \betadom \subseteq
\mathbb{R}^{\betadim}$, with $\p(\x_n \vert \beta_\k)$ denoting the
distribution of data arising from  component $\k$. We will model the $\beta_\k$
as arising IID from a known prior $\beta_\k \iid \pbetaprior(\beta_\k)$, and write
$\beta = (\beta_1, \beta_2, \ldots)$.

Assignment of data point $\n$ to a mixture component is represented by an
(infinite dimensional) vector $\z_\n$ with elements $\z_{\n\k}$
where $\z_{\n\k} = 1$ for exactly one $\k$ and $0$ otherwise.
With $\z_\n$ defined in this way, we can write
%
\begin{align*}
%
\p(\x_n \vert \z_\n, \beta) =
    \prod_{k=1}^\infty \p(\x_n \vert \beta_\k)^{\z_{\n\k}}.
%
\end{align*}


The assignments $\z_{\n}$ are drawn according to the following
``stick-breaking process.''  Fix a density $\pstick(\cdot)$, with respect to the
Lebesgue measure, over stick-breaking proportions $\nuk \in (0, 1)$ and
draw $\nuk\iid\pstick(\nuk)$ for $\k=1,2,\ldots$.
Given these stick lengths, we compute indicator probabilities by
%
\begin{align}\eqlabel{stick_breaking}
%
% \pi_\k := \begin{cases}
% \nuk \prod_{\k' < \k} (1 - \nu_{\k'}) & \textrm{For }k < \infty
%     \textrm{ (all }k\textrm{ when }\infty = \infty\textrm{)}\\
% \prod_{\k' < \k} (1 - \nu_{\k'}). & \textrm{For }k = \infty \\
% \end{cases}
\pi_\k := \nuk \prod_{\k' < \k} (1 - \nu_{\k'})
%
\end{align}
%
where the empty product is taken to be equal to $1$.
Write $\nu := (\nu_1, \nu_2, \ldots)$ for the vector of all stick lengths
and $\pi := (\pi_1, \pi_2, \ldots)$ for the corresponding vector of
probabilities. By construction, $\sum_{\k=1}^{\infty} \pi_\k = 1$.
 We draw $\z_\n$ according to
%
\begin{align*}
%
% \p(\z_{\n\k} = 1 \vert \pi) ={}& \pi_\k \\
% \p(\z_\n \vert \pi) ={}&
%     \ind{\sum_{\k=1}^{\kmax} \z_{\n\k} = 1}
%     \prod_{k=1}^{\kmax} \pi_\k^{\z_{\n\k}}.
\p(\z_\n \vert \pi) ={}&
    \prod_{k=1}^{\infty} \pi_\k^{\z_{\n\k}}.
%
\end{align*}
%
Since $\pi$ is a deterministic function of $\nu$, we can also write
$\p(\z_\n \vert \nu)$ with no ambiguity.

The stick-breaking distribution $\pstick$ can be thought of as inducing a
a distribution on the vector of probabilities $\pi$. Different
stick-breaking distributions will favor different indicator probabilities
with different implied degrees of concentration.
A particularly common choice for
$\pstick(\nuk)$ is the $\mathrm{Beta}(\nuk \vert 1, \alpha)$ distribution,
%
\begin{align*}
%
\mathrm{Beta}(\nuk \vert 1, \alpha) =
    \frac{\Gamma(1 + \alpha) (1 - \nuk)^{\alpha - 1}}
         {\Gamma(\alpha)}.
%
\end{align*}
%
When $\pstick$ is Beta distributed, the resulting distribution on $\pi$ is known as the
$\textit{GEM distribution}$, and we write $\pi \sim \mathrm{GEM}(\alpha)$.

The GEM distribution is closely related to the Dirichlet process (DP).
When $\pi \sim \mathrm{GEM}(\alpha)$ and
$\beta_\k \iid \pbetaprior(\beta_\k)$,
the random measure on $\betadom$,
\begin{align*}
  \mathcal{M} = \sum_{\k = 1}^\infty \pi_\k\delta_{\beta_\k}
\end{align*}
which places atoms at $\beta_k$ with weight $\pi_\k$,
is a distributed according to Dirichlet process with concentration parameter $\alpha$
and base measure $\pbetaprior$.

We keep the generic notation $\pstick$ for stick-breaking distributions
because in our sensitivity analysis,
we will consider stick-breaking distributions that are outside the
family of Beta distributions.

In this notation, we can write the joint distribution of
the observed data and latent variables in a basic DP mixture model as:
%
\begin{align}\eqlabel{bnp_model}
%
% \logp(\x, \beta, \z, \nu) ={}&
%     \sum_{n=1}^N \sum_{k=1}^{\kmax}
%         \z_{\n\k} \left(
%             \logp(\x_n \vert \beta_\k) + \log \pi_\k
%         \right) +
% \nonumber \\ {}&
%     \sum_{k=1}^{\kmax} \left(
%         \log \pstick(\nuk) + \logp(\beta_\k)
%     \right).
\logp(\x, \beta, \z, \nu) =&
\sum_{n=1}^N \sum_{k=1}^{\infty}
    \z_{\n\k} \left(
        \logp(\x_n \vert \beta_\k) + \log \pi_\k
    \right)
\nonumber\\
   & +
    \sum_{k=1}^{\infty} \left(
        \log \pstick(\nuk) + \log \pbetaprior(\beta_\k)
    \right).
%
\end{align}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}[Gaussian mixture model]\exlabel{iris_bnp_process}
%
The observations are vectors $\x_\n \in \mathbb{R}^\d$,
and we model each component with a multivariate Gaussian.
In this model, $\beta_\k = (\mu_k, \Lambda_\k)$,
where $\mu_\k \in \mathbb{R}^\d$, $\Lambda_\k$ is a $\d\times\d$ positive
definite information matrix, and
%
\begin{align*}
%
\p(\x_\n \vert \beta_\k) ={}& \normdist{\x_n \vert \mu_\k, \Lambda_\k} \\
\logp(\x_\n \vert \beta_\k) ={}&
    -\frac{1}{2}(\x_n - \mu_k)^T \Lambda_\k (\x_n - \mu_k)
    + \frac{1}{2} \log |\Lambda_\k| + \const.\\
    & \constdesc{\beta_\k}
%
\end{align*}
We let $\pbetaprior$ to be the conjugate prior, which in this case is normal-Wishart:
\begin{align*}
  \pbetaprior(\beta_\k) &= \normalwishart{\beta_\k \vert \tau_0, n_0, p_0, V_0}\\
  \log\pbetaprior(\beta_\k) &=
      -\frac{\tau_0}{2}(\mu_\k - \mu_0)^T \Lambda_\k (\mu_\k - \mu_0)\\
      &{} + \frac{n_0 - p_0 - 1}{2} \log |\Lambda_\k| -
      \frac{1}{2} \textrm{Tr}(V_0 \Lambda_\k) + \const,
\end{align*}
where $(\tau_0, n_0, p_0, V_0)$ are prior parameters.

% Below, we fit a
% Gaussian mixture model (GMM) to Fisher's iris data set CITE.
% Each observation is an iris flower with
% four measurements:
% sepal length, sepal width, petal length, and petal width.
% The components in this model can be interpreted as latent iris species;
% the inferential goal is to estimate $\z_\n$ and assign each
% observed flower to a latent species.

% For the iris data, we might imagine that each cluster corresponds to a different
% species with a different distribution of flower dimensions.  The BNP model
% implies that there are a potentially infinite number of differet iris species
% that we might observe.  Then $\z_{\n\k} = 1$ would mean that observation $\n$
% was a member of species $\k$, and $\sum_{k=1}^\kmax \ind{ \sum_{n=1}^{N}
% \z_{\n\k} > 1}$ is the number of distinct species observed in our particular
% dataset.
%
\end{ex}

We started with a Gaussian mixture model (GMM)
because it is the most generic model that fits cleanly into the
generative process culminating in \eqref{bnp_model}.
In \secref{results}, we fit a GMM to Fisher's iris data set and
cluster observations into latent iris species based on morphological measurements.
The next two examples on real data sets require more careful modeling considerations,
and we adjust the factorization in \eqref{bnp_model} to suit our purposes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[Regression mixture model]\exlabel{mice_bnp_process}

We cluster time-course gene expression data using a BNP model.
An observation $\x_\n\in\mathbb{R}^\ntimepoints$ is a vector of expression levels at $\ntimepoints$
time points.
Let $\regmatrix$ be an $\ntimepoints \times \d$ regressor matrix;
in our case, we will use a cubic B-spline, so the entry $ij$ of $\regmatrix$
is the $j$-th basis vector evaluated at the $i$-th time point (\secref{results_mice}).

Each cluster is characterized by a vector of regression coefficients
$\mu_\k$ and a variance $\tau^{-1}_\k$, so
in this model, $\beta_k = (\mu_\k, \tau_\k)$.
The distribution of the data arising from cluster $k$ is
\begin{align*}
\p(\x_\n | \beta_\k, \b_\n) =
\normdist{\x_\n | \regmatrix\mu_\k + \b_\n,
\tau_\k^{-1}I_{\ntimepoints \times \ntimepoints}},
\end{align*}
%
where $\b_{n}$ is a gene-specific additive offset.
We include the additive offset because we
are interested in clustering gene expressions based on their patterns over time,
not their absolute level.

The joint distribution can be written in the same form as~\eqref{bnp_model},
except that the conditional log-likelihood now conditions on $\b_\n$
and there is an additional prior term for $\b_\n$:
\begin{align*}
\logp(\x, \beta, \z, \nu) =&
    \sum_{n=1}^N \sum_{k=1}^{\infty}
        \z_{\n\k} \left(
            \logp(\x_n \vert \beta_\k, \b_n) + \log \pshift(\b_n) + \log \pi_\k
        \right)  \\
    &{} + \sum_{k=1}^{\infty} \left(
        \log \pstick(\nuk) + \log \pbetaprior(\beta_\k)
    \right).
\end{align*}

\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our last example is a Bayesian topic model applied to genetic data.
Genotypes at genetic markers take the place of
words in a document; in lieu of inferring ``topics," we infer latent populations.

\begin{ex}[A topic model for population structure]\exlabel{structure_bnp_process}

The data set consists of $\nindiv$ individuals genotyped at $\nloci$ loci.
For diploid species, there are two observations at each loci, one at each chromosome.
Let $\x_{\n\l\i}\in\{1, \ldots, J_\l\}$ be the genotype for individual $\n$ at locus $\l$ and chromosome $\i$;
$J_\l$ is the number of possible genotypes at locus $\l$.
For example, if the measurements are single nucleotides (A, T, C or G)
then $J_\l = 4$ for all $\l$.

A latent population is characterized by the collection
$\beta_k = (\latentpop_{\k1}, \ldots, \latentpop_{\k\nloci})$ where
$\latentpop_{\k\l}\in\Delta^{J_\l - 1}$ are the latent frequencies for the $J_l$
possible genotypes at locus $\l$.
Each observation $\x_{\n\l\i}$ is assigned a latent population;
note that for a given individual $\n$,
different loci, and even different chromosomes at a given locus,
may be assigned to different populations.
The distribution of $\x_{\n\l\i}\in\{1, \ldots, J_\l\}$ arising from population $\k$ is
\begin{align*}
\p(\x_{\n\l\i} \vert \latentpop_{\k}) =
\categoricaldist{\x_{\n\l\i}\vert \latentpop_{\k\l}}.
\end{align*}


Unlike the previous models, we now have a stick-breaking process for each individual.
Draw sticks
\begin{align*}
\nu_{\n\k} \iid \pstick(\nu_{\n\k}) \quad \forall \n = 1, ..., \nindiv; \k = 1, 2, \ldots.
\end{align*}
The mixture weights
$\latentadmix_{\n} = (\latentadmix_{\n1}, \latentadmix_{\n2}, \ldots)$
are formed by the usual stick-breaking construction,
%
\begin{align*}
\latentadmix_{\n\k} = \nu_{\n\k} \prod_{\k' < \k} (1 - \nu_{\n\k'}).
\end{align*}
%
In this genetics application,
we call the mixture weights
$\latentadmix_{\n}$ the
\textit{admixture} of individual $\n$.

The latent population membership $\z_{\n\l\i}$ of the observed genotype
$\x_{\n\l\i}$ is then drawn according to the usual multinomial distribution
\begin{align*}
p(\z_{\n\l\i} | \latentadmix_\n) = \prod_{k=1}^{\infty} \latentadmix_{\n\k}^{\z_{\n\l\i\k}}.
\end{align*}

The joint log-likelihood decomposes as
\begin{align*}
\logp(\x, \latentpop, \z, \nu) &=
\sum_{\n=1}^\nindiv \sum_{\l=1}^\nloci \sum_{i = 1}^2 \sum_{\k=1}^{\infty}
        \z_{\n\l\i\k} \left(
            \logp(\x_{\n\l\i} \vert \latentpop_{\k}) + \log \pi_{\n\k}
        \right)
\nonumber\\&
    \quad +
    \sum_{\n=1}^\nindiv \sum_{k=1}^{\infty} \log \pstick(\nu_{\n\k})
    + \sum_{k=1}^{\infty} \log \pbetaprior(\latentpop_{\k}).
\end{align*}

This model is identical to STRUCTURE,
a model proposed in \citet{pritchard:2000:structure, raj:2014:faststructure},
except that we replace the Dirichlet prior in STRUCTURE
with an infinite stick-breaking process.
The result is a model similar to a heirchical Dirichlet process CITE,
but without the top-level Dirichlet process.

\end{ex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
