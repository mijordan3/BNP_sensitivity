%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do not edit the TeX file your work
% will be overwritten.  Edit the RnW
% file instead.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<setup, include=FALSE>>=
knitr_debug <- FALSE # Set to true to see error output
simple_cache <- FALSE # Set to true to cache knitr output for this analysis.
source("R_scripts/initialize.R", echo=FALSE)
source("R_scripts/plotting_utils.R", echo=FALSE)

# load data
load('./R_scripts/data_processed/iris.RData')

# some quantities we cite in the text
min_enum_clust <- min(alpha_sens_df$n_clusters_refit)
max_enum_clust <- max(alpha_sens_df$n_clusters_refit)

min_enum_clust_pred <- min(alpha_sens_df_pred$n_clusters_refit)
max_enum_clust_pred <- max(alpha_sens_df_pred$n_clusters_refit)

@

In this section, we apply our methods to Fisher's iris data set, using the
Gaussian mixture model (GMM) model from \exref{iris_bnp_process} and VB
approximation given in \eqref{vb_mf} and \exref{iris_var_distr}.  Here, the data
dimension $d = 4$, we set the truncation parameter $\kmax = 15$.  For our
quantities of interest, we investigate both the estimated number of in-sample
clusters as described in \exref{vb_insample_nclusters_simple} and the
\textit{posterior predictive} number of clusters. Let $\nclusters(\z)$ be the
number of clusters as defined as in \exref{insample_nclusters_simple}.  The
posterior predictive number of clusters, $\expect{\p(\pi \vert
\x)}{\expect{\p(\z \vert \pi)}{\nclusters(\z)}}$, is the posterior expectation
of the number of distinct clusters that we would expect to see in a new dataset
the same size as our original dataset.  Analogously to
\exref{insample_nclusters_simple}, we define a variational approximation to
posterior predictive number of clusters as
%
\begin{align*}
\gclusterspredabbr(\eta) :=
    \expect{\q(\nu\vert\eta)}{\expect{\p(\z\vert\nu)}{\nclusters(\z)}}
  = \sum_{k=1}^\kmax\left(1 -
  \expect{\q(\nu \vert \eta)}{(1 - \pi_\k)^\N} \right).
\end{align*}
%
In the iris example, the predictive quantity can interpreted as the expected
number of species one might see if a fresh sample of iris flowers were
collected.

\subsubsection*{Parametric sensitivity}

We first explore sensitivity of the in-sample and posterior predictive number of
clusters to the concentration parameter, $\alpha$, using the results in
\secref{diffable_concentration}.  To choose a plausible range of $\alpha$,
recall that, under the $\gem$ prior, the {\em a priori} expected
number of distinct clusters in a dataset of size $N$ is given
\citep{jordan:2015:gentleintrodp} by
%
\begin{align}\eqlabel{prior_num_clusters}
\expect{\p(\z \vert \pi)\p(\pi \vert \alpha)}{\nclusters(\z)} =
\sum_{\n = 1}^\N \frac{\alpha}{\alpha + \n - 1}.
\end{align}
%
Using this formula, we take $\alpha\in[0.1, 4.0]$, corresponding to an an {\em a
priori} range of approximately $1.5$ to $15$ clusters.  Over this range, the
shape of the stick-breaking density varies considerably, as shown in
\figref{beta_priors}.  We take $\alpha_0 = 2$, near the middle of the range, and
use the prior $\p(\nuk \vert \alpha_0)$ as the base prior at which we compute
the derivatives for all the results below.  \Figref{iris_fit} shows the
posterior clustering for $\alpha_0$.  For our base prior, the posterior thus
recovers the known ground truth that there are in truth three distinct species.

<<beta_priors_cap>>=
beta_priors_cap <-
  paste0("Probability density functions of $\\text{Beta}(1, \\alpha)$ ",
         "distributions, under various $\\alpha$ considered for the ",
         "iris data set.")
SetImageSize(aspect_ratio = base_aspect_ratio * 0.5)
@
<<beta_priors, cache=simple_cache, fig.show='hold', fig.cap=beta_priors_cap>>=
source('./R_scripts/iris/plot_beta_priors.R',
       echo=knitr_debug, print.eval=TRUE)
@


<<iris_fit_cap>>=
iris_fit_cap <- paste0("The iris data in principal component space and
                      GMM fit at $\\alpha = ",
                      alpha0,
                      "$. Colors denote inferred memberships and
                      ellipses represent estimated covariances. ")
SetImageSize(aspect_ratio = base_aspect_ratio,
             image_width = 0.6)
@
<<iris_fit, cache=simple_cache, fig.show='hold', fig.cap=iris_fit_cap>>=
source('./R_scripts/iris/iris_init_fit.R',
       echo=knitr_debug, print.eval=TRUE)
@


<<iris_alpha_sens_cap>>=
iris_alpha_sens_cap <- paste0(
    "The expected number of clusters as $\\alpha$ varies in the",
    "the GMM fit of the iris data. ",
    "On the left is in-sample quantity $\\gclustersabbr$. ",
    "On the right is the the predictive quantity $\\gclusterspredabbr$. ",
    "We formed the linear approximation at $\\alpha=",
    alpha0, '$.')

SetImageSize(aspect_ratio = 0.7 * base_aspect_ratio,
             image_width = 0.8)
@
<<iris_alpha_sens, cache=simple_cache, fig.show='hold', fig.cap=iris_alpha_sens_cap>>=
source("R_scripts/iris/iris_alpha_sens.R", echo=knitr_debug, print.eval=TRUE)
@

\Figref{iris_alpha_sens} shows both the posterior in-sample and predictive
number of distinct clusters as $\alpha$ varies.  Over this range of $\alpha$,
the in-sample number of clusters is quite robust, but the posterior predictive
number of clusters is non-robust, ranging roughly from \Sexpr{sprintf("%2.1f",
min_enum_clust_pred)} to \Sexpr{sprintf("%2.1f", max_enum_clust_pred)} expected
species.  Our approximation captures this qualititative behavior. As expected,
the approximation is least accurate furthest from the $\alpha_0$ at which it is
evalutated.

\subsubsection*{Functional perturbations and the influence function}

\todo{(Bryan) Make it clear that we've already established that
the predictive is non-robust.  Now we do functional perturbations
to see whether the in-sample was robust only because we stayed in the
Beta family.  Say that, along the way, we demonstrate how the influence
function guides the choice of prior perturbations.}

The leftmost column of \figref{iris_fsens} shows in purple the influence
function, $\infl(\nu)$, as given by \corref{etafun_deriv_form}.  For ease of
visualization, we show the influence function in logit stick space, taking $\mu$
to be the Lebesgue measure on $\mathbb{R}^{\kmax - 1}$, and $\theta = (\lnu_1,
\ldots, \lnu_{\kmax - 1})$, suitably transforming the prior density.

We consider perturbations $\phi$ which are Gaussian bumps, with each
perturbation centered at a different location on the real line.  Each row of
\figref{iris_fsens} corresponds to a different $\phi$, which are shown in gray
in the left-hand column of \figref{iris_fsens}. The middle column of
\figref{iris_fsens} shows the stick-breaking prior $\p(\nuk \vert \phi)$ induced
by the corresponding $\phi$, and the rightmost column of \figref{iris_fsens}
shows the changes produced by the corresponding perturbation, as measured by
re-fitting and  as predicted by our approximation.

According to \corref{etafun_deriv_form}, the sign and magnitude of the effect of
a perturbation should be determined by its integral against the influence
function.  Thus, when $\phi$ lines up with a negative part of $\infl$, as in the
first row, we expect the change to be negative.  Similarly, we expect the
perturbation of the bottom row to produce a positive change, and the middle row,
in which $\phi$ overlaps with both negative and positive parts of the influence
function, to produce a relatively small change.  The third column of
\corref{etafun_deriv_form} confirms that this intuition holds, both for
the refits and linear approximation.

\todo{With the new compacted sections, we should use $\norminf{\cdot}$
rather than $L_\infty$.  I'm not really talking about $L_p$ spaces in the
text much.}

<<iris_fsens_cap>>=
iris_fsens_cap <- paste(
        "Sensitivity of
        the expected number of in-sample clusters
        in the iris data set
        to three multiplicative perturbations each with $\\norminf{\\phi} = 1$.
        (Left) The multiplicative perturbation $\\phi$ in grey.
        The influence function $\\Psi$ in purple,
        scaled to also have unit $L_{\\infty}$-norm.
        (Middle) The base and alternative priors $\\pbase(\\nuk)$
        and $\\palt(\\nuk)$.
        (Right) The effect of the perturbation
        on the change in expected number of in-sample clusters
        for $t \\in[0, 1]$.")

SetImageSize(aspect_ratio = 1.1 * base_aspect_ratio)
@
<<iris_fsens, cache=simple_cache, fig.show='hold', fig.cap=iris_fsens_cap>>=
source("R_scripts/iris/iris_func_sens.R", echo=knitr_debug, print.eval=TRUE)
@

<<iris_worstcase_cap>>=
iris_worstcase_cap <- paste(
        "Sensitivity of
        the expected number of in-sample clusters in the iris data set
        to the worst-case multiplicative perturbation with
        $\\norminf{\\phi} = 1$.")

SetImageSize(aspect_ratio = 0.525 * base_aspect_ratio)
@
<<iris_worstcase, cache=simple_cache, fig.show='hold', fig.cap=iris_worstcase_cap>>=
source("R_scripts/iris/iris_worst_case.R", echo=knitr_debug, print.eval=TRUE)
@

Finally, we consider the worst-case multiplicative perturbation with
$\norminf{\phi} = 1$, as given by \corref{etafun_worst_case}.  The panel
layout is the same as in \figref{iris_fsens}.
%
%
% The middle column
% of \figref{iris_worstcase} shows the prior density perturbed by the worst-case
% perturbation; the right column shows the effect on $\gclustersabbr$.
%

\todo{Say something about the fact that this is an unreasonable prior,
but the fact that it's still robust means this is fine.}
This worst-case perturbation has a much larger effect on $\gclustersabbr$
compared to the other unit $L_\infty$ norm perturbations in \figref{iris_fsens}.
However, even with the worst-case perturbation, the change in $\gclustersabbr$
is still small. We conclude that on the iris data set, $\gclustersabbr$ appears
to be a robust quantity for this model and dataset.

\subsubsection*{Timing}

\todo{I really think these timing results could all be in a single
compact table with minimal discussion.}

Forming the linear approximation at $\alpha_0$ required \Sexpr{sprintf('%1.1g',
alpha_hess_time)} seconds, and, after forming the linear approximation,
computing $\etalinglobal(\alpha)$ for the sequence of $\alpha$'s considered in
\figref{iris_alpha_sens} took another \Sexpr{sprintf('%1.1g',
total_alpha_lr_time)} seconds. In contrast, to refit $\etaopt(\alpha)$ for the
set of $\alpha$s took a total of \Sexpr{sprintf('%2.0f',
total_alpha_refit_time)} seconds, with a median refit time of
\Sexpr{sprintf('%1.1g', median_alpha_refit_time)} seconds.  In this case, the
approximation produced the same qualitative conclusions an order of magnitude
faster than refitting.

As with the concentration parameter, the approximation was much faster to
compute than the exact refits. Computing the linearized variational parameters
$\etalinglobal(\t)$ at $\t = 1$ (including the necessary Hessian solve) for a
given functional perturbation required \Sexpr{sprintf('%1.1g', wc_hessian_time +
wc_lr_time)} seconds. A refit using $\palt$ in place of $\pbase$ requires about
one second. The time to refit on the small dataset is, of course, small.
However, the order-of-magnitude difference in timing between the linear
approximation and refit will continue to hold for larger data analysis problems
below.

% \begin{table}[tb]
% \centering
% \caption{Compute time of results on the iris data set. }
% \begin{tabular}{|r|r|}
%     \hline
%     & time (seconds) \\
%     \hline
%     Initial fit & \Sexpr{sprintf('%1.2g', init_fit_time)} \\
%     \hline
%     Hessian solve for $\alpha$ sensitivity &
%         \Sexpr{sprintf('%1.2g', alpha_hess_time)}\\
%     Linear approx. $\eta^{lin}(\alpha)$ for $\alpha = 1, \ldots , 16$ &
%         \Sexpr{sprintf('%1.2g', total_alpha_lr_time)}\\
%     Refits $\eta(\alpha)$ for $\alpha = 1, \ldots , 16$ &
%         \Sexpr{sprintf('%1.2g', total_alpha_refit_time)}\\
%     \hline
%     The influence function & \Sexpr{sprintf('%1.2g', infl_time)}\\
%     Hessian solve for worst-case $\phi$ &
%         \Sexpr{sprintf('%1.2g', wc_hessian_time)}\\
%     Linear approx. $\eta^{lin}(\t)|_{\t = 1}$
%     for worst-case $\phi$ &
%         \Sexpr{sprintf('%1.2g', wc_lr_time)}\\
%     Refit $\eta(\t)|_{\t = 1}$ for worst-case $\phi$ &
%         \Sexpr{sprintf('%1.2g', wc_refit_time)}\\
%     \hline
% \end{tabular}
% \end{table}
