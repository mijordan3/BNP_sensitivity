%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do not edit the TeX file your work
% will be overwritten.  Edit the RnW
% file instead.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
<<setup, include=FALSE>>=
knitr_debug <- FALSE # Set to true to see error output
simple_cache <- TRUE # Set to true to cache knitr output for this analysis.
source("R_scripts/initialize.R", echo=FALSE)
source("R_scripts/plotting_utils.R", echo=FALSE)

# load data
load('./R_scripts/data_processed/iris.RData')

# some quantities we cite in the text
min_enum_clust <- min(alpha_sens_df$n_clusters_refit)
max_enum_clust <- max(alpha_sens_df$n_clusters_refit)

min_enum_clust_pred <- min(alpha_sens_df_pred$n_clusters_refit)
max_enum_clust_pred <- max(alpha_sens_df_pred$n_clusters_refit)

@

We demonstrate the local sensitivity computations on a 
Gaussian mixture model (GMM) of Fisher's iris data set. 
The data set is a sample of 150 iris flowers with 
four measurements taken per flower:  
sepal length, sepal width, petal length, and petal width. 
The GMM was detailed in \exref{iris_bnp_process}; here, the
data dimension $d = 4$. In the variational approximation, 
we set the truncation parameter $\kmax = 15$. 
\figref{iris_fit} shows the inferred clusters at $\alpha = 6$. 
The data set contains three iris species, and
the BNP model correspondingly identifies three dominant clusters. 

<<iris_fit_cap>>=
iris_fit_cap <- paste("The iris data in principal component space and 
                      GMM fit at $\\alpha = 6$. 
                      Colors denote inferred memberships and
                      ellipses represent estimated covariances. ")
SetImageSize(aspect_ratio = base_aspect_ratio, 
             image_width = 0.6)
@
<<iris_fit, cache=simple_cache, fig.show='hold', fig.cap=iris_fit_cap>>=
source('./R_scripts/iris/iris_init_fit.R', 
       echo=knitr_debug, print.eval=TRUE)
@

\subsubsection*{Parametric sensitivity}

We evaluate the sensitivity of the posterior number 
of clusters to the prior parameter $\alpha$.
The expected in-sample number of clusters $\gclusters$ and 
the expected predictive number of clusters $\gclusterspred$ 
were defined in~\exref{insample_nclusters, predictive_nclusters}, respectively; 
we do not use thresholding in computing the number of clusters 
(i.e. we set $\tau = 0$),
so  we simply write $\gclustersabbr$ and $\gclusterspredabbr$ for the 
in-sample and predictive quantities, respectively.
We formed the linear approximation at the initial $\alpha_0 = 6$ fit.
Without further optimization, we use the linear approximation 
(\eqref{global_sens, global_lin_approx}) to quickly evaluate 
$\etalinglobal(\alpha)$ for all $\alpha = 1, \ldots, 16$,
and produce posterior quantities $g(\etalinglobal(\alpha))$. 

The in-sample quantity appears less sensitive to changes in the $\alpha$ parameter
than the predictive equantity (\figref{iris_alpha_sens}). 
As $\alpha$ varies from $\alpha = 1, \ldots, 16$,
the quantity $\gclustersabbr(\etalinglobal(\alpha))$
varies only from \Sexpr{sprintf("%2.1f", min_enum_clust)} to
\Sexpr{sprintf("%2.1f", max_enum_clust)}
(recall that the true number of iris species is three). 
On the other hand, over the same range of $\alpha$,
the predictive quantity $\gclusterspredabbr(\etalinglobal(\alpha))$
varies from 
\Sexpr{sprintf("%2.1f", min_enum_clust_pred)} to 
\Sexpr{sprintf("%2.1f", max_enum_clust_pred)}. 


Subsequent refits at $\alpha\not=\alpha_0$ confirm the sensitivity conclusions 
of our linearized variational parameters. 
Over the range $\alpha = 1, \ldots, 16$,
the values $\g(\etalinglobal(\alpha))$ closely mimic the 
values $g(\etaopt(\alpha))$ found by refitting 
the variational parameters at each $\alpha$ (\figref{iris_alpha_sens}).
Importantly, 
the linear approximation is an order of magnitude faster than refitting. 
Forming the linear approximation at $\alpha = \alpha_0$, 
required \Sexpr{sprintf('%1.1g', alpha_hess_time)} seconds. 
After forming the linear approximation,
computing $\etalinglobal(\alpha)$ for all $\alpha = 1, \ldots, 16$ took another 
\Sexpr{sprintf('%1.1g', total_alpha_lr_time)} seconds.
On the other hand, to refit $\etaopt(\alpha)$ for the same range of
$\alpha$'s took a total of \Sexpr{sprintf('%2.0f', total_alpha_refit_time)} seconds, with a median refit time of \Sexpr{sprintf('%1.1g', median_alpha_refit_time)} seconds. 



<<beta_priors_cap>>=
# TODO: move this figure to the BNP section?
beta_priors_cap <- paste("Probability density functions of $\\text{Beta}(1, \\alpha)$ distributions, for various $\\alpha$. ")
SetImageSize(aspect_ratio = base_aspect_ratio * 0.75)
@
<<beta_priors, cache=simple_cache, fig.show='hold', fig.cap=beta_priors_cap>>=
# source('./R_scripts/iris/plot_beta_priors.R',
#        echo=knitr_debug, print.eval=TRUE)
@



<<iris_alpha_sens_cap>>=
      iris_alpha_sens_cap <- paste(
"The expected number of clusters as $\\alpha$ varies in the 
the GMM fit of the iris data. 
On the left is in-sample quantity $\\gclustersabbr$.  
On the right is the the predictive quantity $\\gclusterspredabbr$. 
We formed the linear approximation at $\\alpha=6$. 
In red, the expected number of clusters computed under the
linearly approximated variational parameters (red).
In blue, the expected number of clusters obtained by refitting the model at each $\\alpha$. ")

SetImageSize(aspect_ratio = 0.7 * base_aspect_ratio, 
             image_width = 0.8)
@
<<iris_alpha_sens, cache=simple_cache, fig.show='hold', fig.cap=iris_alpha_sens_cap>>=
source("R_scripts/iris/iris_alpha_sens.R", echo=knitr_debug, print.eval=TRUE)
@


% 
% - we demonstrate the utility of the influence function 
% - figure ref top three rows shows three different functional perturbations. 
% - can't really tell from densities how these will affect posterior statistic
% - but they do in fact have very different effects (in sign and size). 
% - but their effects make sense when looking at the influence function
% 
% -last row is worst-case

\subsubsection*{Functional perturbations and the influence function}

We demonstrate the ability of the influence function to 
provide guidance on the anticipated effect of potential perturbations 
on the expected number of in-sample clusters. 
Recall from \eqref{vb_eta_infl_sens} that the local sensitivity can 
be represented as an inner product between the influence function $\infl$
and a multiplicative perturbation $\phi$;
the inner product takes the form,
\begin{align}\eqlabel{recall_infl_innerprod}
\fracat{d g(\etaopt(\t \phi))}{d \t}{0} ={}&
    \int \infl_p(\lnuk) \phi(\lnuk) \mu(d\lnuk),
\end{align}
where we expressed both the influence function and the perturbation in logit-stick space, $\lnuk := \log(\nuk) - \log(1 - \nuk)$. 

In order to illustrate this inner product,
we consider functional perturbations $\phi_\mu$ 
which are Guassian bumps, with each perturbation centered at a
different $\mu$ on the real line (\figref{iris_fsens} left column). 
The perturbed priors are
$p(\lnu_k|\t) = p_0(\lnu_k)\exp(\t\phi_\mu(\lnu_k))$.
The middle column of \figref{iris_fsens} displays the initial density
$p_0(\nu_k) = \betadist{\nu_k\vert 1, \alpha_0}$
along with the perturbed densities
$p(\nu_k|\t = 1)$ for different choices of $\mu$, 
in the original $(0, 1)$ constrained space.


% The perturbations of the form 
% $\phi_\mu(x) = e^{2(x - \mu)^2}$, with each perturbation having a 
% different mode $\mu$ (\figref{iris_fsens} left column). 
% The perturbations are multiplicative with perturbed prior 
% $p(\nu_k|\t) = p_0(\nu_k)\exp(\t\phi_\mu(\nu_k))$.
% The middle column of \figref{iris_fsens} displays the initial density,
% $p_0(\nu_k) = \betadist{\nu_k\vert 1, \alpha_0}$,
% along with the perturbed densities
% $p(\nu_k|\t = 1)$ for different choices of $\mu$. 

Each perturbation $\phi_\mu$ with a different choice of $\mu$ 
produces distinct changes in the expected number
of in-sample clusters $\gclustersabbr$. 
The right column of \figref{iris_fsens} plots the differences
$\Delta\gclustersabbr(\eta(\t)) := 
\gclustersabbr(\eta(\t)) - \gclustersabbr(\etaopt(0))$ 
for $\t\in[0,1]$. 
Depending on the perturbation, 
$\Delta\gclustersabbr$ can be positive, negative, or nearly zero. 
In each case, the approximation 
$\Delta\gclustersabbr(\etalinglobal(\t))$
is able to mirror the qualitative behavior of 
$\Delta\gclustersabbr(\etaopt(\t))$
observed by refitting. 

While each perturbation produces distinct changes in $\gclustersabbr$, 
it is unclear how to anticipate the effect of a perturbation 
by comparing the original and perturbed densities alone. 
On the other hand, the sign and magnitude of the change in $\gclustersabbr$ after 
prior perturbation are well-explained by its influence function, 
plotted in purple in the left column of \figref{iris_fsens}, 
and the representation of local sensitivity as an inner product
(\ref{recall_infl_innerprod}). 
When $\phi_\mu$ is centered at a location where the influence function is negative, the effect of the perturbation on $\gclustersabbr$ 
is negative (\figref{iris_fsens} top row); 
conversely, when $\phi_\mu$ is centered at a location where the influence function is positive, its effect on $\gclustersabbr$ is positive (bottom row); 
finally, when $\phi_\mu$ is centered at a location where the influence transitions from negative to positive, its effect on
$\gclustersabbr$ is roughly zero (middle row). 
In the last case, $\phi_\mu$ placed approximately equal weight on the negative and positive portions of the prior-weighted influence function, resulting in an approximately zero inner-product. 
In the other data applications below, 
the influence function will guide our choice of functional perturbation and explain why some perturbations result in greater sensitivity than others. 

Finally, we consider the 
worst-case multiplicative perturbation with unit $L_\infty$ norm.
Recall that the worst-case perturbation with unit $L_\infty$ norm is a 
step-function taking on values $\pm1$ corresponding 
to the sign of the influence function (\figref{iris_worstcase} left).  
The middle column of \figref{iris_worstcase} shows the prior density perturbed by the worst-case perturbation; 
the right column shows the effect on $\gclustersabbr$. 
This worst-case perturbation has a much larger effect on
$\gclustersabbr$ compared to the other unit $L_\infty$ norm perturbations in
\figref{iris_fsens}. 
However, even with the worst-case perturbation, 
the change in $\gclustersabbr$ is still small.
We conclude that on the iris data set, $\gclustersabbr$ appears to be a quantity insensitive to the prior under a Gaussian mixture model. 


<<iris_fsens_cap>>=
iris_fsens_cap <- paste(
        "Sensitivity of
        the expected number of in-sample clusters 
        in the iris data set
        to three multiplicative perturbations with 
        unit $L_{\\infty}$-norm.
        (Left) the multiplicative perturbation $\\phi$ in grey.        
        The influence function $\\Psi$ in purple, 
        scaled to also have unit $L_{\\infty}$-norm. 
        (Middle) the original prior density $\\p_0$ and 
        the perturbed prior density $\\p_t = \\p_0\\times \\exp(t \\phi)$
        at $t = 1$. 
        (Right) the effect of the perturbation 
        on the change in expected number of in-sample clusters 
        as $t\\rightarrow1$.")

SetImageSize(aspect_ratio = 1.1 * base_aspect_ratio)
@
<<iris_fsens, cache=simple_cache, fig.show='hold', fig.cap=iris_fsens_cap>>=
source("R_scripts/iris/iris_func_sens.R", echo=knitr_debug, print.eval=TRUE)
@

<<iris_worstcase_cap>>=
iris_worstcase_cap <- paste(
        "Sensitivity of
        the expected number of in-sample clusters in the iris data set
        to the worst-case multiplicative perturbation with 
        unit $L_{\\infty}$-norm.")

SetImageSize(aspect_ratio = 0.525 * base_aspect_ratio)
@
<<iris_worstcase, cache=simple_cache, fig.show='hold', fig.cap=iris_worstcase_cap>>=
source("R_scripts/iris/iris_worst_case.R", echo=knitr_debug, print.eval=TRUE)
@

Computing the linearized variational parameters $\etalinglobal(\t)$ at $\t = 1$, 
including the necessary Hessian solve, 
for a given functional perturbation 
required \Sexpr{sprintf('%1.1g', wc_hessian_time + wc_lr_time)} seconds. 
A refit at $\t = 1$ requires about one second. 
While a second for a refit is not exceedingly large, the
order-of-magnitude difference in timing between the linear approximation and refit 
will continue to hold for larger data analysis problems below. 
In general, the speed of the linear approximation allows us to quickly 
explore many different potential functional perturbations when 
refitting for each perturbation becomes prohibitive. 


% \begin{table}[tb]
% \centering
% \caption{Compute time of results on the iris data set. }
% \begin{tabular}{|r|r|}
%     \hline 
%     & time (seconds) \\ 
%     \hline 
%     Initial fit & \Sexpr{sprintf('%1.2g', init_fit_time)} \\
%     \hline 
%     Hessian solve for $\alpha$ sensitivity & 
%         \Sexpr{sprintf('%1.2g', alpha_hess_time)}\\
%     Linear approx. $\eta^{lin}(\alpha)$ for $\alpha = 1, \ldots , 16$ & 
%         \Sexpr{sprintf('%1.2g', total_alpha_lr_time)}\\
%     Refits $\eta(\alpha)$ for $\alpha = 1, \ldots , 16$ & 
%         \Sexpr{sprintf('%1.2g', total_alpha_refit_time)}\\
%     \hline 
%     The influence function & \Sexpr{sprintf('%1.2g', infl_time)}\\ 
%     Hessian solve for worst-case $\phi$ & 
%         \Sexpr{sprintf('%1.2g', wc_hessian_time)}\\
%     Linear approx. $\eta^{lin}(\t)|_{\t = 1}$
%     for worst-case $\phi$ & 
%         \Sexpr{sprintf('%1.2g', wc_lr_time)}\\
%     Refit $\eta(\t)|_{\t = 1}$ for worst-case $\phi$ & 
%         \Sexpr{sprintf('%1.2g', wc_refit_time)}\\ 
%     \hline 
% \end{tabular}
% \end{table}

