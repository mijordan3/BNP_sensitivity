In each of the BNP models described in the examples above,
the exact posterior distribution $\p(\beta, \nu, \z \vert \x)$
is intractable.
Variational Bayes (VB) is an approach that seeks an approximate posterior
through solving a numerical optimization problem
\citep{jordan:1999:vi, wainwright:2008:graphical_models, blei:2017:vi_review}.
Let $\zeta := (\beta, \z, \nu)$ denote the full vector of
latent variables.
VB specifies a
family of approximating distributions $\q(\zeta \vert \eta)$
parameterized by a finite-dimensional vector $\eta \in \etadom \subseteq
\mathbb{R}^{\etadim}$
and solves for $\q(\zeta\vert\etaopt)$
that is closest to the posterior $\p(\zeta \vert \x)$ in
Kullback-Leibler (KL) divergence:
%
\begin{align}\eqlabel{vb_optimization}
%
\etaopt :={}&
    \argmin_{\eta \in \etadom}
        \KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)} \mathwhere \\
\KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}
={}&    \expect{\q(\zeta \vert \eta)}{
        \log \q(\zeta \vert \eta) - \logp(\x, \zeta)} + \logp(\x). \nonumber
%
\end{align}
%
As we discuss
below, we will choose $\q(\zeta \vert \eta)$ so that we can easily evaluate (or approximate)
the expectation with respect to $\q(\zeta \vert \eta)$
as a closed-form function of $\eta$.
Notice that the intractable $\logp(\x)$ term does not depend on $\eta$, and
so can be negelcted in the optimziation.

In practice, forming an approximating posterior for BNP
can be challenging since the latent variables $\nu$ and $\beta$ are
(countably) infinite dimensional.
We would like to keep dimension of the variational parameter $\eta$ finite
in order for the optimizization in \eqref{vb_optimization} to be tractable.
In the present paper, we will follow \citet{blei:2006:vi_for_dp} and
use a truncated stick-breaking representation in
the variational distribution.
We choose a truncation parameter $\kmax$ large but finite,
and we set $\q(\nu_\k = 1 | \eta) = 1$ for all $\k > \kmax$.
This implies that under $\q$, $\pi_\k = 0$ with probability one
for all $\k > \kmax$
(\eqref{stick_breaking}).
Correspondingly, we also set $\q(\z_{\n\k} = 0 | \eta) = 1$ for $\k > \kmax$.

For the generic BNP model in \eqref{bnp_model},
we propose a mean-field variational approximating family of the following form:
%
\begin{align}\eqlabel{vb_mf}
%
\q(\zeta \vert \eta) =
    \left( \prod_{\k=1}^{\kmax - 1} \q(\nuk \vert \eta) \right)
    \left( \prod_{\k=1}^{\kmax} \q(\beta_\k \vert \eta) \right)
    \left( \prod_{\n=1}^{\N} \q(\z_{\n} \vert \eta) \right).
%
\end{align}
Because $\pi_\k = 0$ for all $\k > \kmax$,
we can ignore the latent variables $\beta_\k$ for $\k > \kmax$ in
defining our variational approximation.

Notice that the model (\eqref{bnp_model}) itself is not finite. The truncation
parameter $\kmax$ only appears in our variational approximation.
We set $\kmax$ large enough to ensure that a large
proportion of the components are unoccupied with high probability under $\q$,
in which case the truncation approximates the fully nonparametric model with $\kmax = \infty$.

The variational approximation for the topic model (\exref{structure_bnp_process})
is similarly mean-field: the distribution on stick-breaking proportions $\nu$
factorizes over
both individuals $\n$ and components $\k$, while the
assignments $\z$
factorize over individuals $\n$, loci $\l$, and chromosomes $\i$.
For the regression model (\exref{mice_bnp_process}), all terms in the
variational approximation fully-factorize
except for the cluster assignments $\z$ and additive shifts $\b$.
We allow for conditional dependence so that
$\q(\z, \b \vert \eta)$ does not factorize into
$\q(\z \vert \eta)\q(\b \vert \eta)$,
for purposes that will be clear below.

\subsubsection{Conditional conjugacy}

For $\z$ and $\beta$ in all models we consider, we will be always be able to
take advantage of conditional conjugacy. Specifically, we will take $\q(\z_{\n}
\vert \eta_{\z_\n})$ to be multinomial, matching $\p(\z_{\n}
\vert \x, \beta, \nu)$, and we will take $\q(\beta_\k \vert \eta_{\beta_\k})$
to match the distribution of $\p(\beta_{\k} \vert \x, \z, \nu)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\beta_\k$ in a GMM]\exlabel{iris_var_distr}
%
In \exref{iris_bnp_process}, $\beta_\k = (\mu_\k, \Lambda_\k)$ and the likelihoods are Gaussian,
$\p(\x_\n \vert \beta_\k) = \normdist{\x_n \vert \mu_\k, \Lambda_\k^{-1}}$.
The prior $\pbetaprior(\beta_\k)$ a normal-Wishart.

The conditionally conjugate variational distribution on $\beta_\k$ is
also normal-Wishart, $\q(\beta_\k \vert \eta) = \normalwishart{\beta_k \vert \eta}$.
With this choice of $\q$, the expectations
\begin{align*}
\expect{\q(\beta_\k \vert \eta)}{\log \p(\x_\n \vert \beta_\k)} \quad
\expect{\q(\beta_\k \vert \eta)}{\log \pbetaprior(\beta_\k)} \quad
\expect{\q(\beta_\k \vert \eta)}{\log \q(\beta_\k \vert \eta)}
\end{align*}
all have closed form expressions as functions of $\eta$.
%
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Under the mean-field factorization,
the vector $\eta$ will partition into parameters
governing $\nu$, $\beta$, and $\z$. Let the parameters governing a
particular latent variable or latent vector be denoted with a subscript: for example,
$\q(\beta \vert \eta) = \q(\beta \vert \etabeta)$,
$\q(\z_\n \vert \eta) = \q(\z \vert \eta_{\z_{\n}})$, and so on.
With conditionally conjugate distributions and our mean-field assumption,
the parameters $\eta_{\z_{\n}}$ can be optimally set as a function of
parameters $\etabeta$ and $\etanu$.
The next example details this point.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\z_\n$ in a GMM]\exlabel{qz_form}
%
The conditionally conjugate variational distribution for $\z_\n$
is multinomial.
Our variational approximation is truncated at $\kmax$ so
$\z_{\n\k} = 0$ for all $\k > \kmax$;
as a result,
the multinomial distribution on $\z_\n$ has $\kmax$ discrete categories.


We parameterize the the multinomial distribution using the natural parameterization
in its exponential family form. That is,
we let $\eta_{\z_\n} = (\rho_{\n1}, \rho_{\n2}, ..., \rho_{\n(\kmax-1)})$
be an unconstrained vector in $\mathbb{R}^{\kmax-1}$;
in this parameterization, the multinomial expectations are
%
\begin{align*}
  p_{\n\k} := \expect{\q(\z_\n \vert \etaz)}{\z_{\n\k}} =
  \frac{\exp(\rho_{\n\k})}{1 + \sum_{\k'=1}^{\kmax-1}\exp(\rho_{\n\k})}
\end{align*}
%
We use the exponential family parameterization because
we will require the optimal variational parameters $\etaopt$
to be interior to $\etadom$ in our sensitivity analysis
(\secref{local_sensitivity}).
In the mean parameterization,
$\sum_{\k=1}^\kmax p_{\n\k} = 1$, and so the
optimal mean parameters $p_{\n}$ cannot be
interior to $\Delta^{\kmax - 1}$.
On the other hand, $\eta_{\z_\n}$ as defined
is unconstrained in $\mathbb{R}^{\kmax - 1}$.

Moreover, for fixed $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$,
the parameter vector $\eta_{\z_\n}$ that minimizes \eqref{vb_optimization}
has a closed form. Define
%
\begin{align*}
\tilde \rho_{\n\k} = \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
Then the optimal $\eta_{\z_\n}$ is given by
\todo{probably needs more discussion here. we can probably cite BLEI VI review for stats
and still ignore a lot of details}
%
\begin{align*}
%
\etaopt_{\z_\n} = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}},
\log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots,
\log\frac{\tilde\rho_{\n(\kmax-1)}}{\tilde\rho_{\n\kmax}}\right)
%
\end{align*}
%
Thus, as long as the expectation $\tilde\rho_{\n\k}$ has a closed-form as a function of
$(\etabeta, \etanu)$, the optimal $\etaopt_{\z_\n}$ can be also be set in closed-form as
a function of $(\etabeta, \etanu)$.
%
\end{ex}

For fixed $(\etabeta, \etanu)$, the option of setting $\etaz$ at its optimum
extends beyond the GMM example and will play
a key role in computing our local sensitivity
measures in practice (\secref{computing_sensitivity}).
In greater generality, each of our example models
has latent variables that factorize in a global/local structure.
In the GMM example discussed above, we call the variables $(\beta, \nu)$ ``global"
because they are shared across all data points; the $\z$ is ``local"
because each $\z_\n$ is unique to a single data point.
In the regression model (\exref{mice_bnp_process}),
the global variables are again $(\beta, \nu)$,
but the local variables comprise of both the cluster assignments $\z$ and additive shifts $\b$.
In these two models, notice that the global variables scale with $\kmax$, while
local variables scale with the number of observations $\N$.

In the topic model (\exref{structure_bnp_process}),
we still call $(\beta, \nu)$ the global latent variables, even though they scale
with the number of individuals $\N$;
they do not, however, scale with both the number of individuals and the number of loci
like $\z$ does. In the topic model, we call $\z$ the local latent variables.

Let $\gamma = (\beta,\nu)$ be the global latent variables
and let $\etaglob = (\etabeta, \etanu)$ be their variational parameters.
Let $\etalocal$ be the local variational parameters. In \exref{iris_bnp_process} and
\exref{structure_bnp_process}, $\etalocal = \etaz$, while
in \exref{mice_bnp_process}, $\etalocal = (\etaz, \eta_\b)$.

In each case, for $\etaglob$ fixed, the optimal $\etalocal$ that minimizes
the $\mathrm{KL}$ can be set in closed form as a function of $\etaglob$.
The multinomial parameters for $\eta_{\z_\n}$ in the regression and topic models
can be set in an analagous way as described in \exref{qz_form}.
For more details concerning the optimal shift parameters $\eta_\b$
in the regression model, see \appref{put_in_appendix}.





\subsubsection{Evaluating stick expectations}

To evaluate the $\mathrm{KL}$ in \eqref{vb_optimization}, we also need
the expectations
\begin{align}\eqlabel{stick_expectations}
%
\expect{\q(\nuk \vert \eta)}{\log \nuk}
\textrm{,}\quad
\expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
\textrm{,}\quad\textrm{and}\quad
\expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
%
\end{align}
The first two expectations come from decomposing the mixture weights
$\expect{}{\log \pi}$ into its component stick-breaking proportions.

If the prior $\pstick(\nuk)$ were Beta-distributed like in the standard GEM construction,
then the conditionally conjugate distribution for $\q(\nuk \vert \eta)$ would also be Beta.
In this case, all the displayed expectations in \eqref{stick_expectations}
can be computed analytically as a function of
the Beta parameters in the variataional approximation.

However, we will be considering stick-breaking distributions $\pstick(\nuk)$ that
are outside the family of Beta distributions.
To accomodate a generic prior $\pstick(\nuk)$,
we approximate the expecations in \eqref{stick_expectations},
with numerically.
Each expectation is a univariate integral.
A particularly easy approximation method is
Gauss-Hermite (GH) quadrature, which we now describe.

To take advantage of GH quadrature, we first logit transform the stick-breaking
proportion $\nuk$ so that the transformed variable
\begin{align*}
  \lnuk := \log\left(\frac{\nuk}{1 - \nuk}\right)
\end{align*}
is not contrained to be between $(0, 1)$ and can take values in all of $\mathbb{R}$.
Let $\s$ be the sigmoid function,
which provides the inverse transformation,
\begin{align*}
  \nuk = \s(\tilde\nuk) := \frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}.
\end{align*}

We let $\q(\lnu_\k \vert \eta)$ be normally distributed with
location parameter $\lnumean_\k$ and scale parameter $\lnusd_\k$.
This then induces a logit-normal
distribution on our original variable of interest, $\nuk$.

With this choice of a logit-normal $\q(\nuk \vert \eta)$,
we can approximate expectations of smooth functions
$f(\nuk)$ (such as $\pstick(\nuk)$) using GH quadrature with $\ngh$ knots,
located at $\xi_g$, weighted by $\omega_g$:
%
\begin{align}\eqlabel{gh_integral}
%
\expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
\expect{\q(\lnu_\k \vert \eta)}
       {f\circ \s\left(\lnu_\k\right)}
\nonumber\\\approx{}&
    \sum_{g=1}^{\ngh} \omega_g f\circ \s \left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
 \nonumber\\=:{}&
\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
%
\end{align}
%
Conveniently, $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$ is a deteriministic, differentiable
function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.  (This
technique is similar to the ``reparameterization trick,'' only using
GH points rather than standard normal draws).

% \subsection*{subsection name}
%
% we choose $\q(\nuk \vert \eta)$ to be logit-normally distributed,
% and express the expectations in \eqref{stick_expectations} as Gaussian integrals.
% Define
% \begin{align*}
%   \tilde \nuk := \log\left(\frac{\nuk}{1 - \nuk}\right),
% \end{align*}
% which will be normally distributed under our choice of a
% logit-normal $\q(\nuk \vert \eta)$.
%
% Let $\lnumean_\k$ and $\lnusd_\k$ be entries of $\eta$ corresponding to
% the logit-normal parameters of $\nuk$.
%
%
% In order to optimize the variational objective \eqref{vb_optimization} we see
% from \eqref{stick_log_post} that we need to evaluate or approximate expectations
% of the form
% %
% \begin{align*}
% %
% \expect{\q(\nuk \vert \eta)}{\log \nuk}
% \textrm{,}\quad
% \expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
% \textrm{,}\quad\textrm{and}\quad
% \expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
% %
% \end{align*}
%
%
%
%
% First, define a version of $\nuk$ that is not constrained to $(0,1)$:
% %
% \begin{align}\eqlabel{lnuk_transform}
% %
% \lnuk :={} \log \left( \frac{\nuk}{1 - \nuk} \right)
% \quad\Leftrightarrow\quad
% \nuk :={} \frac{\exp(\lnuk)}{1 + \exp(\lnuk)}.
% %
% \end{align}
% %
% It will be useful later to have at hand the transform between densities
% expressed in the space of $\nu$ and $\lnu$, which is given by
% %
% \begin{align}\eqlabel{lnuk_derivatives}
% %
% \fracat{d \lnu_\k}{ d\nuk}{\nuk} ={}
% %     \frac{1-\nuk}{\nuk}
% %     \left(\frac{1}{1 - \nuk} + \frac{\nuk}{(1 - \nuk)^2} \right)
% % \\={}& \frac{1}{\nuk} + \frac{1}{1 - \nuk}
% % \\={}&
%     \frac{1}{\nuk (1 - \nuk)} \mathand
% %
% \fracat{d \nuk}{ d\lnuk}{\lnuk} ={}
%     \frac{\exp(\lnuk)}{(1 + \exp(\lnuk))^2}.
% %
% \end{align}
% %
% We wish to let $\lnu_\k$ be distributed normally under the variational
% distribution.  Let $\lnumean_\k$ and $\lnusd_\k$ be entries of the parameter
% vector $\eta$, and write
% %
% \begin{align}\eqlabel{lnuk_vb_approximation}
% %
% \q(\lnu_\k \vert \eta) ={}& \normdist{\lnu_\k \vert \lnumean_\k, \lnusd_\k}
% \Rightarrow \\
% \q(\nuk \vert \eta) ={}&
%     \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\fracat{d \lnu_\k}{ d\nuk}{\nuk}\right|
% \nonumber\\={}&
% \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\frac{1}{\nuk (1 - \nuk)}\right|.
% \nonumber
% %
% \end{align}
% %
% Given this, we can approximate expectations of smooth functions
% $f(\nuk)$ using GH quadrature with $\ngh$ knots,
% located at $\xi_g$, weighted by $\omega_g$:
% %
% \begin{align}\eqlabel{gh_integral}
% %
% \expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
% \expect{\q(\lnu_\k \vert \eta)}
%        {f\left(\frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}\right)}
% \nonumber\\\approx{}&
%     \sum_{g=1}^{\ngh} \omega_g f\left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
%  \nonumber\\=:{}&
% \expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
% %
% \end{align}
% %
% Conveniently, $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$ is a differentiable
% function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.  (This
% technique is similar to the ``reparameterization trick,'' only using
% GH points rather than standard normal draws.)
