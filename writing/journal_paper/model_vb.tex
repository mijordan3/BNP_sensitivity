The posterior of a BNP model is difficult to compute for two reasons: because
the number of components is (countably) infinite, and because the posterior
normalization constant is intractible.  To address these problems, we follow
\citet{blei:2006:vi_for_dp} and form a mean field truncated variational
approximation to the posterior.

Let $\zeta$ denote the full vector of unkown posterior variables. For example,
in the GMM model of \exref{iris_bnp_process}, $\zeta := (\beta, \z, \nu)$.  The
exact posterior distribution $\p(\zeta \vert \x)$ is intractable. Variational
Bayes (VB) is an approach that seeks an approximate posterior through solving a
numerical optimization problem \citep{jordan:1999:vi,
wainwright:2008:graphical_models, blei:2017:vi_review}.

VB specifies a family of approximating distributions $\q(\zeta \vert \eta)$
parameterized by a finite-dimensional vector $\eta \in \etadom \subseteq
\mathbb{R}^{\etadim}$ and solves for $\q(\zeta\vert\etaopt)$ that is closest to
the posterior $\p(\zeta \vert \x)$ according to a divergence measure on
posterior distributions. We will make the common choice of Kullback-Leibler (KL)
divergence:
%
\begin{align}\eqlabel{kl_def}
%
\KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}
={}&    \expect{\q(\zeta \vert \eta)}{
        \log \q(\zeta \vert \eta) - \logp(\x, \zeta)} + \logp(\x).
%
\end{align}
%
As we discuss below, we will choose $\q(\zeta \vert \eta)$ so that we can easily
approximate the above expectation with respect to $\q(\zeta \vert \eta)$ as a
closed-form function of $\eta$.  We will write $\KL{\eta}$ for our
approximation, choosing the optimal variational parameter $\etaopt$ to satisfy
%
\begin{align}\eqlabel{vb_optimization}
%
\etaopt :={} \argmin_{\eta \in \etadom} \KL{\eta} \mathwhere
\KL{\eta} \approx{} \KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)},
%
\end{align}
%
where the tractable objective function $\KL{\eta}$ may not be an exact KL
divergence. Notice that the intractable $\logp(\x)$ term does not depend on
$\eta$, and so can be neglected in the objective function $\KL{\eta}$.

In practice, forming an approximating posterior for BNP can be challenging since
the latent variables $\nu$ and $\beta$ are (countably) infinite dimensional. We
would like to keep dimension of the variational parameter $\eta$ finite in order
for the optimization in \eqref{vb_optimization} to be tractable. In the present
paper, we will follow \citet{blei:2006:vi_for_dp} and use a truncated
stick-breaking representation in the variational distribution. We choose a
truncation parameter $\kmax$ large but finite,
%
\footnote{For $\k > \kmax$, $\q(\nu_\k)$ is effectively a point mass  but
$\p(\nu_\k \vert \x)$ is dominated by the Lebesgue measure.  So the KL
divergence $\KL{\q(\nu_\k) || \p(\nu_\k \vert \x)}$ is not well-defined, even
though $\q(\nu_\k)$ does form a sensible approximation to $\p(\nu_\k \vert \x)$
in measures of posterior divergence such as the Wasserstein distance.  This is
one sense in which our tractable objective function $\KL{\eta}$ is not a proper
KL divergence. }
%
and we set $\q(\nu_\k = 1 | \eta) =
1$ for all $\k > \kmax$. This implies that under $\q$, $\pi_\k = 0$ with
probability one for all $\k > \kmax$ (\eqref{stick_breaking}). Correspondingly,
we also set $\q(\z_{\n\k} = 0 | \eta) = 1$ for $\k > \kmax$.

Notice that only our variational approximation is truncated---the model
(\eqref{bnp_model}) itself is not finite. We set $\kmax$ large enough in our
variational approximation to ensure that a large proportion of the components
are unoccupied with high probability under $\q$, in which case the truncation
approximates the fully nonparametric model with $\kmax = \infty$.

For the generic BNP mixture model in \eqref{bnp_model}, we use a mean-field
variational approximating family of the following form:
%
\begin{align}\eqlabel{vb_mf}
%
\q(\zeta \vert \eta) =
    \left( \prod_{\k=1}^{\kmax - 1} \q(\nuk \vert \eta) \right)
    \left( \prod_{\k=1}^{\kmax} \q(\beta_\k \vert \eta) \right)
    \left( \prod_{\n=1}^{\N} \q(\z_{\n} \vert \eta) \right).
%
\end{align}
%
Because $\pi_\k = 0$ for all $\k > \kmax$, we can ignore the latent variables
$\beta_\k$ for $\k > \kmax$ in defining our variational approximation.

Given an expected posterior quantity of interest as in
\exref{insample_nclusters_simple}, we can simply approximate it with the
corresponding variational expectation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}\exlabel{insample_nclusters_simple}
%
Given a solution $\etaopt$ to \eqref{vb_optimization}, the intractable posterior
expectation in \exref{insample_nclusters_simple} can be approximated as
a funciton of $\etaopt$:
%
\begin{align*}
%
\g(\etaopt) :={}
    \expect{\q(\z\vert\etaopt)}{\nclusters_0(\z)} \approx
    \expect{\p(\z\vert\x)}{\nclusters_0(\z)}.
%
\end{align*}
%
In fact, given the mean field factorization of \eqref{vb_mf}, the function
$\g(\etaopt)$ has a particularly simple form.  Since each $\z_{\n\k}$ is either
zero or one, $\sum_{\n=1}^N \z_{\n\k} > 0$ if and only if $\prod_{\n=1}^\N (1 -
\z_{\n\k}) = 0$, so we can re-write the quantity of interest as
%
\begin{align*}
%
\nclusters_0(\z) ={}&
    \sumk \left(1 -  \prod_{\n=1}^\N (1 - \z_{\n\k})\right).
%
\end{align*}
%
Then, by the mean field assumption,
%
\begin{align*}
%
g(\etaopt) ={}
\expect{\q(\z \vert \etaopt)}{\nclusters_0(\z)} ={}
    \sumk \left(1 -  \prod_{\n=1}^\N
        (1 - \expect{\q(\z \vert \etaopt)}{\z_{\n\k}})\right).
%
\end{align*}
%
It will be useful later to observe that the map $\eta \mapsto \g(\eta)$ is
differentiable whenever $\eta \mapsto \expect{\q(\z \vert \eta)}{\z_{\n\k}}$
is differentiable.
%
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We will discuss more details of the VB approximation, including the form of the
mean field factors and the evaluation of $\KL{\eta}$ in more detail below in
\secref{computing_sensitivity}, as well as in the experiments of
\secref{results}.
