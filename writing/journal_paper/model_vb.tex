The posterior of a BNP model is difficult to compute for two reasons: because
the number of components is (countably) infinite, and because the posterior
normalization constant is intractible.  To address these problems, we follow
\citet{blei:2006:vi_for_dp} and form a mean field truncated variational
approximation to the posterior.

Let $\zeta$ denote the full vector of unkown posterior variables. For example,
in the GMM model of \exref{iris_bnp_process}, $\zeta := (\beta, \z, \nu)$.  The
exact posterior distribution $\p(\zeta \vert \x)$ is intractable. Variational
Bayes (VB) is an approach that seeks an approximate posterior through solving a
numerical optimization problem \citep{jordan:1999:vi,
wainwright:2008:graphical_models, blei:2017:vi_review}.

VB specifies a family of approximating distributions $\q(\zeta \vert \eta)$
parameterized by a finite-dimensional vector $\eta \in \etadom \subseteq
\mathbb{R}^{\etadim}$ and solves for $\q(\zeta\vert\etaopt)$ that is closest to
the posterior $\p(\zeta \vert \x)$ according to a divergence measure on
posterior distributions. We will make the common choice of Kullback-Leibler (KL)
divergence:
%
\begin{align*}
%
\KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}
={}&    \expect{\q(\zeta \vert \eta)}{
        \log \q(\zeta \vert \eta) - \logp(\x, \zeta)} + \logp(\x). \nonumber
%
\end{align*}
%
As we discuss below, we will choose $\q(\zeta \vert \eta)$ so that we can easily
approximate the above expectation with respect to $\q(\zeta \vert \eta)$ as a
closed-form function of $\eta$.  We will write $\KL{\eta}$ for our
approximation, choosing the optimal variational parameter $\etaopt$ to satisfy
%
\begin{align}\eqlabel{vb_optimization}
%
\etaopt :={} \argmin_{\eta \in \etadom} \KL{\eta} \mathwhere
\KL{\eta} \approx{} \KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)},
%
\end{align}
%
where the tractable objective function $\KL{\eta}$ may not be an exact KL
divergence. Notice that the intractable $\logp(\x)$ term does not depend on
$\eta$, and so can be neglected in the objective function $\KL{\eta}$.

In practice, forming an approximating posterior for BNP can be challenging since
the latent variables $\nu$ and $\beta$ are (countably) infinite dimensional. We
would like to keep dimension of the variational parameter $\eta$ finite in order
for the optimization in \eqref{vb_optimization} to be tractable. In the present
paper, we will follow \citet{blei:2006:vi_for_dp} and use a truncated
stick-breaking representation in the variational distribution. We choose a
truncation parameter $\kmax$ large but finite,
%
\footnote{For $\k > \kmax$, $\q(\nu_\k)$ is effectively a point mass  but
$\p(\nu_\k \vert \x)$ is dominated by the Lebesgue measure.  So the KL
divergence $\KL{\q(\nu_\k) || \p(\nu_\k \vert \x)}$ is not well-defined, even
though $\q(\nu_\k)$ does form a sensible approximation to $\p(\nu_\k \vert \x)$
in measures of posterior divergence such as the Wasserstein distance.  This is
one sense in which our tractable objective function $\KL{\eta}$ is not a proper
KL divergence. }
%
and we set $\q(\nu_\k = 1 | \eta) =
1$ for all $\k > \kmax$. This implies that under $\q$, $\pi_\k = 0$ with
probability one for all $\k > \kmax$ (\eqref{stick_breaking}). Correspondingly,
we also set $\q(\z_{\n\k} = 0 | \eta) = 1$ for $\k > \kmax$.

For the generic BNP mixture model in \eqref{bnp_model},
we propose a mean-field variational approximating family of the following form:
%
\begin{align}\eqlabel{vb_mf}
%
\q(\zeta \vert \eta) =
    \left( \prod_{\k=1}^{\kmax - 1} \q(\nuk \vert \eta) \right)
    \left( \prod_{\k=1}^{\kmax} \q(\beta_\k \vert \eta) \right)
    \left( \prod_{\n=1}^{\N} \q(\z_{\n} \vert \eta) \right).
%
\end{align}
%
Because $\pi_\k = 0$ for all $\k > \kmax$, we can ignore the latent variables
$\beta_\k$ for $\k > \kmax$ in defining our variational approximation.

Notice that only our variational approximation is truncated---the model
(\eqref{bnp_model}) itself is not finite. We set $\kmax$ large enough in our
variational approximation to ensure that a large proportion of the components
are unoccupied with high probability under $\q$, in which case the truncation
approximates the fully nonparametric model with $\kmax = \infty$.
