The posterior of a BNP model is difficult to compute for two reasons: because
the number of components is (countably) infinite, and because the posterior
normalization constant is intractible.  To address these problems, we follow
\citet{blei:2006:vi_for_dp} and form a mean field truncated variational
approximation to the posterior.

Let $\zeta$ denote the full vector of unkown posterior variables. For example,
in the GMM model of \exref{iris_bnp_process}, $\zeta := (\beta, \z, \nu)$.  The
exact posterior distribution $\p(\zeta \vert \x)$ is intractable. Variational
Bayes (VB) is an approach that seeks an approximate posterior through solving a
numerical optimization problem \citep{jordan:1999:vi,
wainwright:2008:graphical_models, blei:2017:vi_review}.

VB specifies a family of approximating distributions $\q(\zeta \vert \eta)$
parameterized by a finite-dimensional vector $\eta \in \etadom \subseteq
\mathbb{R}^{\etadim}$ and solves for $\q(\zeta\vert\etaopt)$ that is closest to
the posterior $\p(\zeta \vert \x)$ according to a divergence measure on
posterior distributions. We will make the common choice of Kullback-Leibler (KL)
divergence:
%
\begin{align}\eqlabel{kl_def}
%
\KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}
={}&    \expect{\q(\zeta \vert \eta)}{
        \log \q(\zeta \vert \eta) - \logp(\x, \zeta)} + \logp(\x).
%
\end{align}
%
As we discuss below, we will choose $\q(\zeta \vert \eta)$ so that we can easily
approximate the above expectation with respect to $\q(\zeta \vert \eta)$ as a
closed-form function of $\eta$.  We will write $\KL{\eta}$ for our
approximation, choosing the optimal variational parameter $\etaopt$ to satisfy
%
\begin{align}\eqlabel{vb_optimization}
%
\etaopt :={} \argmin_{\eta \in \etadom} \KL{\eta} \mathwhere
\KL{\eta} \approx{} \KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)},
%
\end{align}
%
where the tractable objective function $\KL{\eta}$ may not be an exact KL
divergence. Notice that the intractable $\logp(\x)$ term does not depend on
$\eta$, and so can be neglected in the objective function $\KL{\eta}$.

In practice, forming an approximating posterior for BNP can be challenging since
the latent variables $\nu$ and $\beta$ are (countably) infinite dimensional. We
would like to keep dimension of the variational parameter $\eta$ finite in order
for the optimization in \eqref{vb_optimization} to be tractable. In the present
paper, we will follow \citet{blei:2006:vi_for_dp} and use a truncated
stick-breaking representation in the variational distribution. We choose a
truncation parameter $\kmax$ large but finite,
%
\footnote{For $\k > \kmax$, $\q(\nu_\k)$ is effectively a point mass  but
$\p(\nu_\k \vert \x)$ is dominated by the Lebesgue measure.  So the KL
divergence $\KL{\q(\nu_\k) || \p(\nu_\k \vert \x)}$ is not well-defined, even
though $\q(\nu_\k)$ does form a sensible approximation to $\p(\nu_\k \vert \x)$
in measures of posterior divergence such as the Wasserstein distance.  This is
one sense in which our tractable objective function $\KL{\eta}$ is not a proper
KL divergence. }
%
and we set $\q(\nu_\k = 1 | \eta) =
1$ for all $\k > \kmax$. This implies that under $\q$, $\pi_\k = 0$ with
probability one for all $\k > \kmax$ (\eqref{stick_breaking}). Correspondingly,
we also set $\q(\z_{\n\k} = 0 | \eta) = 1$ for $\k > \kmax$.

Notice that only our variational approximation is truncated---the model
(\eqref{bnp_model}) itself is not finite. We set $\kmax$ large enough in our
variational approximation to ensure that a large proportion of the components
are unoccupied with high probability under $\q$, in which case the truncation
approximates the fully nonparametric model with $\kmax = \infty$.

For the generic BNP mixture model in \eqref{bnp_model}, we use a mean-field
variational approximating family of the following form:
%
\begin{align}\eqlabel{vb_mf}
%
\q(\zeta \vert \eta) =
    \left( \prod_{\k=1}^{\kmax - 1} \q(\nuk \vert \eta) \right)
    \left( \prod_{\k=1}^{\kmax} \q(\beta_\k \vert \eta) \right)
    \left( \prod_{\n=1}^{\N} \q(\z_{\n} \vert \eta) \right).
%
\end{align}
%
Because $\pi_\k = 0$ for all $\k > \kmax$, we can ignore the latent variables
$\beta_\k$ for $\k > \kmax$ in defining our variational approximation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For $\z$ and $\beta$ in all models we consider, we have the option of taking
advantage of conditional conjugacy to choose distributions $\q(\z_\n \vert
\eta)$ and $\q(\beta_\k \vert \eta)$. This means that we will take $\q(\z_\n
\vert \eta)$ to be multinomial, matching $\p(\z_{\n} \vert \x, \beta, \nu)$, and
we will take $\q(\beta_\k \vert \eta)$ to match the distribution of
$\p(\beta_{\k} \vert \x, \z, \nu)$. With conditionally conjugate distributions,
all expectations with respect to $\z$ and $\beta$ necessary for computing the
$\mathrm{KL}$ (\eqref{vb_optimization}) will be available analytically.

\begin{ex}[VB approximation for $\beta_\k$ in a GMM]\exlabel{iris_var_distr}
%
The conditionally conjugate variational distribution on $\beta_\k$ is
normal-Wishart, which we denote as
$\q(\beta_\k \vert \eta) = \normalwishart{\beta_k \vert \eta_{\beta_\k}}$,
with $\eta_{\beta_\k}$ the normal-Wishart parameters.

Using the log densities displayed in \exref{iris_bnp_process}
and the mean-field assumption on $\q$,
observe that the variational parameters $\eta_{\beta_\k}$
enter the expected joint log-likelihood only through the
expected moments
%
\begin{align*}
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k},  \quad
\expect{\q(\beta_\k \vert \eta)}{\log|\Lambda_\k|},  \quad
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k\mu_\k}, \quad
\expect{\q(\beta_\k \vert \eta)}{\mu_\k\Lambda_\k\mu_\k}.
\end{align*}
Under a normal-Wishart, all the displayed expectations
can be computed analytically as functions of $\eta_{\beta_\k}$.
%
\end{ex}

If the prior $\pstick(\nuk)$ were Beta-distributed like in the GEM construction,
then the conditionally conjugate distribution for $\q(\nuk \vert \eta)$ would
also be a Beta distribution \citep{blei:2006:vi_for_dp}. Then, in the same way
as \exref{iris_var_distr}, all the necessary expectations with respect to $\nuk$
in the $\mathrm{KL}$ can be computed analytically with respect to the
variational Beta parameters.

However, we will be considering stick-breaking distributions $\pstick(\nuk)$
that are outside the family of Beta distributions. To approximate expectations,
we will use numerical integration. We choose the distribution on the
logit-transformed stick-breaking proportions
%
\begin{align*}
  \lnu_\k := \log(\nu_\k) - \log(1 - \nu_\k)
\end{align*}
%
to be normally distributed. Letting $\s$ be the sigmoid function,
which provides the inverse tranformation,
%
\begin{align*}
    \nuk = \s(\tilde\nuk) := \frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)},
\end{align*}
%
a normal distribution on $\lnu_\k$ induces a \textit{logit-normal} distribution
on $\nuk$. With this choice of a logit-normal distribution, all expectations
with respect to $\nu_\k$ (such as $\expect{\q(\nu_\k\vert\eta)}{\log
\pstick(\nu_\k)}$) can be expressed as a Gaussian integral. We then approximate
the integral with GH quadrature (see \appref{gh_quadrature} for details). Not
only do stick expectations appear in the evaluation of the $\mathrm{KL}$, but
they will also be necessary in computing local sensitivity
(\secref{local_sensitivity}).

Once $\q(\zeta \vert \eta)$ is defined,
we can approximate any expected posterior quantity of interest
(such as \exref{insample_nclusters_simple}) with the
corresponding variational expectation, which will
in turn be a function of the variational parameters $\eta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}\exlabel{vb_insample_nclusters_simple}
%
Given a solution $\etaopt$ to \eqref{vb_optimization}, the intractable posterior
expectation in \exref{insample_nclusters_simple} can be approximated as
a function of $\etaopt$:
%
\begin{align*}
%
\g(\etaopt) :={}
    \expect{\q(\z\vert\etaopt)}{\nclusters_0(\z)} \approx
    \expect{\p(\z\vert\x)}{\nclusters_0(\z)}.
%
\end{align*}

By the mean field assumption, we can write
%
\begin{align*}
%
\expect{\q(\z \vert \etaopt)}{\nclusters_0(\z)} ={}
    \sumk \left(1 -  \prod_{\n=1}^\N
        (1 - \expect{\q(\z \vert \etaoptz)}{\z_{\n\k}})\right).
%
\end{align*}
%
Using the preceding expression, one could express our function of interest
$\g(\eta)$ as a function only of $\eta_\z$, the variational parameters for the
$\z$ indicators.  In our experiments, we write $\g(\eta)$ slightly differently,
taking advantage of the fact that, in all our models, the multinomial posterior
$\p(\z_\n \vert \beta, \nu, \x)$ has a differentiable, easy-to-evaluate closed
form as a function of $\beta$ and $\nu$. Using this fact, we can instead take
%
\begin{align*}
%
\g(\etaopt) :={}&
    \expect{\q(\beta, \nu \vert\etaopt)}{
        \expect{\p(\z \vert \beta, \nu, \x)}{\nclusters_0(\z)}
    } \approx
    \expect{\p(\beta, \nu \vert \x)}{
        \expect{\p(\z \vert \beta, \nu, \x)}{\nclusters_0(\z)}
    }
    = \expect{\p(\z\vert\x)}{\nclusters_0(\z)} \Rightarrow \\
%
\g(\eta) ={}&
    \sumk \left(1 -  \prod_{\n=1}^\N
        \left(1 - \expect{\q(\beta, \nu \vert \eta_\beta, \eta_\nu)}
                    {\expect{\p(\z_{\n} \vert \beta, \nu, \x)}{\z_{\n\k}}}
                    \right)\right).
%
\end{align*}
%
In this way, $\g(\eta)$ depends only on $\eta_\beta$ and $\eta_\nu$, which are
much lower-dimensional than $\eta_\z$, and retains nonlinearities in the map
%
\begin{align*}
%
\eta_\beta, \eta_\nu \mapsto \expect{\q(\beta, \nu \vert \eta_\beta,
\eta_\nu)} {\expect{\p(\z_{\n} \vert \beta, \nu, \x)}{\z_{\n\k}}}.
%
\end{align*}
%
See \secref{computing_sensitivity} for more discussion of this technique.

It will be useful later to observe that, under our chosen parameterizations, the
map $\eta \mapsto \g(\eta)$ is differentiable.
%
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
