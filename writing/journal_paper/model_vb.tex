In each of the BNP models described in the examples above,
the exact posterior distribution $\p(\beta, \nu, \z \vert \x)$
is intractable.
Variational inference is an approach that seeks an approximating distribution
to the exact posterior by solving a numerical optimization problem.
Let $\zeta := (\beta, \z, \nu)$ denote the full vector of
posterior parameters.  We form our variational approximation by specifying a
family of approximating distributions $\q(\zeta \vert \eta)$
parameterized by a finite-dimensional $\eta \in \etadom \subseteq
\mathbb{R}^{\etadim}$.
Given our family of variational approximations, we wish to find the member of
the family that is closest to the posterior $\p(\zeta \vert \x)$ in
Kullback-Leibler (KL) divergence:
%
\begin{align}\eqlabel{vb_optimization}
%
\etaopt :={}&
    \argmin_{\eta \in \etadom}
        \KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)} \mathwhere \\
\KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}
={}&    \expect{\q(\zeta \vert \eta)}{
        \log \q(\zeta \vert \eta) - \logp(\x, \zeta)} + \logp(\x). \nonumber
%
\end{align}
%
As we discuss
below, we will choose $\q(\zeta \vert \eta)$ so that we can easily evaluate (or approximate)
the expectation with respect to $\q(\zeta \vert \eta)$
as a function of $\eta$.
Notice that the intractable term $\logp(\x)$ does not depend on $\eta$, and
so can be negelcted in the optimziation.

In practice, forming an approximating posterior for the models in
\exref{iris_bnp_process, mice_bnp_process, structure_bnp_process}
can be challenging since the latent variables $\nu$ and $\beta$ are
(countably) infinite dimensional.
We would like to keep dimension of the variational parameter $\eta$ finite
in order for the optimizization in \eqref{vb_optimization} to be tractable.
In the present paper, we will follow CITE BLEI and
use a truncated stick-breaking representation in
the variational distribution.
We set a truncation parameter $\kmax$ is large but finite,
and we define $\q(\nu_\k = 1 | \eta) = 1$ for all $\k > \kmax$.
This implies that under the variational distribution, $\pi_\k = 0$ for all $\k > \kmax$
(\eqref{stick_breaking}).
For the model in \eqref{bnp_model},
we propose a mean-field variational approximating family of the following form:
%
\begin{align}\eqlabel{vb_mf}
%
\q(\zeta \vert \eta) =
    \left( \prod_{\k=1}^{\kmax - 1} \q(\nuk \vert \eta) \right)
    \left( \prod_{\k=1}^{\kmax} \q(\beta_\k \vert \eta) \right)
    \left( \prod_{\n=1}^{\N} \q(\z_{\n} \vert \eta) \right).
%
\end{align}
Because $\pi_\k = 0$ for all $\k > \kmax$, we can ignore the
parameters we can ignore the latent variables $\beta_\k$ for $\k > \kmax$,
and deterministically set $\z_{\n\k} = 0$ for $\k > \kmax$ in our variational approximation.

Notice that the model (\eqref{bnp_model}) itself is not finite. The truncation
parameter $\kmax$ only appears in our variational approximation.
We set $\kmax$ set large enough to ensure that a large
proportion of the components are unoccupied with high posterior probability
in which case the truncation approxes the fully nonparametric case with $\kmax = \infty$.

The variational approximation for the models described in \exref{mice_bnp_process, structure_bnp_process}
are similarly mean-field. For the regression model (\exref{mice_bnp_process}),
the approximating posterior
includes additional factors $\q(\b_n\vert\eta)$, one for each individual shift $\b_\n$.
In the topic model (\exref{structure_bnp_process}), the distribution on stick-breaking proportions $\nu$
factorizes over
both individuals $\n$ and components $\k$, while the distribution
cluster memberships $\z$
factorizes over individuals $\n$, loci $\l$, and chromosomes $\i$.

Under the mean-field approximation,
the vector $\eta$ will partition into parameters
governing $\nu$, $\beta$, and $\z$.
Let the VB parameters governing a
particular latent variable be denoted with a subscript: for example,
$\eta_{\beta_\k}$ governs the variational distribution of $\beta_{\k}$,
while $\eta_{\z_\n}$ governs the variational distribution of $\z_\n$, and so on.

For $\z$ and $\beta$ in all models we consider, we will be always be able to
take advantage of conditional conjugacy. Specifically, we will take $\q(\z_{\n}
\vert \eta_{\z_\n})$ to be multinomial, matching $\p(\z_{\n}
\vert \x, \beta, \nu)$, and we will take $\q(\beta_\k \vert \eta_{\beta_\k})$
to match the distribution of $\p(\beta_{\k} \vert \x, \z, \nu)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\beta_\k$ in a GMM]\exlabel{iris_var_distr}
%
In \exref{iris_bnp_process}, $\beta_\k = (\mu_\k, \Lambda_\k)$ and the likelihoods are Gaussian,
$\p(\x_\n \vert \beta_\k) = \normdist{\x_n \vert \mu_\k, \Lambda_\k^{-1}}$,
with prior $\pbetaprior(\beta_\k)$ a normal-Wishart.

The conditionally conjugate variational distribution on $\beta_\k$ is
also normal-Wishart, $\q(\beta_\k \vert \eta) = \normalwishart{\beta_k \vert \eta}$. With this choice of $\q$,
the expectations
\begin{align*}
\expect{\q(\beta_\k \vert \eta)}{\log \p(\x_\n \vert \beta_\k)} \quad
\expect{\q(\beta_\k \vert \eta)}{\log \pbetaprior(\beta_\k)} \quad
\expect{\q(\beta_\k \vert \eta)}{\log \q(\beta_\k \vert \eta)}
\end{align*}
all have closed form expressions as functions of $\eta$.

\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\z_\n$ in a GMM]\exlabel{qz_form}
%
The conditionally conjugate variational distribution for $\z_\n$
is multinomial.
Our variational approximation is truncated at $\kmax$ so
$\z_{\n\k} = 0$ for all $\k > \kmax$;
as a result,
the multinomial distribution on $\z_\n$ has $\kmax$ discrete categories.

For fixed $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$, the multinomial parameters for
$\q(\z_\n \vert \etaoptz)$ that minimize \eqref{vb_optimization} has a closed form.
Let
\begin{align*}
\tilde \rho_{\n\k} = \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
In the exponential family parameterization, the minimizing multinomial parameter is given by
%
\begin{align*}
%
\hat\rho_\n = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}}, \log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots
\log\frac{\tilde\rho_{\n\kmax}}{\tilde\rho_{\n\kmax}}\right)
%
\end{align*}
%
In this exponential family parameterization, the means are given by
\begin{align*}
  \hat p_{\n\k} := \expect{\q(\z_\n \vert \etaz)}{\z_{\n\k}} =
  \frac{\exp(\hat\rho_{\n\k})}{\sum_{\k'=1}^{\kmax}\exp(\hat\rho_{\n\k})}
\end{align*}

We introduce the exponential family parameterization because
we will require $\etaopt$ to be interior to $\etadom$ in our sensitivity analysis
(\secref{local_sensitivity}).
In the mean parameterization,
$\sum_{\k=1}^\kmax \hat p_{\n\k} = 1$, and so the
optimal values for $\hat p_{\n\k}$ cannot be
interior to $(0,1)^\kmax$.
Instead, we define our variational parameters for $\z_\n$
as $\etaopt_{\z_\n} = (\hat\rho_{\n1}, \hat\rho_{\n2}, \ldots, \hat\rho_{\n(\kmax - 1)})$.

%
\end{ex}

For the stick-breaking distributions $\q(\nuk \vert \eta)$ we will need to do
something more complicated, since we wish to accomodate generic stick breaking
distributions.  From \eqref{bnp_model} we see that, up to a constant not
depending on $\nuk$,
%
\begin{align}\eqlabel{stick_log_post}
%
\log \pi_\k ={}&
    \log \nuk + \sum_{\k' < \k} \log (1 - \nuk) \nonumber \\
\logp(\nu_{\k} \vert \x, \beta, \z) ={}&
    \left(\sum_{\n=1}^\N \z_{\n\k'}\right) \log \nuk +
    \left( \sum_{\k' > \k} \sum_{\n=1}^\N \z_{\n\k'} \right) \log (1 - \nuk) +
    \log \pstick(\nuk).
%
\end{align}
%
When $\pstick(\cdot) = \mathrm{Beta}(\cdot | 1, \alpha)$, then, up to a constant
not depending on $\nuk$, $\log \pstick(\nuk) = (\alpha - 1) \log (1 -
\nuk)$, so $\logp(\nu_{\k} \vert \x, \beta, \z)$ is proportional to the
sufficient statistics $\log \nuk$ and $\log(1 - \nuk)$ and so in the
Beta family.  However, for a generic $\pstick(\cdot)$, the posterior
$\p(\nu_{\k} \vert \x, \beta, \z)$ does not have a standard form.

In order to optimize the variational objective \eqref{vb_optimization} we see
from \eqref{stick_log_post} that we need to evaluate or approximate expectations
of the form
%
\begin{align*}
%
\expect{\q(\nuk \vert \eta)}{\log \nuk}
\textrm{,}\quad
\expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
\textrm{,}\quad\textrm{and}\quad
\expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
%
\end{align*}

Each of the expecations are univariate integrals, and so can be efficiently
approximated numerically.  A particularly easy way to do so is with
Gauss-Hermite (GH) quadrature, as we now describe.

First, define a version of $\nuk$ that is not constrained to $(0,1)$:
%
\begin{align}\eqlabel{lnuk_transform}
%
\lnuk :={} \log \left( \frac{\nuk}{1 - \nuk} \right)
\quad\Leftrightarrow\quad
\nuk :={} \frac{\exp(\lnuk)}{1 + \exp(\lnuk)}.
%
\end{align}
%
It will be useful later to have at hand the transform between densities
expressed in the space of $\nu$ and $\lnu$, which is given by
%
\begin{align}\eqlabel{lnuk_derivatives}
%
\fracat{d \lnu_\k}{ d\nuk}{\nuk} ={}
%     \frac{1-\nuk}{\nuk}
%     \left(\frac{1}{1 - \nuk} + \frac{\nuk}{(1 - \nuk)^2} \right)
% \\={}& \frac{1}{\nuk} + \frac{1}{1 - \nuk}
% \\={}&
    \frac{1}{\nuk (1 - \nuk)} \mathand
%
\fracat{d \nuk}{ d\lnuk}{\lnuk} ={}
    \frac{\exp(\lnuk)}{(1 + \exp(\lnuk))^2}.
%
\end{align}
%
We wish to let $\lnu_\k$ be distributed normally under the variational
distribution.  Let $\lnumean_\k$ and $\lnusd_\k$ be entries of the parameter
vector $\eta$, and write
%
\begin{align}\eqlabel{lnuk_vb_approximation}
%
\q(\lnu_\k \vert \eta) ={}& \normdist{\lnu_\k \vert \lnumean_\k, \lnusd_\k}
\Rightarrow \\
\q(\nuk \vert \eta) ={}&
    \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
        \vert \lnumean_\k, \lnusd_\k}
    \left|\fracat{d \lnu_\k}{ d\nuk}{\nuk}\right|
\nonumber\\={}&
\normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
        \vert \lnumean_\k, \lnusd_\k}
    \left|\frac{1}{\nuk (1 - \nuk)}\right|.
\nonumber
%
\end{align}
%
Given this, we can approximate expectations of smooth functions
$f(\nuk)$ using GH quadrature with $\ngh$ knots,
located at $\xi_g$, weighted by $\omega_g$:
%
\begin{align}\eqlabel{gh_integral}
%
\expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
\expect{\q(\lnu_\k \vert \eta)}
       {f\left(\frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}\right)}
\nonumber\\\approx{}&
    \sum_{g=1}^{\ngh} \omega_g f\left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
 \nonumber\\=:{}&
\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
%
\end{align}
%
Conveniently, $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$ is a differentiable
function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.  (This
technique is similar to the ``reparameterization trick,'' only using
GH points rather than standard normal draws.)
