The posterior of a BNP model is difficult to compute for two reasons: because
the number of components is (countably) infinite, and because the posterior
normalization constant is intractable.  To address these problems, we follow
\citet{blei:2006:vi_for_dp} and form a mean field truncated variational
approximation to the posterior. Variational Bayes (VB) is an approach that seeks
an approximate posterior through solving a numerical optimization problem
\citep{jordan:1999:vi, wainwright:2008:graphical_models, blei:2017:vi_review},
minimizing a divergence from a simple, tractable class of candidate posteriors
to the true posterior.

Formally, let $\zeta$ denote the full vector of unknown posterior variables. In
the GMM model of \exref{iris_bnp_process}, $\zeta := (\beta, \z, \nu)$.  The
exact posterior distribution $\p(\zeta \vert \x)$ is intractable. A common
version of VB specifies a family of approximating distributions $\q(\zeta \vert
\eta)$ parameterized by a finite-dimensional vector $\eta \in \etadom \subseteq
\mathbb{R}^{\etadim}$ and solves for $\q(\zeta\vert\etaopt)$ that is closest to
the posterior $\p(\zeta \vert \x)$ according to  the Kullback-Leibler (KL)
divergence:
%
\begin{align}\eqlabel{kl_def}
%
\KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}
={}&    \expect{\q(\zeta \vert \eta)}{
        \log \q(\zeta \vert \eta) - \logp(\x, \zeta)} + \logp(\x).
%
\end{align}
%
As we discuss below, we will choose $\q(\zeta \vert \eta)$ so that we can easily
approximate the above expectation with respect to $\q(\zeta \vert \eta)$ as a
closed-form function of $\eta$.  Nevertheless, except in special cases, one may
not be able to compute \eqref{kl_def} exactly.  For example, in the present
work, we will replace some exact expectations with numerical approximations, and
our choice of approximating family will not admit a well-defined KL divergence.
To avoid ambiguity, we will write $\KL{\eta}$ for the objective that we actually
minimize, which may be only an approximation to the actual KL divergence of
\eqref{kl_def}.  We thus choose the optimal variational parameter $\etaopt$ to
satisfy
%
\begin{align}\eqlabel{vb_optimization}
%
\etaopt :={} \argmin_{\eta \in \etadom} \KL{\eta} \mathwhere
\KL{\eta} \approx{} \KL{\q(\zeta \vert \eta) || \p(\zeta \vert \x)}.
%
\end{align}
%
Notice that the intractable $\logp(\x)$ term does not depend on $\eta$, and so
can be neglected in the objective function $\KL{\eta}$.  In general, when
writing $\KL{\eta}$, we will neglect terms that do not depend on $\eta$.

In a BNP model, the full $\zeta$ vector is infinite-dimensional. In the present
paper, we will follow \citet{blei:2006:vi_for_dp} and use a truncated
stick-breaking representation in the variational distribution, so that the
approximating family $\q(\zeta \vert \eta)$ is finite-dimensional for practical
purposes. We choose a truncation parameter $\kmax$ large but finite, and we set
$\q(\nu_\k = 1 | \eta) = 1$ for all $\k > \kmax$. This implies that under $\q$,
$\pi_\k = 0$ with probability one for all $\k > \kmax$. Correspondingly, we also
set $\q(\z_{\n\k} = 0 | \eta) = 1$ for $\k > \kmax$.~
%
\footnote{For $\k > \kmax$, $\q(\nu_\k)$ is effectively a point mass  but
$\p(\nu_\k \vert \x)$ is dominated by the Lebesgue measure.  So the KL
divergence $\KL{\q(\nu_\k) || \p(\nu_\k \vert \x)}$ is not well-defined, even
though $\q(\nu_\k)$ does form a sensible approximation to $\p(\nu_\k \vert \x)$
in measures of posterior divergence such as the Wasserstein distance.  This is
one sense in which our tractable objective function $\KL{\eta}$ is not a proper
KL divergence.}
%
We emphasize that only our variational approximation is truncated---the model
(\eqref{bnp_model}) itself is not finite. We set $\kmax$ large enough in our
variational approximation to ensure that a large proportion of the components
are unoccupied with high probability under $\q$, in which case the truncation
approximates the fully nonparametric model with $\kmax = \infty$.

For the generic BNP mixture model in \eqref{bnp_model}, we use a mean-field
variational approximating family of the following form:
%
\begin{align}\eqlabel{vb_mf}
%
\q(\zeta \vert \eta) =
    \left( \prod_{\k=1}^{\kmax - 1} \q(\nuk \vert \eta) \right)
    \left( \prod_{\k=1}^{\kmax} \q(\beta_\k \vert \eta) \right)
    \left( \prod_{\n=1}^{\N} \q(\z_{\n} \vert \eta) \right).
%
\end{align}
%
Because $\pi_\k = 0$ for all $\k > \kmax$, we can ignore the latent variables
$\beta_\k$ for $\k > \kmax$ in defining our variational approximation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For $\z$ and $\beta$ in all models we consider, we have the option of taking
advantage of conditional conjugacy to choose distributions $\q(\z_\n \vert
\eta)$ and $\q(\beta_\k \vert \eta)$. This means that we will take $\q(\z_\n
\vert \eta)$ to be multinomial, matching $\p(\z_{\n} \vert \x, \beta, \nu)$, and
we will take $\q(\beta_\k \vert \eta)$ to match the distribution of
$\p(\beta_{\k} \vert \x, \z, \nu)$. With conditionally conjugate distributions,
all expectations with respect to $\z$ and $\beta$ necessary for computing the
$\mathrm{KL}$ (\eqref{vb_optimization}) will be available analytically.

\begin{ex}[VB approximation for $\beta_\k$ in a GMM]\exlabel{iris_var_distr}
%
The conditionally conjugate variational distribution on $\beta_\k$ is
normal-Wishart, which we denote as
$\q(\beta_\k \vert \eta) = \normalwishart{\beta_k \vert \eta_{\beta_\k}}$,
with $\eta_{\beta_\k}$ the normal-Wishart parameters.

Using the log densities displayed in \exref{iris_bnp_process}
and the mean-field assumption on $\q$,
observe that the variational parameters $\eta_{\beta_\k}$
enter the expected joint log-likelihood only through the
expected moments
%
\begin{align*}
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k},  \quad
\expect{\q(\beta_\k \vert \eta)}{\log|\Lambda_\k|},  \quad
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k\mu_\k}, \quad
\expect{\q(\beta_\k \vert \eta)}{\mu_\k\Lambda_\k\mu_\k}.
\end{align*}
Under a normal-Wishart, all the displayed expectations
can be computed analytically as functions of $\eta_{\beta_\k}$.
%
\end{ex}

If the prior $\p(\nuk)$ were Beta-distributed like in the $\gem$
construction, then the conditionally conjugate distribution for $\q(\nuk \vert
\eta)$ would also be a Beta distribution \citep{blei:2006:vi_for_dp}. Then, in
the same way as \exref{iris_var_distr}, all the necessary expectations with
respect to $\nuk$ in the $\mathrm{KL}$ can be computed analytically with respect
to the variational Beta parameters.

However, we will be considering stick-breaking densities $\p(\nuk)$
that are outside the family of Beta densities. To approximate expectations,
we will use numerical integration.  Define the logit-transformed stick-breaking
proportions $\lnuk$ as follows:
%
\begin{align*}
  \lnu_\k := \log(\nu_\k) - \log(1 - \nu_\k)
  \quad \Leftrightarrow \quad
  \nuk = \frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}.
\end{align*}
%
We take $\q(\lnuk \vert \eta)$ to be a normal distribution, which induces a
\textit{logit-normal} distribution on $\nuk$. With this choice of a logit-normal
distribution, all expectations with respect to $\nu_\k$ (such as
$\expect{\q(\nu_\k\vert\eta)}{\log \pstick(\nu_\k)}$) can be expressed as a
Gaussian integral. We then approximate the integral with GH quadrature (see
\appref{gh_quadrature} for details). Not only do stick expectations appear in
the evaluation of the $\mathrm{KL}$, but they will also be necessary in
computing local sensitivity (\secref{local_sensitivity}).

Once $\q(\zeta \vert \eta)$ is defined,
we can approximate any expected posterior quantity of interest
(such as \exref{insample_nclusters_simple}) with the
corresponding variational expectation, which will
in turn be a function of the variational parameters $\eta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}\exlabel{vb_insample_nclusters_simple}
%
Given an optimal VB parameter, $\etaopt$, the intractable posterior expectation
in \exref{insample_nclusters_simple} can be approximated as a function of
$\etaopt$.  For example, by plugging the variational approximation in directly,
we could write
%
\begin{align*}
%
\expect{\p(\z\vert\x)}{\nclusters(\z)} \approx
\expect{\q(\z\vert\etaopt)}{\nclusters(\z)} =
\sumk \left(1 -  \prod_{\n=1}^\N
    (1 - \expect{\q(\z \vert \etaoptz)}{\z_{\n\k}})\right).
%
\end{align*}
%
Each term in the summation is the probability under $\q$ that at least
one observation belongs to cluster $\k$.

Using the preceding expression, one can express our function of interest as a
function only of $\eta_\z$, the variational parameters for the $\z$ indicators,
writing $\g(\eta) := \expect{\q(\z\vert\etaopt)}{\nclusters(\z)}$.

In practice, we in fact take a more effective (but slightly more complicated)
approach to approximating $\expect{\p(\z\vert\x)}{\nclusters(\z)}$, which
takes advantage of the closed form for the conditional posterior $\p(\z_\n \vert
\beta, \nu, \x)$.  See \exref{vb_insample_nclusters_globallocal} of
\secref{computing_sensitivity} for the details.
%
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
