Our goal is to approximate the dependence of the optimal VB parameters on the
prior using a Taylor series, which requires that the optimal VB parameters must
be continuously differentiable as a function of the prior specification. In this
section we state general conditions under which VB optima, as defined by
\eqref{vb_optimization}, are differentiable functions of both parametric and
nonparametric prior perturbations.

We will state our conditions and results in terms of a generic VB approximation
and prior perturbation, which we now articulate in  \defref{prior_t}. The
desired results for the BNP model will follow as special cases of these general
results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}\deflabel{prior_t}
%
For some parameter $\theta \in \thetadom \subseteq \mathbb{R}^{\thetadim}$, let
$\p(\theta \vert \t)$ denote a class of probability densities relative to
a sigma-finite measure $\mu$, defined for $\t$ in an open set $\ball_\t
\subseteq \mathbb{R}$ containing $0$.  Let $\q(\theta \vert \eta)$ be a
family of approximating densities, also defined relative to $\mu$.

Let the variational objective factorize as
%
\begin{align}
%
\KL{\eta, \t} :={}&
    \KL{\eta} -
    \expect{\q(\theta \vert \eta)}
       {\left(\log \p(\theta \vert \t) - \log \p(\theta \vert \t=0)\right)}           \eqlabel{perturbed_objective}\\
\etaopt(\t) :={}& \argmin_{\eta \in \etadom} \KL{\eta, \t}.
    \eqlabel{perturbed_optimum}
%
\end{align}
%
Let $\etaopt$ with no argument refer to $\etaopt(0)$, the minimizer
of $\KL{\eta}$.
%
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The decomposition \eqref{perturbed_objective} is always possible, in the sense
that one could always take $\theta = \zeta$ and $\KL{\eta} = 0$.  We decompose
the objective in this way in order to state strict regularity assumptions only
on the part of the KL divergence that is being perturbed.  Indeed, we will
require little from the $\KL{\eta}$ part of the decomposition other than that it
can be differentiated and optimized.

By identfiying $\t$ with some hyperparameter (e.g. the concentration parameter,
as in \exref{alpha_perturbation} below), we can use \defref{prior_t} to study
parametric perturbations.  Furthermore, by parameterizing a path through the
space of general densities, \defref{prior_t} will allow us to study
nonparametric perturbations (e.g. \exref{gem_mult_perturbation} below and the
detailed analysis of \secrangeref{diffable_nonparametric}{diffable_lp}).  We
can thus study VB prior robustness in general by studying problems of the
form \defref{prior_t}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}\exlabel{alpha_perturbation}
%
For the BNP model with the $\gem$ prior, take $\theta = (\nu_1, \ldots,
\nu_{\kmax-1})$, and take $\mu$ to be the Lebesgue measure on $[0,1]^{\kmax-1}$.
Let $\alpha_0$ be some initial value of the concentration parameter, and
let $\t$ be $\alpha - \alpha_0$, so that deviations of $\t$ away from
$0$ represent deviations of $\alpha$ away from $\alpha_0$.

Expanding the KL divergence in \eqref{kl_def}, we see that the prior
$\p(\nuk \vert \alpha)$ enters the VB objective in a term of the form
$\sum_{\k=1}^\infty \expect{\q(\nuk \vert \eta)}{\log \p(\nuk \vert \alpha)}$.
Adding and subtracting the this term evaluated at $\alpha_0$ gives
%
\begin{align*}
%
\KL{\eta, \alpha} = \KL{\eta, \alpha_0}
-\sum_{\k=1}^{\kmax - 1}
            \left(
                \expect{\q(\nuk \vert \eta)}{\log \p(\nuk \vert \alpha)} -
                \expect{\q(\nuk \vert \eta)}{\log \p(\nuk \vert \alpha_0)}
             \right).
%
\end{align*}
%
Plugging in the definition of $\p(\nuk \vert \alpha)$, recognizing that the
normalizing constant does not depend on $\nuk$ and so can be neglected in the
optimization, letting $\KL{\eta} := \KL{\eta, \alpha_0}$, and substituting $\t =
\alpha - \alpha_0$ gives
%
\begin{align*}
%
\KL{\eta, \t} = \KL{\eta, \alpha_0}
-\t \sum_{\k=1}^{\kmax - 1}
    \expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}.
%
\end{align*}
%
\end{ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ex}\exlabel{gem_mult_perturbation}
%
As in \exref{alpha_perturbation}, take $\theta = (\nu_1, \ldots, \nu_{\kmax-1})$
and $\mu$ to be the Lebesgue measure on $[0,1]^{\kmax-1}$. Let $\pbase(\nuk) :=
\betadist{\nuk \vert 1, \alpha_0}$, and let $\palt(\nuk)$ be a density, not
in the Beta family, that shifts mass towards zero:
%
\begin{align*}
%
\palt(\nuk) :=
    \frac{\exp(-\nuk)\pbase(\nuk)}{\int \exp(-\nuk')\pbase(\nuk') d\nuk'}.
%
\end{align*}
%
For $\t \in [0,1]$ define the multiplicatively perturbed prior
%
\begin{align*}
%
\p(\nuk \vert \t) :=
    \frac{\palt(\nuk)^{\t} \pbase(\nuk)^{1-\t}}
         {\int \palt(\nuk')^{\t} \pbase(\nuk')^{1-\t} d\nuk'}.
%
\end{align*}
%
When $\t = 0$, $\p(\nuk \vert \t) = \pbase(\nuk)$, when $\t = 1$,
$\p(\nuk \vert \t)  = \palt(\nuk)$.  For $\t \in (0,1)$
$\p(\nuk \vert \t)$ varies smoothly between $\pbase$ and $\palt$.

As in \exref{alpha_perturbation}, up to constants not depending on
$\lnuk$ we can write
%
\begin{align*}
%
\log \p(\nuk \vert \t) - \log \p(\nuk \vert \t=0) ={}&
    -\t \log \pbase(\nuk) + \t \log \palt(\nuk) + \const
\\={}& -\t \nuk + \const \Rightarrow
\\
\KL{\eta, \t} ={}& \KL{\eta} -\t \nuk + \const.
%
\end{align*}
%
Different choices for $\palt(\nuk)$ would give different additive
perturbations to the KL divergence.
%
\end{ex}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
