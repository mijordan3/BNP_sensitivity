Two central goals of many clustering problems are inferring how many distinct
clusters are present in a particular dataset and which observations cluster together.
A Bayesian nonparametric (BNP) approach to clustering assumes an infinite
number of \textit{components},
of which a finite number are present in the data as \textit{clusters}.
Being a Bayesian approach, BNP places a generative process
on cluster assignment,
and inference about any quantity of interest,
such as number of distinct clusters present in a data set,
is entirely defined by the posterior distribution.
Like all Bayesian approaches, BNP requires the specification
of a prior, and this prior may favor a greater or fewer number of distinct clusters.
In practice, it is important to establish that the prior is not too informative, particularly
when---as is often the case in BNP---the particular form of the prior is chosen for
mathematical convenience rather than because of a considered subjective belief.

The posterior in a BNP model cannot be calcluated analytically,
and thus approximate methods are required in practice.
We do inference using variational Bayes (VB), which posits a constrained
family of distributions, and uses optimization to find
the member of the family that is closest to the true posterior in
Kullback-Leibler ($\mathrm{KL}$) divergence
\citep{jordan:1999:vi, wainwright:2008:graphical_models, blei:2017:vi_review}.
VB is a popular method on large-scale data sets
because solving the optimization problem can be much faster than
running Markov chain Monte Carlo (MCMC).
VB is particularly apt for data analysis problems that are
exploratory in nature, and where quick, approximate solutions suffice.
For these applications, we thus would like a similarly quick, possibly approximate,
method to assess
the sensitivity of the VB posterior to prior choices.
Assessing sensitivity is especially important
when the results of these exploratory analyses will be guiding further
downstream investigation.

A conceptually straightforward way to assess sensitivity would be to refit the
VB posterior for several different prior choices.
However, repeatedly solving for variational optima after each prior choice
may be unnecessarily expensive, particularly for the settings we consider where
approximate optima might suffice.
In this work, we derive local sensitivity measures which
approximate the nonlinear dependence of the VB optimum on prior parameters
using a first-order Taylor series expansion.
We use the Taylor expansion to quickly extrapolate,
from an initial variational optimum,
the posterior quantities that would be obtained after a model perturbation,
without having to refit the VB posterior.
Notably, we go beyond previous work on local Bayesian sensitivity (e.g. \citet{basu:1996:local})
which treated sensitivity as a measure of robustness \textit{per se};
rather, in the design and evaluation of our local sensitivity
measures we pay special attention to our ability
to accurately extrapolate posterior inferences to different priors.

Using a stick-breaking representation of a Dirichlet process, we consider
perturbations both to the scalar concentration parameter and to the functional
form of the stick-breaking distribution. To evaluate functional perturbations,
we follow \cite{gustafson:1996:local} and embed the stick-breaking density in
the $L_p$ vector spaces of integrable functions. In considering functional
sensitivity however, it is impossible to evaluate senstivity to all possible
perturbations. To uncover potentially influential perturbations, we also show
that the local sensitivity takes the form of an integral of the prior
perturbation times an \textit{influence function}. Using H{\"o}lder's
inequality, we can use the influence function to construct maximally influential
alternative priors.

Importantly, in order to use the influence function to search for
influential prior perturbations, the derivative needs to be a
uniformly good approximation in
a neighborhood of the original prior: that is, the VB optimum must be
\textit{Fre{\'e}chet differentiable} with respect to the prior perturbation.
We prove that, though the VB optimum is directional differentiable for all
$1 \le p \le \infty$,  only for multiplicative perturbations and
$p=\infty$ does the stronger condition of Fre{\'e}chet differentiability hold.
The differentiability properties of the VB posterior is distinct from the
exact Bayesian posterior, whose Fre{\'e}chet differentiability
holds irrespective of the choice of $p$.
With this in mind, we employ function sensitivity with only with multiplicative
perturbations.

The variational approach is particularly suitable for the
sensitivity calculations we derive.
Because the approximate posterior is characterized as a fixed point to an optimization
objective, the derivative of the VB optimum with respect to any prior
parameter can be computed in closed form.
Using modern automatic differentiation tools, computing local sensitivity
in practice requires almost no additional code beyond
implementing the optimization objective itself.
The computation time of forming the local approximation can be
more than an order of magnitude faster than refitting the VB posterior.

\secref{model_bnp} details the stick-breaking construction of Dirichlet process priors.
\secref{model_vb} outlines our truncated variational approximation.
\secref{local_sensitivity} and \secref{functional_perturbations} presents our
local approximation for parametric
and functional perturbations, respectively.
\secref{computing_sensitivity} discusses how computing the sensitivity is done in practice.

In our results (\secref{results}),
we apply our methods to several real-world datasets, estimating the sensitivity
of key posterior quantities to
the BNP prior specification.
We show the accuracy of our local approximation both for
parametric and nonparametric perturbations by comparing
against the much more expensive process of refitting the variational posterior.
We find that posterior quantities can be sensitive or non-sensitive,
depending on both the application and the quantity of interest.
In most cases, the local approximation well-approximates the results found under
refitting while running many times faster.
We also discuss some limitations of local sensitivity
and present scenarios where it fails to be a good approximation to refitting.
\secref{bnp_conclusion} concludes.
