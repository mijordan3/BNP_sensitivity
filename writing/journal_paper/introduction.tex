Two central goals of many clustering problems are inferring how many distinct
clusters are present in a particular dataset and which observations cluster
together. A Bayesian nonparametric (BNP) approach to clustering assumes an
infinite number of \textit{components}, of which a finite number are present in
the data as \textit{clusters}. Being a Bayesian approach, BNP places a
generative process on cluster assignment, and inference about any quantity of
interest, such as number of distinct clusters present in a data set, is entirely
defined by the posterior distribution. Like all Bayesian approaches, BNP
requires the specification of a prior, and this prior may favor a greater or
fewer number of distinct clusters. In practice, it is important to establish
that the prior is not too informative, particularly when---as is often the case
in BNP---the particular form of the prior is chosen for mathematical convenience
rather than because of a considered subjective belief.

Consider, as a simple motivating example, a field biologist fitting a Gaussian
mixture model (GMM) fit to the Fisher iris dataset \citep{anderson:1936:iris,
fisher:1936:iris} using a Dirichelt process prior with concetration parameter
$\alpha$.  One might imagine the biologist asking: ``Suppose I went out and
gathered a new dataset the same size as the one I have.  How many distinct
species of iris would I expect to find?''  The posterior predictive expectation
of the number of distinct species may depend strongly on the choice of $\alpha$
as $\alpha$ varies over some reasonable range, and, if it does, the biologist
would presumably like to be warned that her posterior quantity of interest is
{\em non-robust} to the choice of $\alpha$.

The posterior in a BNP model cannot be calcluated analytically, and thus
approximate methods are required in practice. In the present work, we consider
inference using variational Bayes (VB), which posits a constrained family of
distributions, and uses optimization to find the member of the family that is
closest to the true posterior in Kullback-Leibler ($\mathrm{KL}$) divergence
\citep{jordan:1999:vi, wainwright:2008:graphical_models, blei:2017:vi_review}.
VB is a popular method on large-scale data sets because solving the optimization
problem can be much faster than running Markov chain Monte Carlo (MCMC). VB is
particularly apt for data analysis problems that are exploratory in nature, and
where quick, approximate solutions suffice. For these applications, we thus
would like a similarly quick, possibly approximate, method to assess the
sensitivity of the VB posterior to prior choices. Assessing sensitivity is
especially important when the results of these exploratory analyses will be
guiding further downstream investigation.

Concretely, the output of a VB approximation is a vector of optimal VB
parameters, which we can denote as $\etaopt$.  For example, $\etaopt$ might be a
vector of approximate posterior means and variances for all the quantities in
the posterior.  Of course, the optimal VB parameters depend on the choice of
prior, and we can denote this dependence with $\etaopt(\alpha)$.  Posterior
quantites of interest, such as posterior expectations, can be approximated using
expectations of the variational distributions, which in turn are functions of
the VB parameters $\etaopt(\alpha)$. For example, in our motivating example, can
write $\g(\etaopt(\alpha))$ as the map from the space of variational parameters
to the approximate posterior predictive number of distinct species in a new
dataset.

A conceptually straightforward way to assess sensitivity would be to refit the
VB posterior for several different prior choices.  For example, one might find
$\etaopt(\alpha_g)$ for each $\alpha_g \in \{\alpha_1, \ldots, \alpha_G \}$ in a
grid of $G$ plausible values, and see how much $\g(\alpha_g)$ varies. However,
repeatedly solving for variational optima after each prior choice may be
unnecessarily expensive, particularly for the settings we consider where
approximate optima might suffice.

In this work, we circumvent the need for repeated optimization by approximating
the nonlinear dependence of the VB optimum on prior parameters using a
first-order Taylor series expansion. We use the Taylor expansion to quickly
extrapolate, from an initial variational optimum, the posterior quantities that
would be obtained after a model perturbation, without having to refit the VB
posterior.  In our example, we would choose some ``base value'' $\alpha_0$,
and approximate
%
\begin{align*}
%
% \etaopt(\alpha) \approx
\etalin(\alpha) :=
    \etaopt(\alpha_0) +
    \fracat{d\etaopt(\alpha)}{d\alpha}{\alpha_0} (\alpha - \alpha_0)
\mathand
g(\etaopt(\alpha)) \approx g(\etalin(\alpha)).
%
\end{align*}

If the map $\alpha \mapsto \etaopt(\alpha)$ is continuously differentiable, then
we might expect $g(\etaopt(\alpha)) \approx g(\etalin(\alpha))$ when
$\abs{\alpha - \alpha_0}$ is small.  And if the derivative $d\etaopt(\alpha) /
d\alpha$ can be computed much more efficiently than the cost of optimizing
directly, $\etalin(\alpha)$ can be much faster to compute than
$\etaopt(\alpha)$.  Even when the approximation $\etalin(\alpha)$ is not a
completely adequate substitue for exact re-optimization, the derivative can be a
useful guide for what sorts of prior perturbations might be problematic.

The approximation immediately motivates three key questions:
%
\begin{enumerate}
%
\item \itemlabel{intro_diff}
    When is $\alpha \mapsto \etaopt(\alpha)$ continuously differentiable?
%
\item \itemlabel{intro_comp}
    How can we easily and efficiently compute the derivative?
%
\item \itemlabel{intro_extrap}
    How well does the linear approximation extrapolate in real-world problems?
%
\end{enumerate}
%
For the remainder of the paper, we address these three questions in order, after
introducing our BNP model and VB approximation in \secref{model}.

In \secref{local_sensitivity}, we state sufficient conditions for
differentiability of VB approximations in general, both for parametric
perturbations such as the concentration parameter $\alpha$, as well as for
nonparametric perturbations to the prior density.  Amongst a class of
nonparametric perturbations considered by \citet{gustafson:1996:local}, we prove
in \secref{diffable_nonparametric, diffable_lp} that VB optima are Fr{\'e}chet
differentiable only for multiplicative perturbations to the prior density. The
differentiability of our motivating BNP problems follow as a special case.

In \secref{computing_sensitivity}, we address practical computability of the
derivative, and show that the approximation can be easily and efficiently
computed even in high-dimensional problems like BNP models, using moder
automatic differentiation tools \citep{baydin:2018:automatic, jax2018github} and
the iterative linear algebra techniques such as the conjugate gradient algorithm
\citep{nocedal:2006:numerical}.  In our experiments, we find that the derivative
can be computed roughly an order of magnitude faster than re-optimizing.

Finally, in \secref{results}, we demonstrate the usefulness of the approximation
in practice on a series of increasingly complex real-world datasets.  We
investigate the accuracy of our local approximation both for parametric and
nonparametric perturbations by comparing against the much more expensive process
of refitting the variational posterior.  We demonstrate how different posterior
quantities can be sensitive or non-sensitive, depending on both the application
and the quantity of interest.  In most cases, the local approximation provides
qualitatively accurate results many times faster re-optimizing.  We also discuss
some limitations of local sensitivity and present scenarios where it fails to be
a good approximation to refitting. \Secref{conclusion} concludes.

We close the introduction by observing that  the present work is essentially a
VB extension of the local Bayesian robustness literature
\citet{gustafson:1996:local, basu:1996:local}.  In a sense, VB is more naturally
amenable to sensitivity analysis than Markov Chain Monte Carlo, since the
derivative of VB approximations is typically available in closed form, whereas
the derivative of Bayesian posteriors must be approximated using potentially
noisy posterior covariances and/ or posterior conditional densities that are not
readily available from MCMC draws \citep{gustafson:1996:marginal}.  In addition
to providing rigorous theory and computational tools for the VB context, we go
beyond previous work on local Bayesian sensitivity which treat sensitivity as a
measure of robustness \textit{per se}.  Rather, in the design and evaluation of
our local sensitivity measures we pay special attention to our ability to
accurately and usefully extrapolate VB posterior inferences to different priors.


% Using a stick-breaking representation of a Dirichlet process, we consider
% perturbations both to the scalar concentration parameter and to the functional
% form of the stick-breaking distribution. To evaluate functional perturbations,
% we follow \cite{gustafson:1996:local} and embed the stick-breaking density in
% the $L_p$ vector spaces of integrable functions. In considering functional
% sensitivity however, it is impossible to evaluate senstivity to all possible
% perturbations. To uncover potentially influential perturbations, we also show
% that the local sensitivity takes the form of an integral of the prior
% perturbation times an \textit{influence function}. Using H{\"o}lder's
% inequality, we can use the influence function to construct maximally influential
% alternative priors.

% Importantly, in order to use the influence function to search for
% influential prior perturbations, the derivative needs to be a
% uniformly good approximation in
% a neighborhood of the original prior: that is, the VB optimum must be
% \textit{Fre{\'e}chet differentiable} with respect to the prior perturbation.
% We prove that, though the VB optimum is directional differentiable for all
% $1 \le p \le \infty$,  only for multiplicative perturbations and
% $p=\infty$ does the stronger condition of Fre{\'e}chet differentiability hold.
% The differentiability properties of the VB posterior is distinct from the
% exact Bayesian posterior, whose Fre{\'e}chet differentiability
% holds irrespective of the choice of $p$.
% With this in mind, we employ function sensitivity with only with multiplicative
% perturbations.

% The variational approach is particularly suitable for the sensitivity
% calculations we derive. Because the approximate posterior is characterized as a
% fixed point to an optimization objective, the derivative of the VB optimum with
% respect to any prior parameter can be computed in closed form. Using modern
% automatic differentiation tools, computing local sensitivity in practice
% requires almost no additional code beyond implementing the optimization
% objective itself. The computation time of forming the local approximation can be
% more than an order of magnitude faster than refitting the VB posterior.

% \Secref{model_bnp} details the stick-breaking construction of Dirichlet process
% priors. \Secref{model_vb} outlines our truncated variational approximation.
% \Secref{local_sensitivity} presents our local approximation for parametric and
% functional perturbations, respectively. \secref{computing_sensitivity} discusses
% how computing the sensitivity is done in practice.
