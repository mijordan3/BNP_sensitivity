%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do not edit the TeX file your work
% will be overwritten.  Edit the RnW
% file instead.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



In this section, we apply our methods to Fisher's iris data set, using the
Gaussian mixture model (GMM) model from \exref{iris_bnp_process} and VB
approximation given in \eqref{vb_mf} and \exref{iris_var_distr}.  Here, the data
dimension $d = 4$, we set the truncation parameter $\kmax = 15$.  For our
quantities of interest, we investigate both the estimated number of in-sample
clusters as described in \exref{vb_insample_nclusters_simple} and the
\textit{posterior predictive} number of clusters. Let $\nclusters(\z)$ be the
number of clusters as defined as in \exref{insample_nclusters_simple}.  The
posterior predictive number of clusters, $\expect{\p(\pi \vert
\x)}{\expect{\p(\z \vert \pi)}{\nclusters(\z)}}$, is the posterior expectation
of the number of distinct clusters that we would expect to see in a new dataset
the same size as our original dataset.  Analogously to
\exref{insample_nclusters_simple}, we define a variational approximation to
posterior predictive number of clusters as
%
\begin{align*}
\gclusterspredabbr(\eta) :=
    \expect{\q(\nu\vert\eta)}{\expect{\p(\z\vert\nu)}{\nclusters(\z)}}
  = \sum_{k=1}^\kmax\left(1 -
  \expect{\q(\nu \vert \eta)}{(1 - \pi_\k)^\N} \right).
\end{align*}
%
In the iris example, the predictive quantity can interpreted as the expected
number of species one might see if a fresh sample of iris flowers were
collected.

\subsubsection*{Parametric sensitivity}

We first explore sensitivity of the in-sample and posterior predictive number of
clusters to the concentration parameter, $\alpha$, using the results in
\secref{diffable_concentration}.  To choose a plausible range of $\alpha$,
note that, under the $\gem$ prior, the {\em a priori} expected
number of distinct clusters in a dataset of size $N$ is given by
%
\begin{align}\eqlabel{prior_num_clusters}
\expect{\p(\z \vert \pi)\p(\pi \vert \alpha)}{\nclusters(\z)} =
\sum_{\n = 1}^\N \frac{\alpha}{\alpha + \n - 1}.
\end{align}
%
(See \citep[Equation 75]{jordan:2015:gentleintrodp}.)
We take $\alpha\in[0.1, 4.0]$, which
according to \eqref{prior_num_clusters}, corresponds to an an {\em a priori}
range of approximately $1.5$ to $15$ clusters.  Over this range, the shape of
the stick-breaking density varies considerably, as shown in
\figref{beta_priors}.  We take $\alpha_0 = 2$, near the middle of the range, and
use the prior $\p(\nuk \vert \alpha_0)$ as the initial prior at which we form the
linear approximation for all the results below. \Figref{iris_fit} shows the
posterior clustering for $\alpha_0$.  Using our initial prior, the posterior
recovers the known ground truth that there are in truth three distinct species.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!h]

{\centering \includegraphics[width=0.980\linewidth,height=0.392\linewidth]{figure/beta_priors-1} 

}

\caption[Probability density functions of $\text{Beta}(1, \alpha)$ distributions, under various $\alpha$ considered for the iris data set]{Probability density functions of $\text{Beta}(1, \alpha)$ distributions, under various $\alpha$ considered for the iris data set.}\label{fig:beta_priors}
\end{figure}


\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!h]

{\centering \includegraphics[width=0.588\linewidth,height=0.470\linewidth]{figure/iris_fit-1} 

}

\caption[The iris data in principal component space and
                      GMM fit at $\alpha = 2$]{The iris data in principal component space and
                      GMM fit at $\alpha = 2$. Colors denote inferred memberships and
                      ellipses represent estimated covariances. }\label{fig:iris_fit}
\end{figure}


\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!h]

{\centering \includegraphics[width=0.784\linewidth,height=0.439\linewidth]{figure/iris_alpha_sens-1} 

}

\caption[The expected number of clusters as $\alpha$ varies in thethe GMM fit of the iris data]{The expected number of clusters as $\alpha$ varies in thethe GMM fit of the iris data. On the left is in-sample quantity $\gclustersabbr$. On the right is the the predictive quantity $\gclusterspredabbr$. We formed the linear approximation at $\alpha=2$.}\label{fig:iris_alpha_sens}
\end{figure}


\end{knitrout}

\Figref{iris_alpha_sens} shows both the posterior in-sample and predictive
number of distinct clusters as $\alpha$ varies. Over this range of $\alpha$,
the in-sample number of clusters is quite robust, remaining nearly constant at three, but the posterior predictive
number of clusters is non-robust, ranging roughly from 3.0 to 5.6 expected
species.  Our approximation captures this qualitative behavior. As expected,
the approximation is least accurate furthest from the $\alpha_0$ at which it is
evaluated.

\subsubsection*{Functional perturbations and the influence function}

While the number of in-sample clusters $\gclustersabbr$ was insensitive over our
range of $\alpha$, insensitivity to $\alpha$ does not rule out sensitivity to
other prior perturbations. In this subsection, we first demonstrate the ability
of the influence function to predict the effect of nonparametric changes to the
prior. Then, we use the influence function to construct a worst-case
perturbation for $\gclustersabbr$.

As in the previous subsection, we take $\pbase(\nuk) = \betadist{\nuk \vert 1,
\alpha_0}$ for each $\k$.  The leftmost column of \figref{iris_fsens} shows in
purple the influence function, $\infl(\nu)$, as given by
\corref{etafun_deriv_form}.  For ease of visualization, we show the influence
function in logit stick space, taking $\mu$ to be the Lebesgue measure on
$\mathbb{R}^{\kmax - 1}$, and $\theta = (\lnu_1, \ldots, \lnu_{\kmax - 1})$,
suitably transforming the prior density.  We additionally perturb all the sticks
in the same way, reducing the domain of the influence function to $\mathbb{R}$
as in \exref{infl_univariate}.

We consider perturbations $\phi$ which are Gaussian bumps, with each
perturbation centered at a different location on the real line.  Each row of
\figref{iris_fsens} corresponds to a different $\phi$, which are shown in gray
in the left-hand column of \figref{iris_fsens}. The middle column of
\figref{iris_fsens} shows the stick-breaking prior $\p(\nuk \vert \phi)$ induced
by the corresponding $\phi$, and the rightmost column of \figref{iris_fsens}
shows the changes produced by the corresponding perturbation, as measured by
re-fitting and as predicted by our approximation.

According to \corref{etafun_deriv_form}, the sign and magnitude of the effect of
a perturbation should be determined by its integral against the influence
function.  Thus, when $\phi$ lines up with a negative part of $\infl$, as in the
first row, we expect the change to be negative.  Similarly, we expect the
perturbation of the bottom row to produce a positive change, and the middle row,
in which $\phi$ overlaps with both negative and positive parts of the influence
function, to produce a relatively small change.  The third column of
\corref{etafun_deriv_form} confirms that this intuition holds, both for
the refits and linear approximation.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!h]

{\centering \includegraphics[width=0.980\linewidth,height=0.862\linewidth]{figure/iris_fsens-1} 

}

\caption{Sensitivity of
        the expected number of in-sample clusters
        in the iris data set
        to three multiplicative perturbations each with $\norminf{\phi} = 1$.
        (Left) The multiplicative perturbation $\phi$ in grey.
        The influence function $\Psi$ in purple,
        scaled to also have $\norminf{\Psi}=1$.
        (Middle) The initial and alternative priors $\pbase(\nuk)$
        and $\palt(\nuk)$.
        (Right) The effect of the perturbation
        on the change in expected number of in-sample clusters
        for $t \in[0, 1]$.}\label{fig:iris_fsens}
\end{figure}


\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[!h]

{\centering \includegraphics[width=0.980\linewidth,height=0.412\linewidth]{figure/iris_worstcase-1} 

}

\caption[Sensitivity of
        the expected number of in-sample clusters in the iris data set
        to the worst-case multiplicative perturbation with
        $\norminf{\phi} = 1$]{Sensitivity of
        the expected number of in-sample clusters in the iris data set
        to the worst-case multiplicative perturbation with
        $\norminf{\phi} = 1$.}\label{fig:iris_worstcase}
\end{figure}


\end{knitrout}

Finally, \figref{iris_worstcase} shows the worst-case multiplicative perturbation with $\norminf{\phi} = 1$, as given by \corref{etafun_worst_case},
along with its effect on the prior and $\gclustersabbr$.
As expected, this worst-case perturbation
has a much larger effect on $\gclustersabbr$
compared to the other unit-norm perturbations in \figref{iris_fsens}.
However, even with the worst-case perturbation---which results in a rather unreasonably shaped prior density---the change in $\gclustersabbr$
is still small. We conclude that on the iris data set, $\gclustersabbr$ appears
to be a robust quantity for this model and dataset.
