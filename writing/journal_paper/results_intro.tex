We evaluate the prior sensitivity in BNP models applied to three distinct data analysis examples.
We first demonstrate local sensitivity in a Gaussian mixture model of the canonical iris data set.
Secondly, we use BNP to cluster time-course gene expression data
and evaluate the sensitivity of the resulting co-clustering matrix to prior perturbations.
Finally, we adapt STRUCTURE~\citep{pritchard:2000:structure}, a Bayesian population genetics model,
to incorporate a BNP prior, thus allowing for
an infinite number latent populations.
We study how the estimated population structure may change as a
result of the prior specification.

We consider both the effects of varying the parameter $\alpha$
in the $\betadist{\nuk \vert 1, \alpha}$ stick distribution, as well as
the effects of changing the functional form the Beta prior itself.
For a given prior perturbation,
we validate the performace of the linear approximation against
the more expensive process of re-fitting the model.

In each data example,
the initial variational parameters were fit to a model
with Beta-distributed sticks at some chosen parameter $\alpha = \alpha_0$.
We optimized the initial variational parameters with
the BFGS algorithm run with a loose convergence tolerance
followed by trust-region Newton conjugate gradient (trust-ncg) to find a high-quality optimum.
Unless otherwise noted, all subsequent refits after a prior pertrubation were found
using trust-ncg with the variational parameters at $\alpha = \alpha_0$ as an initialization.

For our linear approximation,
we take advantage of the global/local structure.
As described in \secref{computing_sensitivity}, we linearize only the
global variational parameters (\eqref{global_lin_approx}) and
approximate all variational parameters (both global and local)
with $\etalinglobal$ (\eqref{global_lin_approx_all}).

We solved the linear system in~\eqref{global_sens} using
the conjugate gradient algorithm, which requires only Hessian-vector products;
this avoids instatiating the full Hessian matrix in memory.
If memory were not limited,
we could have computed the full Hessian and
either factorized it (e.g. with a Cholesky decomposition) or inverted it directly.
Then, the Hessian inverse (or its Cholesky decomposition) can
be re-used for different perturbations
(since different choices of prior perturbation
results in a different $\crosshessian$).
By using conjugate gradient, we have to re-solve the linear system (\eqref{global_sens})
for each choice of prior perturbation.
On our data sets, the conjugate graient algorithm was at least
an order of magnitude faster than refitting, and did not pose a meaningful bottleneck
to exploring different perturbations.

The conjugate gradient algorithm along with the optimizers BFGS and trust-ncg
are implemented in SciPy~\citep{scipy}.
All derivatives were computed using the Python
automatic differentiation libarary Jax~\citep{jax2018github}.

% To describe our results in this section, we use the following notation:
% \begin{align*}
% \grefit(t) &:= \g(\etaopt(t)) \\
% \gapproxglobal(t) &:= \g(\etalinglobal(t))
% \end{align*}
% to describe a posterior quantity under the refitted variational parameters and
% linearly approximated variational parameters, respectively.

% We demonstrate that the linear approximation provides a descriptive tool that well-captures both the
% direction and size of changes in posterior quantities after a prior perturbation.
% In addition, the influence function provides a useful guide in revealing which
% functional pertrubations will result in large changes in the posterior statistic of interest.
% Importantly, the speed in forming a linear approximation is an order
% of magnitude faster than refitting the model on the data examples we consider.
