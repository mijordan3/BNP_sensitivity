
% \subsection*{subsection name}
%
% we choose $\q(\nuk \vert \eta)$ to be logit-normally distributed,
% and express the expectations in \eqref{stick_expectations} as Gaussian integrals.
% Define
% \begin{align*}
%   \tilde \nuk := \log\left(\frac{\nuk}{1 - \nuk}\right),
% \end{align*}
% which will be normally distributed under our choice of a
% logit-normal $\q(\nuk \vert \eta)$.
%
% Let $\lnumean_\k$ and $\lnusd_\k$ be entries of $\eta$ corresponding to
% the logit-normal parameters of $\nuk$.
%
%
% In order to optimize the variational objective \eqref{vb_optimization} we see
% from \eqref{stick_log_post} that we need to evaluate or approximate expectations
% of the form
% %
% \begin{align*}
% %
% \expect{\q(\nuk \vert \eta)}{\log \nuk}
% \textrm{,}\quad
% \expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
% \textrm{,}\quad\textrm{and}\quad
% \expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
% %
% \end{align*}
%
%
%
%
% First, define a version of $\nuk$ that is not constrained to $(0,1)$:
% %
% \begin{align}\eqlabel{lnuk_transform}
% %
% \lnuk :={} \log \left( \frac{\nuk}{1 - \nuk} \right)
% \quad\Leftrightarrow\quad
% \nuk :={} \frac{\exp(\lnuk)}{1 + \exp(\lnuk)}.
% %
% \end{align}
% %
It will be useful later to have at hand the transform between densities
expressed in the space of $\nu$ and $\lnu$, which is given by
%
\begin{align}\eqlabel{lnuk_derivatives}
%
\fracat{d \lnu_\k}{ d\nuk}{\nuk} ={}
%     \frac{1-\nuk}{\nuk}
%     \left(\frac{1}{1 - \nuk} + \frac{\nuk}{(1 - \nuk)^2} \right)
% \\={}& \frac{1}{\nuk} + \frac{1}{1 - \nuk}
% \\={}&
    \frac{1}{\nuk (1 - \nuk)} \mathand
%
\fracat{d \nuk}{ d\lnuk}{\lnuk} ={}
    \frac{\exp(\lnuk)}{(1 + \exp(\lnuk))^2}.
%
\end{align}
% %
% We wish to let $\lnu_\k$ be distributed normally under the variational
% distribution.  Let $\lnumean_\k$ and $\lnusd_\k$ be entries of the parameter
% vector $\eta$, and write
% %
% \begin{align}\eqlabel{lnuk_vb_approximation}
% %
% \q(\lnu_\k \vert \eta) ={}& \normdist{\lnu_\k \vert \lnumean_\k, \lnusd_\k}
% \Rightarrow \\
% \q(\nuk \vert \eta) ={}&
%     \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\fracat{d \lnu_\k}{ d\nuk}{\nuk}\right|
% \nonumber\\={}&
% \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\frac{1}{\nuk (1 - \nuk)}\right|.
% \nonumber
% %
% \end{align}
% %
% Given this, we can approximate expectations of smooth functions
% $f(\nuk)$ using GH quadrature with $\ngh$ knots,
% located at $\xi_g$, weighted by $\omega_g$:
% %
% \begin{align}\eqlabel{gh_integral}
% %
% \expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
% \expect{\q(\lnu_\k \vert \eta)}
%        {f\left(\frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}\right)}
% \nonumber\\\approx{}&
%     \sum_{g=1}^{\ngh} \omega_g f\left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
%  \nonumber\\=:{}&
% \expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
% %
% \end{align}
% %
% Conveniently, $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$ is a differentiable
% function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.  (This
% technique is similar to the ``reparameterization trick,'' only using
% GH points rather than standard normal draws.)

\hrulefill

In the regression example (\exref{mice_bnp_process}), $\zeta$ includes
the additive shifts, $\zeta := (\beta, \z, \nu, \b)$.

The variational approximation for the topic model
(\exref{structure_bnp_process}) is similarly mean-field: the distribution on
stick-breaking proportions $\nu$ factorizes over both individuals $\n$ and
components $\k$, while the assignments $\z$ factorize over individuals $\n$,
loci $\l$, and chromosomes $\i$. For the regression model
(\exref{mice_bnp_process}), all terms in the variational approximation
fully-factorize except for the cluster assignments $\z$ and additive shifts
$\b$. While we assume $(\z, \b)$ to be independent from all other latent
variables under $\q$, we will allow conditional dependence between $\z$ and $\b$
(\appref{app_mice}).




\hrulefill


% This is necessary here becuase it is a condition under which we have
% differentiability.  Alternatively, we could define it later when
% stating our differentiability theorem...
The KL divergence of \eqref{kl_def} contains a term of the form $\expect{\q(\nuk
\vert \eta)}{\log \pstick(\nuk)}$.  Since we will be considering generic
densities $\pstick(\nuk)$, we will need to compute this integral numerically.
To facilitate numerical integration, we model the sticks using a logit-normal
distribution as follows.  Define
%
\begin{align*}
%
\lnuk := \log\left(\frac{\nuk}{1 - \nuk}\right),
%
\end{align*}
%
and choose $\q(\lnu_\k \vert \eta)$ to be normally distributed.  This then
induces a logit-normal distribution on our original variable of interest,
$\nuk$.  See \secref{stick_expectations} below for more details.




\hrulefill




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We conclude this section with a brief remark about computing the expectation
$\crosshessian$ in our BNP sensitivity analysis.
We are interested in sensitivity to the stick-breaking distribution,
so only the prior terms on stick-breaking proportions
$\nu = (\nu_1, ..., \nu_{\kmax - 1})$ depends on $t$.
Because the elements of $\nu$ fully factorize
under both the prior and the variational distributions,
$\crosshessian$ decomposes as
\begin{align}
  \crosshessian &=
  \sum_{\k=1}^{\kmax - 1}
          \expect{\q(\nuk \vert \eta)}
                 {
                 \lqgrad{\nuk \vert \etaopt}
                 \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                 } \notag\\
  &= \sum_{\k=1}^{\kmax - 1}
         \evalat{\nabla_\eta \expect{\q(\nuk \vert \eta)}
                {
                \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
                }}{\eta = \etaopt(0)},
\eqlabel{sens_mixed_partial}
\end{align}
where we assumed that $\q(\theta \vert \eta)$ is normalized, so
$\lqgradbar{\theta \vert \etaopt} = \lqgrad{\theta \vert \etaopt}$,
and that the assumptions of \thmref{etat_deriv} hold, so we
can freely exchange derivatives with expectations.

We approximate the expectation using GH quadrature (\eqref{gh_integral}), with
$f(\nu_k) = \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}$. In all
the functional forms for $\t \mapsto \pstick(\nuk \vert \t)$ considered below,
$f(\nu_k)$ can be provided in either closed-form or computed with automatic
differentiation. The resulting GH approximation is a deterministic function of
$\eta$, and thus the gradient in \eqref{sens_mixed_partial} can be computed with
another application of automatic differentiation. Note that $\crosshessian$ is
sparse in \eqref{sens_mixed_partial}: it is zero for all entries of $\eta$ other
than those that parameterize the sticks.


\hrulefill

\subsection{Function-valued prior perturbations}
\seclabel{functional_perturbations}
\input{function_perturbations.tex}

\section{Valid priors and $\lp{\mu,p}$ balls}\seclabel{valid_priors}
\input{valid_priors.tex}

\section{Differentiability}\seclabel{differentiability}
\input{differentiability.tex}
