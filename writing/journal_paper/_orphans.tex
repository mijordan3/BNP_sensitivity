
% \subsection*{subsection name}
%
% we choose $\q(\nuk \vert \eta)$ to be logit-normally distributed,
% and express the expectations in \eqref{stick_expectations} as Gaussian integrals.
% Define
% \begin{align*}
%   \tilde \nuk := \log\left(\frac{\nuk}{1 - \nuk}\right),
% \end{align*}
% which will be normally distributed under our choice of a
% logit-normal $\q(\nuk \vert \eta)$.
%
% Let $\lnumean_\k$ and $\lnusd_\k$ be entries of $\eta$ corresponding to
% the logit-normal parameters of $\nuk$.
%
%
% In order to optimize the variational objective \eqref{vb_optimization} we see
% from \eqref{stick_log_post} that we need to evaluate or approximate expectations
% of the form
% %
% \begin{align*}
% %
% \expect{\q(\nuk \vert \eta)}{\log \nuk}
% \textrm{,}\quad
% \expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
% \textrm{,}\quad\textrm{and}\quad
% \expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
% %
% \end{align*}
%
%
%
%
% First, define a version of $\nuk$ that is not constrained to $(0,1)$:
% %
% \begin{align}\eqlabel{lnuk_transform}
% %
% \lnuk :={} \log \left( \frac{\nuk}{1 - \nuk} \right)
% \quad\Leftrightarrow\quad
% \nuk :={} \frac{\exp(\lnuk)}{1 + \exp(\lnuk)}.
% %
% \end{align}
% %
It will be useful later to have at hand the transform between densities
expressed in the space of $\nu$ and $\lnu$, which is given by
%
\begin{align}\eqlabel{lnuk_derivatives}
%
\fracat{d \lnu_\k}{ d\nuk}{\nuk} ={}
%     \frac{1-\nuk}{\nuk}
%     \left(\frac{1}{1 - \nuk} + \frac{\nuk}{(1 - \nuk)^2} \right)
% \\={}& \frac{1}{\nuk} + \frac{1}{1 - \nuk}
% \\={}&
    \frac{1}{\nuk (1 - \nuk)} \mathand
%
\fracat{d \nuk}{ d\lnuk}{\lnuk} ={}
    \frac{\exp(\lnuk)}{(1 + \exp(\lnuk))^2}.
%
\end{align}
% %
% We wish to let $\lnu_\k$ be distributed normally under the variational
% distribution.  Let $\lnumean_\k$ and $\lnusd_\k$ be entries of the parameter
% vector $\eta$, and write
% %
% \begin{align}\eqlabel{lnuk_vb_approximation}
% %
% \q(\lnu_\k \vert \eta) ={}& \normdist{\lnu_\k \vert \lnumean_\k, \lnusd_\k}
% \Rightarrow \\
% \q(\nuk \vert \eta) ={}&
%     \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\fracat{d \lnu_\k}{ d\nuk}{\nuk}\right|
% \nonumber\\={}&
% \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\frac{1}{\nuk (1 - \nuk)}\right|.
% \nonumber
% %
% \end{align}
% %
% Given this, we can approximate expectations of smooth functions
% $f(\nuk)$ using GH quadrature with $\ngh$ knots,
% located at $\xi_g$, weighted by $\omega_g$:
% %
% \begin{align}\eqlabel{gh_integral}
% %
% \expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
% \expect{\q(\lnu_\k \vert \eta)}
%        {f\left(\frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}\right)}
% \nonumber\\\approx{}&
%     \sum_{g=1}^{\ngh} \omega_g f\left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
%  \nonumber\\=:{}&
% \expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
% %
% \end{align}
% %
% Conveniently, $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$ is a differentiable
% function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.  (This
% technique is similar to the ``reparameterization trick,'' only using
% GH points rather than standard normal draws.)

\hrulefill

In the regression example (MICE BNP PROCESS), $\zeta$ includes
the additive shifts, $\zeta := (\beta, \z, \nu, \b)$.

The variational approximation for the topic model
(STRUCTURE BNP PROCESS) is similarly mean-field: the distribution on
stick-breaking proportions $\nu$ factorizes over both individuals $\n$ and
components $\k$, while the assignments $\z$ factorize over individuals $\n$,
loci $\l$, and chromosomes $\i$. For the regression model
(MICE BNP PROCESS), all terms in the variational approximation
fully-factorize except for the cluster assignments $\z$ and additive shifts
$\b$. While we assume $(\z, \b)$ to be independent from all other latent
variables under $\q$, we will allow conditional dependence between $\z$ and $\b$
(\appref{app_mice}).




\hrulefill


% This is necessary here becuase it is a condition under which we have
% differentiability.  Alternatively, we could define it later when
% stating our differentiability theorem...
The KL divergence of \eqref{kl_def} contains a term of the form $\expect{\q(\nuk
\vert \eta)}{\log \pstick(\nuk)}$.  Since we will be considering generic
densities $\pstick(\nuk)$, we will need to compute this integral numerically.
To facilitate numerical integration, we model the sticks using a logit-normal
distribution as follows.  Define
%
\begin{align*}
%
\lnuk := \log\left(\frac{\nuk}{1 - \nuk}\right),
%
\end{align*}
%
and choose $\q(\lnu_\k \vert \eta)$ to be normally distributed.  This then
induces a logit-normal distribution on our original variable of interest,
$\nuk$.  See STICK EXPECTATIONS below for more details.




\hrulefill




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% We conclude this section with a brief remark about computing the expectation
% $\crosshessian$ in our BNP sensitivity analysis.
% We are interested in sensitivity to the stick-breaking distribution,
% so only the prior terms on stick-breaking proportions
% $\nu = (\nu_1, ..., \nu_{\kmax - 1})$ depends on $t$.
% Because the elements of $\nu$ fully factorize
% under both the prior and the variational distributions,
% $\crosshessian$ decomposes as
% \begin{align}
%   \crosshessian &=
%   \sum_{\k=1}^{\kmax - 1}
%           \expect{\q(\nuk \vert \eta)}
%                  {
%                  \lqgrad{\nuk \vert \etaopt}
%                  \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
%                  } \notag\\
%   &= \sum_{\k=1}^{\kmax - 1}
%          \evalat{\nabla_\eta \expect{\q(\nuk \vert \eta)}
%                 {
%                 \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}
%                 }}{\eta = \etaopt(0)},
% \eqlabel{sens_mixed_partial}
% \end{align}
% where we assumed that $\q(\theta \vert \eta)$ is normalized, so
% $\lqgradbar{\theta \vert \etaopt} = \lqgrad{\theta \vert \etaopt}$,
% and that the assumptions of \thmref{etat_deriv} hold, so we
% can freely exchange derivatives with expectations.
%
% We approximate the expectation using GH quadrature (\eqref{gh_integral}), with
% $f(\nu_k) = \fracat{\log \pstick(\nuk \vert \t)}{\partial \t}{\t = 0}$. In all
% the functional forms for $\t \mapsto \pstick(\nuk \vert \t)$ considered below,
% $f(\nu_k)$ can be provided in either closed-form or computed with automatic
% differentiation. The resulting GH approximation is a deterministic function of
% $\eta$, and thus the gradient in \eqref{sens_mixed_partial} can be computed with
% another application of automatic differentiation. Note that $\crosshessian$ is
% sparse in \eqref{sens_mixed_partial}: it is zero for all entries of $\eta$ other
% than those that parameterize the sticks.
%

\hrulefill

% \section{Differentiability}\seclabel{differentiability}
% \input{differentiability.tex}



In \appref{cont_lemmas}, we state easy-to-verify (but technical) sufficient
conditions that allow us to establish \assuref{exchange_order}.  The key to
establishing \assuref{exchange_order} is the dominated convergence theorem
\citep[Theorem 16.8]{billingsley:1986:probability}, which states roughly that,
for some scalar-valued funciton $f(\theta, \tau)$,
%
\begin{align*}
%
\left. \frac{d}{d\tau} \int f(\theta, \tau) \mu(d\theta) \right|_{\tau=0} =
     \int \fracat{df(\theta, \tau)}{d\tau}{\tau=0}  \mu(d\theta)
%
\end{align*}
%
if there exists a dominating function $M(\theta) > 0$ such that
$\int M(\theta) \mu(d\theta) < \infty$ with $\abs{f(\theta, \tau)} < M(\theta)$
and $\abs{df(\theta, \tau) / d\tau} < M(\theta)$ in a neighborhood of $\tau=0$.



\hrulefill

NB: The problem with this proof is that it requires you to be able
to interchange integration and differentiation with $\q(\theta \vert \eta) \ind{A}$
for all sets $A$, which is not transparently a weaker assumption than
is required for the parametric.

%
It suffices to show that \assuref{exchange_order_q} implies
\assuref{exchange_order} for the perturbation given in \defref{prior_nl_pert}
when $\norminf{\phi} < \infty$.  Observe that $\log \ptil(\theta \vert \t) = \t
\phi(\theta)$, so
%
\begin{align*}
%
\expect{\q(\theta \vert \eta)}{\log \ptil(\theta \vert \t)} =
    \t \int \q(\theta \vert \eta) \phi(\theta) \mu(d\theta).
%
\end{align*}
%
Consider the derivative $\partial / \partial \eta$.  It suffices to show that
%
\begin{align*}
%
\MoveEqLeft
\norm{ \lim_{\eta \rightarrow \etaopt}
\int \phi(\theta)
    \left(\frac{\q(\theta \vert \eta) - \q(\theta \vert \etaopt)}
               {\eta - \etaopt} -
               \fracat{\partial \q(\theta \vert \eta)}{\partial \eta}{\etaopt}
           \right) \mu(d\theta) }_2
\\\le{}&
\norminf{\phi}
\lim_{\eta \rightarrow \etaopt}
\int
    \norm{\frac{\q(\theta \vert \eta) - \q(\theta \vert \etaopt)}
               {\eta - \etaopt} -
               \fracat{\partial \q(\theta \vert \eta)}{\partial \eta}{\etaopt}
           }_2  \mu(d\theta)
={} 0.
%
\end{align*}
%
The second derivative follows analogously.
%
