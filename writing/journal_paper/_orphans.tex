
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[Regression mixture model]\exlabel{mice_bnp_process}

We cluster time-course gene expression data.
An observation $\x_\n\in\mathbb{R}^\ntimepoints$ is a vector of
expression levels at $\ntimepoints$
time points.
Let $\regmatrix$ be an $\ntimepoints \times \d$ regressor matrix.
In our case, we will use cubic B-splines to smooth the time-course observations,
so the $ij$-th entry of $\regmatrix$
will be the $j$-th B-spline basis vector evaluated at the
$i$-th time point (\secref{results_mice}).

Each component is characterized by a vector of regression coefficients
$\mu_\k$ and a variance $\tau^{-1}_\k$, so
in this model, $\beta_k = (\mu_\k, \tau_\k)$.
The distribution of the data arising from component $k$ is
\begin{align*}
\p(\x_\n | \beta_\k, \b_\n) =
\normdist{\x_\n | \regmatrix\mu_\k + \b_\n,
\tau_\k^{-1}I_{\ntimepoints \times \ntimepoints}},
\end{align*}
%
where $\b_{n}$ is a gene-specific additive offset and $I$ is the identity matrix.
We include the additive offset because we
are interested in clustering gene expressions based on their patterns over time,
not their absolute level.

The joint distribution can be written in the same form as~\eqref{bnp_model},
except that the conditional data likelihood now depends on $\b_\n$ as well as $\beta_\k$,
and we include an additional prior term for $\b_\n$.
% \begin{align*}
% \logp(\x, \beta, \z, \nu, \b) =&
%     \sum_{n=1}^N \sum_{k=1}^{\infty}
%         \z_{\n\k} \left(
%             \logp(\x_n \vert \beta_\k, \b_n) + \log \pshift(\b_n) + \log \pi_\k
%         \right)  \\
%     &{} + \sum_{k=1}^{\infty} \left(
%         \log \pstick(\nuk) + \log \pbetaprior(\beta_\k)
%     \right).
% \end{align*}
%
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our last example is a Bayesian topic model applied to genetic data.
Genotypes at genetic markers take the place of
words in a document; in lieu of inferring ``topics," we infer latent populations.

\begin{ex}[A topic model for population structure]\exlabel{structure_bnp_process}

We consider genetic data where the
data set consists of $\nindiv$ individuals genotyped at $\nloci$ loci.
For diploid organisms, there are two observations at each loci, one at each chromosome.
Let $\x_{\n\l\i}\in\{1, \ldots, J_\l\}$ be the observed genotype for
individual $\n$ at locus $\l$ and chromosome $\i$;
$J_\l$ is the number of possible genotypes at locus $\l$.
For example, if the measurements are single nucleotides (A, T, C or G)
then $J_\l = 4$ for all $\l$.

A latent population is characterized by the collection
$\beta_k = (\latentpop_{\k1}, \ldots, \latentpop_{\k\nloci})$ where
$\latentpop_{\k\l}\in\Delta^{J_\l - 1}$ are the latent frequencies for the $J_l$
possible genotypes at locus $\l$.
Let $\z_{\n\l\i}$ be the assignment of observation $\x_{\n\l\i}$ to a latent population.
Notice that for a given individual $\n$,
different loci, or even different chromosomes at a given locus,
may have different population assignments.
The distribution of $\x_{\n\l\i}\in\{1, \ldots, J_\l\}$ arising from population $\k$ is
\begin{align*}
\p(\x_{\n\l\i} \vert \latentpop_{\k}) =
\categoricaldist{\x_{\n\l\i}\vert \latentpop_{\k\l}}.
\end{align*}


Unlike the previous models, we now have a stick-breaking process for each individual.
Draw sticks
\begin{align*}
\nu_{\n\k} \iid \pstick(\nu_{\n\k}) \quad \forall \n = 1, \ldots, \nindiv; \k = 1, 2, \ldots \infty.
\end{align*}
The prior assignment probability vector
$\latentadmix_{\n} = (\latentadmix_{\n1}, \latentadmix_{\n2}, \ldots)$,
now unique to each individual,
is formed by the same stick-breaking construction as before,
%
\begin{align*}
\latentadmix_{\n\k} = \nu_{\n\k} \prod_{\k' < \k} (1 - \nu_{\n\k'}).
\end{align*}
%
The population assignment $\z_{\n\l\i}$
is drawn from the
usual multinomial distribution
%
\begin{align*}
p(\z_{\n\l\i} | \latentadmix_\n) = \prod_{k=1}^{\infty} \latentadmix_{\n\k}^{\z_{\n\l\i\k}}.
\end{align*}
%
In this genetics application,
we call $\latentadmix_{\n}$ the
\textit{admixture} of individual $\n$.


The joint log-likelihood decomposes as
\begin{align*}
\logp(\x, \latentpop, \z, \nu) &=
\sum_{\n=1}^\nindiv \sum_{\l=1}^\nloci \sum_{i = 1}^2 \sum_{\k=1}^{\infty}
        \z_{\n\l\i\k} \left(
            \logp(\x_{\n\l\i} \vert \latentpop_{\k}) + \log \pi_{\n\k}
        \right)
\nonumber\\&
    \quad +
    \sum_{\n=1}^\nindiv \sum_{k=1}^{\infty} \log \pstick(\nu_{\n\k})
    + \sum_{k=1}^{\infty} \log \pbetaprior(\latentpop_{\k}).
\end{align*}

This model is identical to STRUCTURE,
a model proposed in \citet{pritchard:2000:structure, raj:2014:faststructure},
except that we replace the Dirichlet prior in STRUCTURE
with an infinite stick-breaking process.
The result is a model similar to a hierarchical Dirichlet process for topic modeling,
\citep{teh:2006:hdp},
but without the top-level Dirichlet process.
%
\end{ex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsubsection{Conditional conjugacy}\seclabel{vb_conjugacy}

For $\z$ and $\beta$ in all models we consider, we will
take advantage of conditional conjugacy to choose distributions
$\q(\z_\n\vert\eta)$ and $\q(\beta_k\vert\eta)$, unless otherwise stated.
This means that we will take $\q(\z_{\n}
\vert \eta)$ to be multinomial, matching $\p(\z_{\n}
\vert \x, \beta, \nu)$, and we will take $\q(\beta_\k \vert \eta)$
to match the distribution of $\p(\beta_{\k} \vert \x, \z, \nu)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\beta_\k$ in a GMM]\exlabel{iris_var_distr}
%
To evaluate the expectation in \eqref{vb_optimization}, we need to compute
the expected joint log-likelihood
\begin{align*}
  \expect{\q(\beta_\k \vert \eta)}{\log \p(\x_\n, \beta_\k)}. % , \quad
  % \expect{\q(\beta_\k \vert \eta)}{\log \q(\beta_\k \vert \eta)}.
\end{align*}

In \exref{iris_bnp_process}, $\beta_\k = (\mu_\k, \Lambda_\k)$,
and the likelihoods are Gaussian,
$\p(\x_\n \vert \beta_\k) = \normdist{\x_n \vert \mu_\k, \Lambda_\k^{-1}}$.
The prior $\pbetaprior(\beta_\k)$ a normal-Wishart.
Using the log densities displayed in \exref{iris_bnp_process},
observe that $\beta_\k$ enters the expected joint log-likelihood only through the
expected moments
%
\begin{align*}
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k},  \quad
\expect{\q(\beta_\k \vert \eta)}{\log|\Lambda_\k|},  \quad
\expect{\q(\beta_\k \vert \eta)}{\Lambda_\k\mu_\k}, \quad
\expect{\q(\beta_\k \vert \eta)}{\mu_\k\Lambda_\k\mu_\k}.
\end{align*}

The conditionally conjugate variational distribution on $\beta_\k$ is
normal-Wishart, which we denote as
$\q(\beta_\k \vert \eta) = \normalwishart{\beta_k \vert \eta}$.
With this choice of $\q(\beta_\k \vert \eta)$,
all the preceding expected moments
can be provided as closed-form functions of $\eta$.
%
% With this choice of $\q$, the expectations
% \begin{align*}
% \expect{\q(\beta_\k \vert \eta)}{\log \p(\x_\n \vert \beta_\k)} \quad
% \expect{\q(\beta_\k \vert \eta)}{\log \pbetaprior(\beta_\k)} \quad
% \expect{\q(\beta_\k \vert \eta)}{\log \q(\beta_\k \vert \eta)}
% \end{align*}
% all have closed form expressions as functions of $\eta$.
%
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Under the mean-field factorization (\eqref{vb_mf}),
the vector $\eta$ will partition into parameters
governing $\nu$, $\beta$, and $\z$. Let the parameters governing a
particular latent variable or latent vector be denoted with a subscript: for example,
$\q(\beta \vert \eta) = \q(\beta \vert \etabeta)$,
$\q(\z_\n \vert \eta) = \q(\z \vert \eta_{\z_{\n}})$, and so on.
With conditionally conjugate distributions and our mean-field assumption,
the parameters $\eta_{\z_{\n}}$ can be optimally set as a function of
parameters $\etabeta$ and $\etanu$.
The next example details this point.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[VB approximation for $\z_\n$ in a GMM]\exlabel{qz_form}
%
The conditionally conjugate variational distribution for $\z_\n$
is multinomial.
Our variational approximation is truncated at $\kmax$ so
$\z_{\n\k} = 0$ for all $\k > \kmax$;
the multinomial distribution under $\q$ has $\kmax$ discrete categories.

We parameterize the the multinomial distribution
using its natural parameterization
in exponential family form. That is,
we let $\eta_{\z_\n} = (\rho_{\n1}, \rho_{\n2}, ..., \rho_{\n(\kmax-1)})$
be an unconstrained vector in $\mathbb{R}^{\kmax-1}$;
in this parameterization, the multinomial expectations are
%
\begin{align*}
  p_{\n\k} := \expect{\q(\z_\n \vert \etaz)}{\z_{\n\k}} =
  \frac{\exp(\rho_{\n\k})}{1 + \sum_{\k'=1}^{\kmax-1}\exp(\rho_{\n\k})}
\end{align*}
%
We use the exponential family parameterization because
we will require the optimal variational parameters $\etaopt$
to be interior to $\etadom$ in our sensitivity analysis
(\secref{local_sensitivity}).
In the mean parameterization,
$\sum_{\k=1}^\kmax p_{\n\k} = 1$, so the
optimal mean parameters $\hat p_{\n}$ cannot be
interior to $\Delta^{\kmax - 1}$.
On the other hand, $\eta_{\z_\n}$ as defined
is unconstrained in $\mathbb{R}^{\kmax - 1}$.

Moreover, with the distributions $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$ fixed,
the parameter vector $\eta_{\z_\n}$ that minimizes \eqref{vb_optimization}
has a closed form.
Fixing $\q(\beta\vert\etabeta)$ and $\q(\nu\vert\etanu)$,
the optimal $\etaopt_{\z_\n}$ must satisfy
%
\begin{align*}
& \q(\z_\n | \etaopt_{\z_\n}) \propto \exp\left(\tilde \rho_{\n\k}\right)\\
& \mathwhere \tilde \rho_{\n\k} := \expect{\q(\beta, \nu \vert \eta)}
       {\logp(\x_n \vert \beta_\k) + \log \pi_\k}.
\end{align*}
%
See \citet{bishop:2006:PRML} and \citet{blei:2017:vi_review} for details.
To satisfy this optimality condition,
we set the optimal $\etaopt_{\z_\n}$ to be
%
\begin{align*}
%
\etaopt_{\z_\n} = \left(\log\frac{\tilde\rho_{\n1}}{\tilde\rho_{\n\kmax}},
\log\frac{\tilde\rho_{\n2}}{\tilde\rho_{\n\kmax}}, \ldots,
\log\frac{\tilde\rho_{\n(\kmax-1)}}{\tilde\rho_{\n\kmax}}\right).
%
\end{align*}
%
Thus, as long as the expectation $\tilde\rho_{\n\k}$ has a closed-form as a function of
$(\etabeta, \etanu)$, the optimal $\etaopt_{\z_\n}$ can be also be set in closed-form as
a function of $(\etabeta, \etanu)$.
%
\end{ex}

For fixed $(\etabeta, \etanu)$, the option of setting $\etaz$ at its optimum
extends beyond the GMM example and will play
a key role in computing our local sensitivity
measures in practice (\secref{computing_sensitivity}).
In greater generality, each of our example models
has latent variables that factorize in a global/local structure.
In the GMM example discussed above, we call the variables $(\beta, \nu)$ ``global"
because they are shared across all data points; the $\z$ is ``local"
because each $\z_\n$ is unique to a single data point.
In the regression model (\exref{mice_bnp_process}),
the global variables are again $(\beta, \nu)$,
but the local variables comprise of both the cluster assignments $\z$ and additive shifts $\b$.
In these two models, notice that the dimension of global variables scale with $\kmax$, while
the dimension of local variables scale with the number of observations $\N$.

In the topic model (\exref{structure_bnp_process}),
we still call $(\beta, \nu)$ the global latent variables, even though they scale
with the number of individuals $\N$;
they do not, however, scale with both the number of individuals and the number of loci
like $\z$ does. In the topic model, we call $\z$ the local latent variables.

Let $\gamma = (\beta,\nu)$ be the global latent variables
and let $\etaglob = (\etabeta, \etanu)$ be their variational parameters.
Let $\etalocal$ be the local variational parameters. In \exref{iris_bnp_process} and
\exref{structure_bnp_process}, $\etalocal = \etaz$, while
in \exref{mice_bnp_process}, $\etalocal = (\etaz, \eta_\b)$.

In each model we consider, for $\etaglob$ fixed, the optimal $\etalocal$ that minimizes
the $\mathrm{KL}$ can be set in closed form as a function of $\etaglob$.
The multinomial parameters for $\eta_{\z_\n}$ in the regression and topic models
can be set in the same way as described in \exref{qz_form}.
For more details concerning the optimal shift parameters $\eta_\b$
in the regression model, see \appref{app_mice}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Evaluating stick expectations}\seclabel{stick_expectations}

To evaluate the $\mathrm{KL}$ in \eqref{vb_optimization}, we also need
the expectations over stick-breaking proportions,
\begin{align}\eqlabel{stick_expectations}
%
\expect{\q(\nuk \vert \eta)}{\log \nuk}
\textrm{,}\quad
\expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
\textrm{,}\quad\textrm{and}\quad
\expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
%
\end{align}
(The discussion in this subsection applies to the topic model as well,
with stick-breaking proportions indexed by $\n\k$).
The first two expectations appear in the $\mathrm{KL}$
when decomposing the mixture weights
$\expect{}{\log \pi}$ into its component stick-breaking proportions (\eqref{stick_breaking}).

If the prior $\pstick(\nuk)$ were Beta-distributed like in the GEM construction,
then the conditionally conjugate distribution for $\q(\nuk \vert \eta)$ would
also be Beta. In this case, all the displayed expectations in
\eqref{stick_expectations} can be computed analytically as a function of the
Beta parameters in the variational approximation.

However, we will be considering stick-breaking distributions $\pstick(\nuk)$
that are outside the family of Beta distributions. To accommodate a generic
prior $\pstick(\nuk)$, we approximate the expectations in
\eqref{stick_expectations} numerically. Each expectation is a univariate
integral. A particularly easy approximation method is Gauss-Hermite (GH)
quadrature, which we now describe.

To take advantage of GH quadrature, we first logit transform the stick-breaking
proportion $\nuk$ so that the transformed variable
\begin{align*}
  \lnuk := \log\left(\frac{\nuk}{1 - \nuk}\right)
\end{align*}
is not constrained to be between $(0, 1)$ and can take values in all of $\mathbb{R}$.
Let $\s$ be the sigmoid function,
which provides the inverse transformation,
\begin{align*}
  \nuk = \s(\tilde\nuk) := \frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}.
\end{align*}

We choose $\q(\lnu_\k \vert \eta)$ to be normally distributed with
location parameter $\lnumean_\k$ and scale parameter $\lnusd_\k$.
This then induces a logit-normal
distribution on our original variable of interest, $\nuk$.
To compute expectations of a smooth function
$f(\nuk)$ (such as $f(\nuk) = \pstick(\nuk)$),
the law of the unconscious statistician states that,
\begin{align*}
  \expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
  \expect{\q(\lnu_\k \vert \eta)}
         {f\circ \s\left(\lnu_\k\right)}.
\end{align*}
By choosing $\q(\lnu_\k \vert \eta)$ to be Gaussian,
the right-hand side is a Gaussian integral,
which we approximate
using GH quadrature with $\ngh$ knots,
located at $\xi_g$, weighted by $\omega_g$:
%
\begin{align}\eqlabel{gh_integral}
%
\expect{\q(\lnu_\k \vert \eta)}
       {f\circ \s\left(\lnu_\k\right)}
\approx{}&
    \sum_{g=1}^{\ngh} \omega_g f\circ \s \left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
 \nonumber\\=:{}&
\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
%
\end{align}
%
Using GH quadrature to approximate the expectation
is similar to the ``reparameterization trick,'' only using
GH points rather than standard normal draws.
Conveniently, the approximation $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$
is a deterministic, differentiable
function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.
This will be useful in our sensitivity computations in the next section.

\hrulefill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Posterior quantities}
\seclabel{posterior_quantities}

In a VB approach, all posterior quantities of interest can be expressed as
functions of the variational parameter $\eta$. We will use $\g(\eta)$ to denote
such quantities. Often, $\g(\eta)$ takes the form of an expectation over $\q$,
\begin{align*}
  g(\eta) = \expect{q(\zeta\vert\eta)}{f(\zeta)}
\end{align*}
for some function $f(\eta)$.

In the next few examples, we define some posterior quantities that we will
consider in \secref{results}. We will evaluate the sensitivity
of these quantities to the prior specification $\pstick$.

\begin{ex}[The in-sample number of clusters]\exlabel{insample_nclusters}

One might ask, \textit{how many clusters are present in the data set}?
For example, in the iris data set, answering this question has the interpretation
of counting the number of iris species present.
To estimate the number of clusters in the context of a BNP model,
define the random variable
\begin{align*}
  \nclusters_\tau := \sum_{k=1}^\kmax \ind{ \left(\sum_{n=1}^{N}
  \z_{\n\k}\right) > \tau},
\end{align*}
where $\ind{\cdot}$ is the indicator function.
$\nclusters_\tau$ counts the number of clusters with at least $\tau$
observations in a set of assignments $\z$.
The expected number of clusters under the variational posterior is
\begin{align*}
  \gclusters(\eta) := \expect{\q(\z\vert\eta)}{\nclusters_\tau}.
\end{align*}

When $\tau = 0$, $\gclusterszero$ can be written as a function with respect to
the assignment probabilities
\begin{align*}
  \gclusterszero(\eta) = \sum_{k=1}^\kmax \left(1 -  \prod_{n=1}^N
  \left(1 - \expect{\q(\z_{nk}\vert\eta)}{\z_{nk}}\right)\right).
\end{align*}
%
\end{ex}

\begin{ex}[The predictive number of clusters]\exlabel{predictive_nclusters}

In the Bayesian approach, we can formulate the posterior predictive question,
\textit{how many clusters would be present if a new data set were collected}?
In the iris example, this can interpreted as predicting the number of species
one might see if a fresh sample of iris flowers were collected.
Under the BNP model, the expected number of predictive clusters is defined as
\begin{align*}
  \gclusterspred(\eta) &:= \expect{\q(\pi\vert\eta)}{\expect{\p(\z\vert\pi)}{\nclusters_\tau}}.
\end{align*}
Notice that the inner expectation conditions on $\pi$ and the randomness is
over $\z$ sampled from the generative model $\z\sim\p(\z\vert\pi)$.
We can write out the inner expectation:
%
\begin{align*}
  \gclusterspred(\eta) &= \expect{\q(\pi\vert\eta)}{\sum_{k=1}^\kmax\left(1 -
  \sum_{i=0}^{\lfloor\tau\rfloor} {\N \choose i} (1 - \pi_k)^\N \right)},
\end{align*}
where we use the convention that ${\N\choose 0} = 1$.
%
\end{ex}

\begin{ex}[Co-clustering]\exlabel{posterior_coclustering}

Finally, in a clustering problem, we are often interested in understanding
which observations group with each other.
One way to visualize the clusters is to construct the co-clustering matrix,
\begin{align*}
\gcoclustering(\eta) := \expect{\q(\z\vert\eta)}{\z\z^{T}},
\end{align*}
where we view $\z$ as a $\N\times \kmax$ matrix of cluster assignments.
Unlike the quantities in \exref{insample_nclusters, predictive_nclusters},
$\gcoclustering$ is a matrix quantity, not a scalar quantity.

\end{ex}

For some posterior quantities, the expectation over $\q$ will not be a simple
closed-form function of $\eta$. For example, computing $\gclusters$ with a
threshold $\tau > 0$ requires forming all ${\N\choose\tau}$ combination of
$\tau$-length products $\expect{}{\z_{\n_1\k}}\times \ldots \times
\expect{}{\z_{\n_\tau\k}}$ for each $\k$. In such cases, we resorted to
Monte-Carlo approximations of the expectation. Specifically, we used the
``reparameterization trick" to sample from the variational distribution. In the
case of $\gclusters$, we constructed an $\eta$-dependent transformation
$f(\cdot, \eta)$ that satisfies \begin{align*} u \iid\text{Uniform}(0,
1)^{\N\kmax} \implies f(u, \eta) \stackrel{d}{=} \z \sim \q(\cdot | \eta).
\end{align*} To form a Monte Carlo estimate of $\gclusters$, we sampled $u_1,
\dots, u_m$ uniformly, and then averaged the expression inside the expectation
evaluated at points $f(u_1, \eta), \ldots, f(u_m, \eta)$. The uniform draws
$u_1, \dots, u_m$ can be fixed beforehand. This is important for two reasons.
First, we will be evaluating the same $\g$ at different parameter vectors
$\eta$; conditional on the fixed $m$ uniform draws, $\g$ will be a deterministic
function of $\eta$, and we can compare how $\g$ changes without stochasticity.
Secondly, in the construction of our ``influence function"
(\secref{functional_perturbations}), it will be useful to evaluate the gradient
of $\g$ with respect to $\eta$; conditional on the random draws, $\g$ (or more
precisely, our Monte Carlo approximation of $\g$), will be differentiable with
respect to $\eta$.


\hrulefill



% \subsection*{subsection name}
%
% we choose $\q(\nuk \vert \eta)$ to be logit-normally distributed,
% and express the expectations in \eqref{stick_expectations} as Gaussian integrals.
% Define
% \begin{align*}
%   \tilde \nuk := \log\left(\frac{\nuk}{1 - \nuk}\right),
% \end{align*}
% which will be normally distributed under our choice of a
% logit-normal $\q(\nuk \vert \eta)$.
%
% Let $\lnumean_\k$ and $\lnusd_\k$ be entries of $\eta$ corresponding to
% the logit-normal parameters of $\nuk$.
%
%
% In order to optimize the variational objective \eqref{vb_optimization} we see
% from \eqref{stick_log_post} that we need to evaluate or approximate expectations
% of the form
% %
% \begin{align*}
% %
% \expect{\q(\nuk \vert \eta)}{\log \nuk}
% \textrm{,}\quad
% \expect{\q(\nuk \vert \eta)}{\log (1 - \nuk)}
% \textrm{,}\quad\textrm{and}\quad
% \expect{\q(\nuk \vert \eta)}{\log \pstick(\nuk)}.
% %
% \end{align*}
%
%
%
%
% First, define a version of $\nuk$ that is not constrained to $(0,1)$:
% %
% \begin{align}\eqlabel{lnuk_transform}
% %
% \lnuk :={} \log \left( \frac{\nuk}{1 - \nuk} \right)
% \quad\Leftrightarrow\quad
% \nuk :={} \frac{\exp(\lnuk)}{1 + \exp(\lnuk)}.
% %
% \end{align}
% %
It will be useful later to have at hand the transform between densities
expressed in the space of $\nu$ and $\lnu$, which is given by
%
\begin{align}\eqlabel{lnuk_derivatives}
%
\fracat{d \lnu_\k}{ d\nuk}{\nuk} ={}
%     \frac{1-\nuk}{\nuk}
%     \left(\frac{1}{1 - \nuk} + \frac{\nuk}{(1 - \nuk)^2} \right)
% \\={}& \frac{1}{\nuk} + \frac{1}{1 - \nuk}
% \\={}&
    \frac{1}{\nuk (1 - \nuk)} \mathand
%
\fracat{d \nuk}{ d\lnuk}{\lnuk} ={}
    \frac{\exp(\lnuk)}{(1 + \exp(\lnuk))^2}.
%
\end{align}
% %
% We wish to let $\lnu_\k$ be distributed normally under the variational
% distribution.  Let $\lnumean_\k$ and $\lnusd_\k$ be entries of the parameter
% vector $\eta$, and write
% %
% \begin{align}\eqlabel{lnuk_vb_approximation}
% %
% \q(\lnu_\k \vert \eta) ={}& \normdist{\lnu_\k \vert \lnumean_\k, \lnusd_\k}
% \Rightarrow \\
% \q(\nuk \vert \eta) ={}&
%     \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\fracat{d \lnu_\k}{ d\nuk}{\nuk}\right|
% \nonumber\\={}&
% \normdist{\log \left( \frac{\nuk}{1 - \nuk} \right)
%         \vert \lnumean_\k, \lnusd_\k}
%     \left|\frac{1}{\nuk (1 - \nuk)}\right|.
% \nonumber
% %
% \end{align}
% %
% Given this, we can approximate expectations of smooth functions
% $f(\nuk)$ using GH quadrature with $\ngh$ knots,
% located at $\xi_g$, weighted by $\omega_g$:
% %
% \begin{align}\eqlabel{gh_integral}
% %
% \expect{\q(\nuk \vert \eta)}{f(\nuk)} ={}&
% \expect{\q(\lnu_\k \vert \eta)}
%        {f\left(\frac{\exp(\lnu_\k)}{1 + \exp(\lnu_\k)}\right)}
% \nonumber\\\approx{}&
%     \sum_{g=1}^{\ngh} \omega_g f\left(\lnusd_\k \xi_{g} + \lnumean_\k\right)
%  \nonumber\\=:{}&
% \expecthat{\q(\nuk \vert \eta)}{f(\nuk)}.
% %
% \end{align}
% %
% Conveniently, $\expecthat{\q(\nuk \vert \eta)}{f(\nuk)}$ is a differentiable
% function of $\lnumean_\k$ and $\lnusd_\k$, and so also of $\eta$.  (This
% technique is similar to the ``reparameterization trick,'' only using
% GH points rather than standard normal draws.)

\hrulefill

In the regression example (\exref{mice_bnp_process}), $\zeta$ includes
the additive shifts, $\zeta := (\beta, \z, \nu, \b)$.

The variational approximation for the topic model
(\exref{structure_bnp_process}) is similarly mean-field: the distribution on
stick-breaking proportions $\nu$ factorizes over both individuals $\n$ and
components $\k$, while the assignments $\z$ factorize over individuals $\n$,
loci $\l$, and chromosomes $\i$. For the regression model
(\exref{mice_bnp_process}), all terms in the variational approximation
fully-factorize except for the cluster assignments $\z$ and additive shifts
$\b$. While we assume $(\z, \b)$ to be independent from all other latent
variables under $\q$, we will allow conditional dependence between $\z$ and $\b$
(\appref{app_mice}).
