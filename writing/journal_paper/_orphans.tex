
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{ex}[Regression mixture model]\exlabel{mice_bnp_process}

We cluster time-course gene expression data.
An observation $\x_\n\in\mathbb{R}^\ntimepoints$ is a vector of
expression levels at $\ntimepoints$
time points.
Let $\regmatrix$ be an $\ntimepoints \times \d$ regressor matrix.
In our case, we will use cubic B-splines to smooth the time-course observations,
so the $ij$-th entry of $\regmatrix$
will be the $j$-th B-spline basis vector evaluated at the
$i$-th time point (\secref{results_mice}).

Each component is characterized by a vector of regression coefficients
$\mu_\k$ and a variance $\tau^{-1}_\k$, so
in this model, $\beta_k = (\mu_\k, \tau_\k)$.
The distribution of the data arising from component $k$ is
\begin{align*}
\p(\x_\n | \beta_\k, \b_\n) =
\normdist{\x_\n | \regmatrix\mu_\k + \b_\n,
\tau_\k^{-1}I_{\ntimepoints \times \ntimepoints}},
\end{align*}
%
where $\b_{n}$ is a gene-specific additive offset and $I$ is the identity matrix.
We include the additive offset because we
are interested in clustering gene expressions based on their patterns over time,
not their absolute level.

The joint distribution can be written in the same form as~\eqref{bnp_model},
except that the conditional data likelihood now depends on $\b_\n$ as well as $\beta_\k$,
and we include an additional prior term for $\b_\n$.
% \begin{align*}
% \logp(\x, \beta, \z, \nu, \b) =&
%     \sum_{n=1}^N \sum_{k=1}^{\infty}
%         \z_{\n\k} \left(
%             \logp(\x_n \vert \beta_\k, \b_n) + \log \pshift(\b_n) + \log \pi_\k
%         \right)  \\
%     &{} + \sum_{k=1}^{\infty} \left(
%         \log \pstick(\nuk) + \log \pbetaprior(\beta_\k)
%     \right).
% \end{align*}
%
\end{ex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our last example is a Bayesian topic model applied to genetic data.
Genotypes at genetic markers take the place of
words in a document; in lieu of inferring ``topics," we infer latent populations.

\begin{ex}[A topic model for population structure]\exlabel{structure_bnp_process}

We consider genetic data where the
data set consists of $\nindiv$ individuals genotyped at $\nloci$ loci.
For diploid organisms, there are two observations at each loci, one at each chromosome.
Let $\x_{\n\l\i}\in\{1, \ldots, J_\l\}$ be the observed genotype for
individual $\n$ at locus $\l$ and chromosome $\i$;
$J_\l$ is the number of possible genotypes at locus $\l$.
For example, if the measurements are single nucleotides (A, T, C or G)
then $J_\l = 4$ for all $\l$.

A latent population is characterized by the collection
$\beta_k = (\latentpop_{\k1}, \ldots, \latentpop_{\k\nloci})$ where
$\latentpop_{\k\l}\in\Delta^{J_\l - 1}$ are the latent frequencies for the $J_l$
possible genotypes at locus $\l$.
Let $\z_{\n\l\i}$ be the assignment of observation $\x_{\n\l\i}$ to a latent population.
Notice that for a given individual $\n$,
different loci, or even different chromosomes at a given locus,
may have different population assignments.
The distribution of $\x_{\n\l\i}\in\{1, \ldots, J_\l\}$ arising from population $\k$ is
\begin{align*}
\p(\x_{\n\l\i} \vert \latentpop_{\k}) =
\categoricaldist{\x_{\n\l\i}\vert \latentpop_{\k\l}}.
\end{align*}


Unlike the previous models, we now have a stick-breaking process for each individual.
Draw sticks
\begin{align*}
\nu_{\n\k} \iid \pstick(\nu_{\n\k}) \quad \forall \n = 1, \ldots, \nindiv; \k = 1, 2, \ldots \infty.
\end{align*}
The prior assignment probability vector
$\latentadmix_{\n} = (\latentadmix_{\n1}, \latentadmix_{\n2}, \ldots)$,
now unique to each individual,
is formed by the same stick-breaking construction as before,
%
\begin{align*}
\latentadmix_{\n\k} = \nu_{\n\k} \prod_{\k' < \k} (1 - \nu_{\n\k'}).
\end{align*}
%
The population assignment $\z_{\n\l\i}$
is drawn from the
usual multinomial distribution
%
\begin{align*}
p(\z_{\n\l\i} | \latentadmix_\n) = \prod_{k=1}^{\infty} \latentadmix_{\n\k}^{\z_{\n\l\i\k}}.
\end{align*}
%
In this genetics application,
we call $\latentadmix_{\n}$ the
\textit{admixture} of individual $\n$.


The joint log-likelihood decomposes as
\begin{align*}
\logp(\x, \latentpop, \z, \nu) &=
\sum_{\n=1}^\nindiv \sum_{\l=1}^\nloci \sum_{i = 1}^2 \sum_{\k=1}^{\infty}
        \z_{\n\l\i\k} \left(
            \logp(\x_{\n\l\i} \vert \latentpop_{\k}) + \log \pi_{\n\k}
        \right)
\nonumber\\&
    \quad +
    \sum_{\n=1}^\nindiv \sum_{k=1}^{\infty} \log \pstick(\nu_{\n\k})
    + \sum_{k=1}^{\infty} \log \pbetaprior(\latentpop_{\k}).
\end{align*}

This model is identical to STRUCTURE,
a model proposed in \citet{pritchard:2000:structure, raj:2014:faststructure},
except that we replace the Dirichlet prior in STRUCTURE
with an infinite stick-breaking process.
The result is a model similar to a hierarchical Dirichlet process for topic modeling,
\citep{teh:2006:hdp},
but without the top-level Dirichlet process.
%
\end{ex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% We start with a Gaussian mixture model (GMM) because it conforms cleanly to the
% generative process culminating in \eqref{bnp_model}.
In \secref{results}, we fit a GMM to Fisher's iris data set
\citep{anderson:1936:iris, fisher:1936:iris} and cluster irises into latent
species based on morphological measurements. Our other experiments on real data
sets in \secref{results_mice, results_structure} require slightly different
modeling considerations, and we adjust the factorization in \eqref{bnp_model} to
suit our purposes.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
